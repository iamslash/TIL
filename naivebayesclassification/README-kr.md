# Abstract

Naive Bayes Classification (ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜)ì€ **ë² ì´ì¦ˆ ì •ë¦¬(Bayes Theorem)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í™•ë¥ ì  ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. "Naive(ìˆœì§„í•œ)"ë¼ëŠ” ì´ë¦„ì´ ë¶™ì€ ì´ìœ ëŠ” **ëª¨ë“  íŠ¹ì„±(feature)ì´ ì„œë¡œ ë…ë¦½ì **ì´ë¼ëŠ” ë‹¨ìˆœí•œ ê°€ì •ì„ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

> "ìŠ¤íŒ¸ ë©”ì¼ì¸ì§€ íŒë‹¨í•˜ë ¤ë©´, ì´ ë©”ì¼ì´ ìŠ¤íŒ¸ì¼ í™•ë¥ ì„ ê³„ì‚°í•˜ì"

**Naive Bayesì˜ í•µì‹¬ ê°œë…:**
- **ë² ì´ì¦ˆ ì •ë¦¬**: ì¡°ê±´ë¶€ í™•ë¥ ì„ ì´ìš©í•œ ì—­í™•ë¥  ê³„ì‚°
- **ì‚¬ì „ í™•ë¥ (Prior)**: ê¸°ì¡´ì— ì•Œê³  ìˆë˜ í™•ë¥ 
- **ìš°ë„(Likelihood)**: ì£¼ì–´ì§„ ì¡°ê±´ì—ì„œ ë°ì´í„°ê°€ ë‚˜íƒ€ë‚  í™•ë¥ 
- **ì‚¬í›„ í™•ë¥ (Posterior)**: ë°ì´í„°ë¥¼ ê´€ì°°í•œ í›„ì˜ í™•ë¥ 
- **ë…ë¦½ ê°€ì •(Independence Assumption)**: ëª¨ë“  íŠ¹ì„±ì´ ì„œë¡œ ë…ë¦½

ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤íŒ¸ ë©”ì¼ì„ íŒë‹¨í•  ë•Œ:
- "ë¬´ë£Œ"ë¼ëŠ” ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ìŠ¤íŒ¸ì¼ í™•ë¥ ì´ ì–¼ë§ˆë‚˜ ë ê¹Œ?
- "ëŒ€ì¶œ"ì´ë¼ëŠ” ë‹¨ì–´ê°€ ì¶”ê°€ë¡œ ìˆìœ¼ë©´ í™•ë¥ ì´ ì–´ë–»ê²Œ ë³€í• ê¹Œ?
- ì´ ë©”ì¼ì˜ ê° ë‹¨ì–´ë“¤ì„ ë³´ê³  ìŠ¤íŒ¸ í™•ë¥ ì„ ê³„ì‚°!

**Naive Bayesì˜ ì¥ì :**
- êµ¬í˜„ì´ ë§¤ìš° ê°„ë‹¨í•˜ê³  ë¹ ë¦„
- ì ì€ ë°ì´í„°ë¡œë„ ì˜ ì‘ë™
- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— íš¨ê³¼ì 
- í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— íŠ¹íˆ ê°•ë ¥í•¨
- í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡ ì œê³µ
- ì°¨ì›ì´ ë†’ì•„ë„ ì„±ëŠ¥ ìœ ì§€

**Naive Bayesì˜ ë‹¨ì :**
- ë…ë¦½ ê°€ì •ì´ í˜„ì‹¤ì ì´ì§€ ì•ŠìŒ (ë‹¨ì–´ë“¤ì€ ì‹¤ì œë¡œ ì„œë¡œ ì—°ê´€)
- í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” ì¡°í•©ì€ í™•ë¥ ì´ 0ì´ ë¨
- ì—°ì†í˜• ë°ì´í„°ëŠ” ì •ê·œë¶„í¬ ê°€ì • í•„ìš”
- íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ ê³ ë ¤ ë¶ˆê°€

**ì£¼ìš” í™œìš© ë¶„ì•¼:**
- í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ìŠ¤íŒ¸ í•„í„°ë§, ê°ì„± ë¶„ì„, ë¬¸ì„œ ë¶„ë¥˜)
- ì¶”ì²œ ì‹œìŠ¤í…œ
- ì˜ë£Œ ì§„ë‹¨ (ì¦ìƒ ê¸°ë°˜ ì§ˆë³‘ ì˜ˆì¸¡)
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ (ë¹ ë¥¸ ì‘ë‹µ í•„ìš”í•œ ì‹œìŠ¤í…œ)
- ì–¼êµ´ ì¸ì‹

# Materials

- [Scikit-learn Naive Bayes Documentation](https://scikit-learn.org/stable/modules/naive_bayes.html)
- [Naive Bayes Classifier - Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
- [StatQuest: Naive Bayes](https://www.youtube.com/watch?v=O2L2Uv9pdDA)

# Basic

## Naive Bayesë€?

Naive BayesëŠ” **ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°**í•˜ê³ , **ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜**í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

### ì§ê´€ì  ì´í•´: ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§

ë‹¹ì‹ ì—ê²Œ ìƒˆë¡œìš´ ì´ë©”ì¼ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤:

```
"ë¬´ë£Œ ëŒ€ì¶œ ì‹ ì²­í•˜ì„¸ìš”!"
```

ì´ ë©”ì¼ì´ ìŠ¤íŒ¸ì¼ê¹Œìš”, ì •ìƒ ë©”ì¼ì¼ê¹Œìš”?

**ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ê²ƒ:**
```
P(ìŠ¤íŒ¸ | ì´ ë©”ì¼) = ì´ ë©”ì¼ì„ ë´¤ì„ ë•Œ ìŠ¤íŒ¸ì¼ í™•ë¥ ì€?
```

**ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´:**
```
P(ìŠ¤íŒ¸ | ë©”ì¼) = P(ë©”ì¼ | ìŠ¤íŒ¸) Ã— P(ìŠ¤íŒ¸) / P(ë©”ì¼)

ì—¬ê¸°ì„œ:
- P(ìŠ¤íŒ¸): ì‚¬ì „ í™•ë¥  - ì›ë˜ ìŠ¤íŒ¸ ë©”ì¼ì´ ì˜¬ í™•ë¥ 
- P(ë©”ì¼ | ìŠ¤íŒ¸): ìš°ë„ - ìŠ¤íŒ¸ ë©”ì¼ì¼ ë•Œ ì´ëŸ° ë‚´ìš©ì´ ë‚˜íƒ€ë‚  í™•ë¥ 
- P(ë©”ì¼): ì¦ê±° - ì´ëŸ° ë©”ì¼ì´ ë‚˜íƒ€ë‚  í™•ë¥  (ì •ê·œí™” ìƒìˆ˜)
- P(ìŠ¤íŒ¸ | ë©”ì¼): ì‚¬í›„ í™•ë¥  - ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ìµœì¢… í™•ë¥ !
```

### ë² ì´ì¦ˆ ì •ë¦¬ (Bayes Theorem)

**ê¸°ë³¸ ê³µì‹:**

```
P(A|B) = P(B|A) Ã— P(A) / P(B)

ì½ëŠ” ë²•: "Bê°€ ì£¼ì–´ì¡Œì„ ë•Œ Aì¼ í™•ë¥ "
```

**ê° í•­ì˜ ì˜ë¯¸:**

1. **P(A)**: ì‚¬ì „ í™•ë¥  (Prior Probability)
   - Bë¥¼ ë³´ê¸° ì „ì— Aì¼ í™•ë¥ 
   - ì˜ˆ: ì „ì²´ ë©”ì¼ ì¤‘ 30%ê°€ ìŠ¤íŒ¸ â†’ P(ìŠ¤íŒ¸) = 0.3

2. **P(B|A)**: ìš°ë„ (Likelihood)
   - Aì¼ ë•Œ Bê°€ ê´€ì°°ë  í™•ë¥ 
   - ì˜ˆ: ìŠ¤íŒ¸ ë©”ì¼ì— "ë¬´ë£Œ"ë¼ëŠ” ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ 

3. **P(B)**: ì¦ê±° (Evidence)
   - Bê°€ ê´€ì°°ë  ì „ì²´ í™•ë¥ 
   - ì •ê·œí™”ë¥¼ ìœ„í•´ ì‚¬ìš©

4. **P(A|B)**: ì‚¬í›„ í™•ë¥  (Posterior Probability)
   - Bë¥¼ ê´€ì°°í•œ í›„ Aì¼ í™•ë¥ 
   - ìš°ë¦¬ê°€ ìµœì¢…ì ìœ¼ë¡œ êµ¬í•˜ê³ ì í•˜ëŠ” ê°’!

### ì™œ "Naive(ìˆœì§„í•œ)"ì¸ê°€?

**ë…ë¦½ ê°€ì • (Independence Assumption):**

ë©”ì¼ì— ["ë¬´ë£Œ", "ëŒ€ì¶œ", "ì‹ ì²­"] ì„¸ ë‹¨ì–´ê°€ ìˆì„ ë•Œ:

```
ì¼ë°˜ì ì¸ í™•ë¥  (ë…ë¦½ì´ ì•„ë‹ ë•Œ):
P(ë¬´ë£Œ, ëŒ€ì¶œ, ì‹ ì²­ | ìŠ¤íŒ¸) = ë³µì¡í•œ ê²°í•© í™•ë¥  ê³„ì‚° í•„ìš”
â†’ "ë¬´ë£Œ"ì™€ "ëŒ€ì¶œ"ì´ í•¨ê»˜ ë‚˜ì˜¬ í™•ë¥ , "ëŒ€ì¶œ"ê³¼ "ì‹ ì²­"ì´ í•¨ê»˜ ë‚˜ì˜¬ í™•ë¥  ë“±...

Naive Bayes (ë…ë¦½ ê°€ì •):
P(ë¬´ë£Œ, ëŒ€ì¶œ, ì‹ ì²­ | ìŠ¤íŒ¸) = P(ë¬´ë£Œ|ìŠ¤íŒ¸) Ã— P(ëŒ€ì¶œ|ìŠ¤íŒ¸) Ã— P(ì‹ ì²­|ìŠ¤íŒ¸)
â†’ ê° ë‹¨ì–´ì˜ í™•ë¥ ë§Œ ê³±í•˜ë©´ ë¨!
```

**í˜„ì‹¤ì—ì„œëŠ”:**
- "ë¬´ë£Œ"ì™€ "ëŒ€ì¶œ"ì€ ì‹¤ì œë¡œ í•¨ê»˜ ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ ë†’ìŒ (ë…ë¦½ì´ ì•„ë‹˜)
- í•˜ì§€ë§Œ ë…ë¦½ì´ë¼ê³  ê°€ì •í•˜ë©´ ê³„ì‚°ì´ í›¨ì”¬ ê°„ë‹¨í•´ì§
- ë†€ëê²Œë„ ì´ ë‹¨ìˆœí•œ ê°€ì •ìœ¼ë¡œë„ ì‹¤ì œë¡œ ì˜ ì‘ë™í•¨!

## ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ ì˜ˆì œ (ë™ì˜ìƒ ì˜ˆì œ)

### í›ˆë ¨ ë°ì´í„°

ìš°ë¦¬ì—ê²Œ 6ê°œì˜ ì´ë©”ì¼ì´ ìˆìŠµë‹ˆë‹¤:

**ìŠ¤íŒ¸ ë©”ì¼ (3ê°œ):**
1. "ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
2. "ê¸´ê¸‰ ëŒ€ì¶œ ì‹ ì²­í•˜ì„¸ìš”"
3. "ë¬´ë£Œ ì²´í—˜ ì‹ ì²­"

**ì •ìƒ ë©”ì¼ (3ê°œ):**
1. "íšŒì˜ ì¼ì • ê³µìœ í•©ë‹ˆë‹¤"
2. "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©"
3. "ë‚´ì¼ ì ì‹¬ ì•½ì†"

### ë‹¨ì–´ ë¹ˆë„ ë¶„ì„

ê° ë‹¨ì–´ê°€ ìŠ¤íŒ¸/ì •ìƒ ë©”ì¼ì— ë‚˜íƒ€ë‚œ íšŸìˆ˜ë¥¼ ì„¸ì–´ë´…ì‹œë‹¤:

**ìŠ¤íŒ¸ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„:**
```
ë‹¨ì–´      | ì¶œí˜„ íšŸìˆ˜ | ì´ ë‹¨ì–´ ìˆ˜
---------|----------|----------
ë¬´ë£Œ      | 2        | 9
ëŒ€ì¶œ      | 2        | 9
ì‹ ì²­      | 2        | 9
ê¸´ê¸‰      | 1        | 9
ê°€ëŠ¥      | 1        | 9
ì²´í—˜      | 1        | 9
```

**ì •ìƒ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„:**
```
ë‹¨ì–´      | ì¶œí˜„ íšŸìˆ˜ | ì´ ë‹¨ì–´ ìˆ˜
---------|----------|----------
íšŒì˜      | 1        | 9
ì¼ì •      | 1        | 9
ê³µìœ       | 1        | 9
í”„ë¡œì íŠ¸   | 1        | 9
ì§„í–‰      | 1        | 9
ìƒí™©      | 1        | 9
ë‚´ì¼      | 1        | 9
ì ì‹¬      | 1        | 9
ì•½ì†      | 1        | 9
```

### ìƒˆë¡œìš´ ë©”ì¼ ë¶„ë¥˜: "ë¬´ë£Œ ëŒ€ì¶œ ì‹ ì²­"

**Step 1: ì‚¬ì „ í™•ë¥  ê³„ì‚°**

```
P(ìŠ¤íŒ¸) = 3/6 = 0.5
P(ì •ìƒ) = 3/6 = 0.5
```

**Step 2: ìš°ë„ ê³„ì‚° (ê° ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ )**

**ìŠ¤íŒ¸ì¼ ë•Œ:**
```
P(ë¬´ë£Œ | ìŠ¤íŒ¸) = 2/9 â‰ˆ 0.222
P(ëŒ€ì¶œ | ìŠ¤íŒ¸) = 2/9 â‰ˆ 0.222
P(ì‹ ì²­ | ìŠ¤íŒ¸) = 2/9 â‰ˆ 0.222

ë…ë¦½ ê°€ì •ìœ¼ë¡œ ê³±í•˜ê¸°:
P(ë¬´ë£Œ, ëŒ€ì¶œ, ì‹ ì²­ | ìŠ¤íŒ¸) = 0.222 Ã— 0.222 Ã— 0.222 â‰ˆ 0.0109
```

**ì •ìƒì¼ ë•Œ:**
```
P(ë¬´ë£Œ | ì •ìƒ) = 0/9 = 0  â† ë¬¸ì œ ë°œìƒ!
P(ëŒ€ì¶œ | ì •ìƒ) = 0/9 = 0
P(ì‹ ì²­ | ì •ìƒ) = 0/9 = 0

P(ë¬´ë£Œ, ëŒ€ì¶œ, ì‹ ì²­ | ì •ìƒ) = 0 Ã— 0 Ã— 0 = 0
```

**Step 3: ì‚¬í›„ í™•ë¥  ê³„ì‚°**

```
P(ìŠ¤íŒ¸ | ë©”ì¼) âˆ P(ë©”ì¼ | ìŠ¤íŒ¸) Ã— P(ìŠ¤íŒ¸)
              = 0.0109 Ã— 0.5
              = 0.00545

P(ì •ìƒ | ë©”ì¼) âˆ P(ë©”ì¼ | ì •ìƒ) Ã— P(ì •ìƒ)
              = 0 Ã— 0.5
              = 0

â†’ ìŠ¤íŒ¸ì¼ í™•ë¥ ì´ ë” ë†’ìŒ! (ì‹¤ì œë¡œëŠ” ë¬´í•œëŒ€)
```

**ê²°ë¡ :** ì´ ë©”ì¼ì€ **ìŠ¤íŒ¸**ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤!

### ë¬¸ì œì : í™•ë¥ ì´ 0ì´ ë˜ëŠ” ê²½ìš°

ìœ„ ì˜ˆì œì—ì„œ ì •ìƒ ë©”ì¼ í™•ë¥ ì´ 0ì´ ë‚˜ì™”ìŠµë‹ˆë‹¤. ì´ëŠ”:

```
ì •ìƒ ë©”ì¼ í›ˆë ¨ ë°ì´í„°ì— "ë¬´ë£Œ", "ëŒ€ì¶œ", "ì‹ ì²­" ë‹¨ì–´ê°€ í•œ ë²ˆë„ ì•ˆ ë‚˜íƒ€ë‚¨
â†’ P(ë¬´ë£Œ | ì •ìƒ) = 0
â†’ ì „ì²´ í™•ë¥ ë„ 0
```

**ì´ê²ƒì€ í° ë¬¸ì œì…ë‹ˆë‹¤:**
- í›ˆë ¨ ë°ì´í„°ì— ì—†ë˜ ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ í™•ë¥ ì´ 0ì´ ë¨
- í•˜ë‚˜ë¼ë„ 0ì´ë©´ ì „ì²´ê°€ 0ì´ ë¨
- í˜„ì‹¤ì ì´ì§€ ì•ŠìŒ (ì •ìƒ ë©”ì¼ì—ë„ "ë¬´ë£Œ"ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)

**í•´ê²°ì±…:** ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© (Laplace Smoothing) â†’ Advanced ì„¹ì…˜ì—ì„œ ë‹¤ë£¸

## Python ì½”ë“œ ì˜ˆì œ

### 1. Naive Bayes ê¸°ë³¸ ì‚¬ìš© (Iris ë°ì´í„°)

```python
"""
Naive Bayes ê¸°ë³¸ ì˜ˆì œ: Iris ê½ƒ ë¶„ë¥˜
- Gaussian Naive Bayes ì‚¬ìš© (ì—°ì†í˜• ë°ì´í„°)
"""

from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

# 1. ë°ì´í„° ë¡œë“œ
iris = load_iris()
X = iris.data
y = iris.target

print("ë°ì´í„° í¬ê¸°:", X.shape)
print("í´ë˜ìŠ¤:", iris.target_names)
print("íŠ¹ì„±:", iris.feature_names)

# 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Gaussian Naive Bayes ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# 4. ì˜ˆì¸¡
y_pred = gnb.predict(X_test)

# 5. í‰ê°€
accuracy = accuracy_score(y_test, y_pred)
print(f"\nì •í™•ë„: {accuracy:.4f}")
print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 6. í˜¼ë™ í–‰ë ¬
print("í˜¼ë™ í–‰ë ¬:")
print(confusion_matrix(y_test, y_pred))

# 7. í™•ë¥  ì˜ˆì¸¡
proba = gnb.predict_proba(X_test[:5])
print("\nì²˜ìŒ 5ê°œ ìƒ˜í”Œì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ :")
for i, p in enumerate(proba):
    print(f"ìƒ˜í”Œ {i}: Setosa={p[0]:.3f}, Versicolor={p[1]:.3f}, Virginica={p[2]:.3f}")
    print(f"  â†’ ì˜ˆì¸¡: {iris.target_names[y_pred[i]]}, ì‹¤ì œ: {iris.target_names[y_test[i]]}")

# 8. í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥  í™•ì¸
print("\ní´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥  (í›ˆë ¨ ë°ì´í„° ê¸°ë°˜):")
for i, class_name in enumerate(iris.target_names):
    print(f"{class_name}: {gnb.class_prior_[i]:.3f}")

# 9. íŠ¹ì„±ë³„ í‰ê· ê³¼ ë¶„ì‚° (Gaussian ê°€ì •)
print("\nê° í´ë˜ìŠ¤ë³„ íŠ¹ì„± í‰ê· :")
print("(Gaussian NBëŠ” ê° íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •)")
for i, class_name in enumerate(iris.target_names):
    print(f"\n{class_name}:")
    for j, feature_name in enumerate(iris.feature_names):
        print(f"  {feature_name}: í‰ê· ={gnb.theta_[i][j]:.2f}, ë¶„ì‚°={gnb.var_[i][j]:.2f}")

"""
ì¶œë ¥ ì˜ˆì‹œ:
ë°ì´í„° í¬ê¸°: (150, 4)
í´ë˜ìŠ¤: ['setosa' 'versicolor' 'virginica']
íŠ¹ì„±: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

ì •í™•ë„: 0.9778

ë¶„ë¥˜ ë¦¬í¬íŠ¸:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      0.92      0.96        13
   virginica       0.93      1.00      0.96        13

    accuracy                           0.98        45
   macro avg       0.98      0.97      0.97        45
weighted avg       0.98      0.98      0.98        45

í˜¼ë™ í–‰ë ¬:
[[19  0  0]
 [ 0 12  1]
 [ 0  0 13]]

ì²˜ìŒ 5ê°œ ìƒ˜í”Œì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ :
ìƒ˜í”Œ 0: Setosa=1.000, Versicolor=0.000, Virginica=0.000
  â†’ ì˜ˆì¸¡: virginica, ì‹¤ì œ: virginica
ìƒ˜í”Œ 1: Setosa=1.000, Versicolor=0.000, Virginica=0.000
  â†’ ì˜ˆì¸¡: setosa, ì‹¤ì œ: setosa
...

í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥  (í›ˆë ¨ ë°ì´í„° ê¸°ë°˜):
setosa: 0.333
versicolor: 0.333
virginica: 0.333
"""
```

### 2. ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ (ë™ì˜ìƒ ì˜ˆì œ ì¬í˜„)

```python
"""
ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜: ë™ì˜ìƒ ì˜ˆì œ ì™„ì „ ì¬í˜„
- ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ ë¶„ë¥˜
- ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš© ë¹„êµ
"""

import numpy as np
from collections import defaultdict

# í›ˆë ¨ ë°ì´í„°
spam_emails = [
    "ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
    "ê¸´ê¸‰ ëŒ€ì¶œ ì‹ ì²­í•˜ì„¸ìš”",
    "ë¬´ë£Œ ì²´í—˜ ì‹ ì²­"
]

normal_emails = [
    "íšŒì˜ ì¼ì • ê³µìœ í•©ë‹ˆë‹¤",
    "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©",
    "ë‚´ì¼ ì ì‹¬ ì•½ì†"
]

# ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ê³µë°± ê¸°ì¤€)
def tokenize(text):
    return text.split()

# ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
def count_words(emails):
    word_count = defaultdict(int)
    total_words = 0
    for email in emails:
        for word in tokenize(email):
            word_count[word] += 1
            total_words += 1
    return word_count, total_words

spam_word_count, spam_total = count_words(spam_emails)
normal_word_count, normal_total = count_words(normal_emails)

print("=== í›ˆë ¨ ë°ì´í„° ë¶„ì„ ===\n")
print(f"ìŠ¤íŒ¸ ë©”ì¼ ìˆ˜: {len(spam_emails)}")
print(f"ì •ìƒ ë©”ì¼ ìˆ˜: {len(normal_emails)}")
print(f"\nìŠ¤íŒ¸ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: {spam_total}")
print(f"ì •ìƒ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: {normal_total}")

print("\n=== ìŠ¤íŒ¸ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„ ===")
for word, count in sorted(spam_word_count.items(), key=lambda x: -x[1]):
    print(f"{word}: {count}íšŒ (P={count}/{spam_total} = {count/spam_total:.3f})")

print("\n=== ì •ìƒ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„ ===")
for word, count in sorted(normal_word_count.items(), key=lambda x: -x[1]):
    print(f"{word}: {count}íšŒ (P={count}/{normal_total} = {count/normal_total:.3f})")

# ì‚¬ì „ í™•ë¥ 
P_spam = len(spam_emails) / (len(spam_emails) + len(normal_emails))
P_normal = len(normal_emails) / (len(spam_emails) + len(normal_emails))

print(f"\n=== ì‚¬ì „ í™•ë¥  ===")
print(f"P(ìŠ¤íŒ¸) = {P_spam:.3f}")
print(f"P(ì •ìƒ) = {P_normal:.3f}")

# ìƒˆë¡œìš´ ë©”ì¼ ë¶„ë¥˜ í•¨ìˆ˜ (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì—†ì´)
def classify_email(email, use_smoothing=False):
    words = tokenize(email)
    print(f"\n{'='*60}")
    print(f"ë¶„ë¥˜í•  ë©”ì¼: '{email}'")
    print(f"ë‹¨ì–´: {words}")
    print(f"ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©: {'ì‚¬ìš©' if use_smoothing else 'ë¯¸ì‚¬ìš©'}")
    print(f"{'='*60}\n")

    # ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„°
    alpha = 1 if use_smoothing else 0

    # ì–´íœ˜ í¬ê¸° (ëª¨ë“  ê³ ìœ  ë‹¨ì–´)
    vocab = set(spam_word_count.keys()) | set(normal_word_count.keys())
    vocab_size = len(vocab)

    # ìŠ¤íŒ¸ í™•ë¥  ê³„ì‚°
    log_prob_spam = np.log(P_spam)
    print("=== ìŠ¤íŒ¸ í™•ë¥  ê³„ì‚° ===")
    print(f"log P(ìŠ¤íŒ¸) = log({P_spam:.3f}) = {log_prob_spam:.4f}")

    for word in words:
        count = spam_word_count.get(word, 0)
        prob = (count + alpha) / (spam_total + alpha * vocab_size)
        log_prob = np.log(prob)
        log_prob_spam += log_prob
        print(f"  '{word}': ë¹ˆë„={count}, P={prob:.6f}, log(P)={log_prob:.4f}")

    print(f"â†’ ìµœì¢… log P(ë©”ì¼|ìŠ¤íŒ¸) Ã— P(ìŠ¤íŒ¸) = {log_prob_spam:.4f}")

    # ì •ìƒ í™•ë¥  ê³„ì‚°
    log_prob_normal = np.log(P_normal)
    print("\n=== ì •ìƒ í™•ë¥  ê³„ì‚° ===")
    print(f"log P(ì •ìƒ) = log({P_normal:.3f}) = {log_prob_normal:.4f}")

    for word in words:
        count = normal_word_count.get(word, 0)
        prob = (count + alpha) / (normal_total + alpha * vocab_size)
        log_prob = np.log(prob)
        log_prob_normal += log_prob
        print(f"  '{word}': ë¹ˆë„={count}, P={prob:.6f}, log(P)={log_prob:.4f}")

    print(f"â†’ ìµœì¢… log P(ë©”ì¼|ì •ìƒ) Ã— P(ì •ìƒ) = {log_prob_normal:.4f}")

    # ê²°ê³¼ ë¹„êµ
    print(f"\n=== ê²°ê³¼ ===")
    print(f"log P(ìŠ¤íŒ¸|ë©”ì¼) = {log_prob_spam:.4f}")
    print(f"log P(ì •ìƒ|ë©”ì¼) = {log_prob_normal:.4f}")

    if log_prob_spam > log_prob_normal:
        print(f"â†’ ìŠ¤íŒ¸ì´ {log_prob_spam - log_prob_normal:.4f} ë” ë†’ìŒ")
        print(f"â†’ ë¶„ë¥˜ ê²°ê³¼: ìŠ¤íŒ¸ âš ï¸")
        return "ìŠ¤íŒ¸"
    else:
        print(f"â†’ ì •ìƒì´ {log_prob_normal - log_prob_spam:.4f} ë” ë†’ìŒ")
        print(f"â†’ ë¶„ë¥˜ ê²°ê³¼: ì •ìƒ âœ“")
        return "ì •ìƒ"

# í…ŒìŠ¤íŠ¸ 1: "ë¬´ë£Œ ëŒ€ì¶œ ì‹ ì²­" (ìŠ¤ë¬´ë”© ì—†ì´)
result1 = classify_email("ë¬´ë£Œ ëŒ€ì¶œ ì‹ ì²­", use_smoothing=False)

# í…ŒìŠ¤íŠ¸ 2: "íšŒì˜ ì¼ì •" (ìŠ¤ë¬´ë”© ì—†ì´)
result2 = classify_email("íšŒì˜ ì¼ì •", use_smoothing=False)

# í…ŒìŠ¤íŠ¸ 3: "ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ" (ìŠ¤ë¬´ë”© ì—†ì´ - ë¬¸ì œ ë°œìƒ!)
print("\n" + "="*60)
print("âš ï¸ 'ì˜¤ëŠ˜'ì´ë¼ëŠ” ë‹¨ì–´ëŠ” í›ˆë ¨ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
print("ì •ìƒ ë©”ì¼ì—ì„œ í™•ë¥ ì´ 0ì´ ë˜ëŠ” ë¬¸ì œ ë°œìƒ ì˜ˆìƒ")
print("="*60)
result3 = classify_email("ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ", use_smoothing=False)

"""
ì¶œë ¥ ì˜ˆì‹œ:
=== í›ˆë ¨ ë°ì´í„° ë¶„ì„ ===

ìŠ¤íŒ¸ ë©”ì¼ ìˆ˜: 3
ì •ìƒ ë©”ì¼ ìˆ˜: 3

ìŠ¤íŒ¸ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: 9
ì •ìƒ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: 9

=== ìŠ¤íŒ¸ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„ ===
ë¬´ë£Œ: 2íšŒ (P=2/9 = 0.222)
ëŒ€ì¶œ: 2íšŒ (P=2/9 = 0.222)
ì‹ ì²­: 2íšŒ (P=2/9 = 0.222)
ê¸´ê¸‰: 1íšŒ (P=1/9 = 0.111)
ê°€ëŠ¥í•©ë‹ˆë‹¤: 1íšŒ (P=1/9 = 0.111)
ì‹ ì²­í•˜ì„¸ìš”: 1íšŒ (P=1/9 = 0.111)
ì²´í—˜: 1íšŒ (P=1/9 = 0.111)

=== ì •ìƒ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„ ===
íšŒì˜: 1íšŒ (P=1/9 = 0.111)
ì¼ì •: 1íšŒ (P=1/9 = 0.111)
ê³µìœ í•©ë‹ˆë‹¤: 1íšŒ (P=1/9 = 0.111)
í”„ë¡œì íŠ¸: 1íšŒ (P=1/9 = 0.111)
ì§„í–‰: 1íšŒ (P=1/9 = 0.111)
ìƒí™©: 1íšŒ (P=1/9 = 0.111)
ë‚´ì¼: 1íšŒ (P=1/9 = 0.111)
ì ì‹¬: 1íšŒ (P=1/9 = 0.111)
ì•½ì†: 1íšŒ (P=1/9 = 0.111)

=== ì‚¬ì „ í™•ë¥  ===
P(ìŠ¤íŒ¸) = 0.500
P(ì •ìƒ) = 0.500

============================================================
ë¶„ë¥˜í•  ë©”ì¼: 'ë¬´ë£Œ ëŒ€ì¶œ ì‹ ì²­'
ë‹¨ì–´: ['ë¬´ë£Œ', 'ëŒ€ì¶œ', 'ì‹ ì²­']
ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©: ë¯¸ì‚¬ìš©
============================================================

=== ìŠ¤íŒ¸ í™•ë¥  ê³„ì‚° ===
log P(ìŠ¤íŒ¸) = log(0.500) = -0.6931
  'ë¬´ë£Œ': ë¹ˆë„=2, P=0.222222, log(P)=-1.5041
  'ëŒ€ì¶œ': ë¹ˆë„=2, P=0.222222, log(P)=-1.5041
  'ì‹ ì²­': ë¹ˆë„=2, P=0.222222, log(P)=-1.5041
â†’ ìµœì¢… log P(ë©”ì¼|ìŠ¤íŒ¸) Ã— P(ìŠ¤íŒ¸) = -6.2095

=== ì •ìƒ í™•ë¥  ê³„ì‚° ===
log P(ì •ìƒ) = log(0.500) = -0.6931
  'ë¬´ë£Œ': ë¹ˆë„=0, P=0.000000, log(P)=-inf
  'ëŒ€ì¶œ': ë¹ˆë„=0, P=0.000000, log(P)=-inf
  'ì‹ ì²­': ë¹ˆë„=0, P=0.000000, log(P)=-inf
â†’ ìµœì¢… log P(ë©”ì¼|ì •ìƒ) Ã— P(ì •ìƒ) = -inf

=== ê²°ê³¼ ===
log P(ìŠ¤íŒ¸|ë©”ì¼) = -6.2095
log P(ì •ìƒ|ë©”ì¼) = -inf
â†’ ìŠ¤íŒ¸ì´ inf ë” ë†’ìŒ
â†’ ë¶„ë¥˜ ê²°ê³¼: ìŠ¤íŒ¸ âš ï¸
"""
```

### 3. Multinomial Naive Bayes (í…ìŠ¤íŠ¸ ë¶„ë¥˜)

```python
"""
Multinomial Naive Bayes: í…ìŠ¤íŠ¸ ë¶„ë¥˜
- ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ (Bag of Words)
- CountVectorizer ì‚¬ìš©
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°
texts = [
    "ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
    "ê¸´ê¸‰ ëŒ€ì¶œ ì‹ ì²­í•˜ì„¸ìš”",
    "ë¬´ë£Œ ì²´í—˜ ì‹ ì²­",
    "íŠ¹ë³„ í• ì¸ ì´ë²¤íŠ¸",
    "ê³µì§œ ì¿ í° ë°›ìœ¼ì„¸ìš”",
    "íšŒì˜ ì¼ì • ê³µìœ í•©ë‹ˆë‹¤",
    "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©",
    "ë‚´ì¼ ì ì‹¬ ì•½ì†",
    "ì—…ë¬´ ë³´ê³ ì„œ ì œì¶œ",
    "íŒ€ íšŒì‹ ê³µì§€",
]

labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1=ìŠ¤íŒ¸, 0=ì •ìƒ

# 1. í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

print("=== ë°ì´í„° ë²¡í„°í™” ===")
print(f"íŠ¹ì„± ìˆ˜ (ê³ ìœ  ë‹¨ì–´ ìˆ˜): {len(vectorizer.get_feature_names_out())}")
print(f"ë‹¨ì–´ ëª©ë¡: {vectorizer.get_feature_names_out()}")
print(f"\nì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë²¡í„°:")
print(X[0].toarray())
print(f"â†’ ê° ìœ„ì¹˜ëŠ” í•´ë‹¹ ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜")

# 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.3, random_state=42
)

# 3. Multinomial Naive Bayes í•™ìŠµ
mnb = MultinomialNB(alpha=1.0)  # alpha=1.0ì€ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©
mnb.fit(X_train, y_train)

# 4. ì˜ˆì¸¡
y_pred = mnb.predict(X_test)

# 5. í‰ê°€
accuracy = accuracy_score(y_test, y_pred)
print(f"\nì •í™•ë„: {accuracy:.4f}")

# 6. ìƒˆë¡œìš´ ë©”ì¼ ë¶„ë¥˜
new_emails = [
    "ë¬´ë£Œ ì¿ í° ì´ë²¤íŠ¸",
    "íšŒì˜ ì¼ì • ë³€ê²½",
    "ëŒ€ì¶œ ì‹ ì²­ ë°©ë²•"
]

new_emails_vec = vectorizer.transform(new_emails)
predictions = mnb.predict(new_emails_vec)
probas = mnb.predict_proba(new_emails_vec)

print("\n=== ìƒˆë¡œìš´ ë©”ì¼ ë¶„ë¥˜ ===")
for email, pred, proba in zip(new_emails, predictions, probas):
    label = "ìŠ¤íŒ¸" if pred == 1 else "ì •ìƒ"
    print(f"\n'{email}'")
    print(f"  â†’ {label} (ì •ìƒ: {proba[0]:.2%}, ìŠ¤íŒ¸: {proba[1]:.2%})")

# 7. ê° í´ë˜ìŠ¤ë³„ ë‹¨ì–´ í™•ë¥  ìƒìœ„ 5ê°œ
print("\n=== ìŠ¤íŒ¸ ë©”ì¼ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ TOP 5 ===")
feature_names = vectorizer.get_feature_names_out()
spam_log_probs = mnb.feature_log_prob_[1]  # í´ë˜ìŠ¤ 1 (ìŠ¤íŒ¸)
top_spam_indices = spam_log_probs.argsort()[-5:][::-1]

for idx in top_spam_indices:
    print(f"{feature_names[idx]}: {np.exp(spam_log_probs[idx]):.4f}")

print("\n=== ì •ìƒ ë©”ì¼ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ TOP 5 ===")
normal_log_probs = mnb.feature_log_prob_[0]  # í´ë˜ìŠ¤ 0 (ì •ìƒ)
top_normal_indices = normal_log_probs.argsort()[-5:][::-1]

for idx in top_normal_indices:
    print(f"{feature_names[idx]}: {np.exp(normal_log_probs[idx]):.4f}")

"""
ì¶œë ¥ ì˜ˆì‹œ:
=== ë°ì´í„° ë²¡í„°í™” ===
íŠ¹ì„± ìˆ˜ (ê³ ìœ  ë‹¨ì–´ ìˆ˜): 23
ë‹¨ì–´ ëª©ë¡: ['ê°€ëŠ¥í•©ë‹ˆë‹¤' 'ê³µìœ í•©ë‹ˆë‹¤' 'ê³µì§œ' 'ê³µì§€' 'ë‚´ì¼' ...]

ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë²¡í„°:
[[1 0 0 0 0 1 0 0 0 1 0 0 ...]]
â†’ ê° ìœ„ì¹˜ëŠ” í•´ë‹¹ ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜

ì •í™•ë„: 1.0000

=== ìƒˆë¡œìš´ ë©”ì¼ ë¶„ë¥˜ ===

'ë¬´ë£Œ ì¿ í° ì´ë²¤íŠ¸'
  â†’ ìŠ¤íŒ¸ (ì •ìƒ: 12.34%, ìŠ¤íŒ¸: 87.66%)

'íšŒì˜ ì¼ì • ë³€ê²½'
  â†’ ì •ìƒ (ì •ìƒ: 78.45%, ìŠ¤íŒ¸: 21.55%)

'ëŒ€ì¶œ ì‹ ì²­ ë°©ë²•'
  â†’ ìŠ¤íŒ¸ (ì •ìƒ: 23.12%, ìŠ¤íŒ¸: 76.88%)

=== ìŠ¤íŒ¸ ë©”ì¼ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ TOP 5 ===
ë¬´ë£Œ: 0.1250
ëŒ€ì¶œ: 0.1250
ì‹ ì²­: 0.1250
ì´ë²¤íŠ¸: 0.0833
í• ì¸: 0.0833
"""
```

### 4. Bernoulli Naive Bayes (ì´ì§„ íŠ¹ì„±)

```python
"""
Bernoulli Naive Bayes: ì´ì§„ íŠ¹ì„± (ë‹¨ì–´ ì¡´ì¬ ì—¬ë¶€)
- ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ê°€ ì•„ë‹Œ ì¡´ì¬ ì—¬ë¶€ë§Œ ì‚¬ìš©
- ì§§ì€ ë¬¸ì„œë‚˜ ì´ì§„ íŠ¹ì„±ì— íš¨ê³¼ì 
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# ê°™ì€ ë°ì´í„° ì‚¬ìš©
texts = [
    "ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥",
    "ê¸´ê¸‰ ëŒ€ì¶œ ì‹ ì²­",
    "ë¬´ë£Œ ì²´í—˜",
    "íŠ¹ë³„ í• ì¸",
    "ê³µì§œ ì¿ í°",
    "íšŒì˜ ì¼ì •",
    "í”„ë¡œì íŠ¸ ì§„í–‰",
    "ë‚´ì¼ ì ì‹¬",
    "ì—…ë¬´ ë³´ê³ ì„œ",
    "íŒ€ íšŒì‹",
]

labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1=ìŠ¤íŒ¸, 0=ì •ìƒ

# 1. ë²¡í„°í™” (ì´ì§„ìœ¼ë¡œ ë³€í™˜)
vectorizer = CountVectorizer(binary=True)  # binary=True!
X = vectorizer.fit_transform(texts)

print("=== Bernoulli NB: ì´ì§„ íŠ¹ì„± ===")
print("ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ê°€ ì•„ë‹Œ ì¡´ì¬ ì—¬ë¶€ë§Œ ì‚¬ìš©\n")
print("ì²« ë²ˆì§¸ ë¬¸ì„œ 'ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥':")
print(f"ë²¡í„°: {X[0].toarray()[0]}")
print("â†’ 1 = ë‹¨ì–´ ì¡´ì¬, 0 = ë‹¨ì–´ ì—†ìŒ")

# 2. í•™ìŠµ
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.3, random_state=42
)

bnb = BernoulliNB(alpha=1.0)
bnb.fit(X_train, y_train)

# 3. ì˜ˆì¸¡
y_pred = bnb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nì •í™•ë„: {accuracy:.4f}")

# 4. Multinomial vs Bernoulli ë¹„êµ
print("\n=== Multinomial NB vs Bernoulli NB ===")

# ë‹¨ì–´ê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°
test_cases = [
    ("ë¬´ë£Œ ë¬´ë£Œ ë¬´ë£Œ ëŒ€ì¶œ ëŒ€ì¶œ", "ë‹¨ì–´ ë°˜ë³µ ë§ìŒ"),
    ("ë¬´ë£Œ ëŒ€ì¶œ", "ë‹¨ì–´ ë°˜ë³µ ì—†ìŒ")
]

# Multinomial NB (ë¹ˆë„ ê³ ë ¤)
from sklearn.naive_bayes import MultinomialNB
vectorizer_multi = CountVectorizer(binary=False)
X_multi = vectorizer_multi.fit_transform(texts)
mnb = MultinomialNB(alpha=1.0)
mnb.fit(X_multi[:7], labels[:7])  # ê°„ë‹¨íˆ ì¼ë¶€ë§Œ í•™ìŠµ

# Bernoulli NB (ì¡´ì¬ ì—¬ë¶€ë§Œ)
vectorizer_bern = CountVectorizer(binary=True)
X_bern = vectorizer_bern.fit_transform(texts)
bnb = BernoulliNB(alpha=1.0)
bnb.fit(X_bern[:7], labels[:7])

for text, desc in test_cases:
    print(f"\ní…ŒìŠ¤íŠ¸: '{text}' ({desc})")

    # Multinomial
    vec_multi = vectorizer_multi.transform([text])
    prob_multi = mnb.predict_proba(vec_multi)[0]
    print(f"  Multinomial NB: ì •ìƒ={prob_multi[0]:.2%}, ìŠ¤íŒ¸={prob_multi[1]:.2%}")
    print(f"  â†’ ë¹ˆë„ ë²¡í„°: {vec_multi.toarray()[0]}")

    # Bernoulli
    vec_bern = vectorizer_bern.transform([text])
    prob_bern = bnb.predict_proba(vec_bern)[0]
    print(f"  Bernoulli NB: ì •ìƒ={prob_bern[0]:.2%}, ìŠ¤íŒ¸={prob_bern[1]:.2%}")
    print(f"  â†’ ì´ì§„ ë²¡í„°: {vec_bern.toarray()[0]}")

    print(f"  ğŸ“Œ Multinomialì€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ê³ ë ¤, BernoulliëŠ” ë¬´ì‹œ")

"""
ì¶œë ¥ ì˜ˆì‹œ:
=== Bernoulli NB: ì´ì§„ íŠ¹ì„± ===
ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ê°€ ì•„ë‹Œ ì¡´ì¬ ì—¬ë¶€ë§Œ ì‚¬ìš©

ì²« ë²ˆì§¸ ë¬¸ì„œ 'ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥':
ë²¡í„°: [1 1 0 0 0 1 0 0 0 0 0]
â†’ 1 = ë‹¨ì–´ ì¡´ì¬, 0 = ë‹¨ì–´ ì—†ìŒ

ì •í™•ë„: 1.0000

=== Multinomial NB vs Bernoulli NB ===

í…ŒìŠ¤íŠ¸: 'ë¬´ë£Œ ë¬´ë£Œ ë¬´ë£Œ ëŒ€ì¶œ ëŒ€ì¶œ' (ë‹¨ì–´ ë°˜ë³µ ë§ìŒ)
  Multinomial NB: ì •ìƒ=5.23%, ìŠ¤íŒ¸=94.77%
  â†’ ë¹ˆë„ ë²¡í„°: [0 3 2 0 0 0 0 ...]
  Bernoulli NB: ì •ìƒ=12.45%, ìŠ¤íŒ¸=87.55%
  â†’ ì´ì§„ ë²¡í„°: [0 1 1 0 0 0 0 ...]
  ğŸ“Œ Multinomialì€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ê³ ë ¤, BernoulliëŠ” ë¬´ì‹œ

í…ŒìŠ¤íŠ¸: 'ë¬´ë£Œ ëŒ€ì¶œ' (ë‹¨ì–´ ë°˜ë³µ ì—†ìŒ)
  Multinomial NB: ì •ìƒ=15.34%, ìŠ¤íŒ¸=84.66%
  â†’ ë¹ˆë„ ë²¡í„°: [0 1 1 0 0 0 0 ...]
  Bernoulli NB: ì •ìƒ=12.45%, ìŠ¤íŒ¸=87.55%
  â†’ ì´ì§„ ë²¡í„°: [0 1 1 0 0 0 0 ...]
  ğŸ“Œ Multinomialì€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ê³ ë ¤, BernoulliëŠ” ë¬´ì‹œ
"""
```

# Advanced

## ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© (Laplace Smoothing)

### ë¬¸ì œ ìƒí™©

Basic ì„¹ì…˜ì˜ "ë¬´ë£Œ ëŒ€ì¶œ ì‹ ì²­" ì˜ˆì œì—ì„œ ë´¤ë“¯ì´:

```
ì •ìƒ ë©”ì¼ í›ˆë ¨ ë°ì´í„°ì— "ë¬´ë£Œ"ê°€ ì—†ìŒ
â†’ P(ë¬´ë£Œ | ì •ìƒ) = 0/9 = 0
â†’ P(ë¬´ë£Œ, ëŒ€ì¶œ, ì‹ ì²­ | ì •ìƒ) = 0 Ã— ... = 0
â†’ ë¶„ë¥˜ ë¶ˆê°€ëŠ¥!
```

**ë” ì‹¬ê°í•œ ì˜ˆ: "ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ"**

```
ìŠ¤íŒ¸ ë©”ì¼:
- "ì˜¤ëŠ˜": 0íšŒ â†’ P(ì˜¤ëŠ˜ | ìŠ¤íŒ¸) = 0/9 = 0
- "ê¸´ê¸‰": 1íšŒ â†’ P(ê¸´ê¸‰ | ìŠ¤íŒ¸) = 1/9
- "ëŒ€ì¶œ": 2íšŒ â†’ P(ëŒ€ì¶œ | ìŠ¤íŒ¸) = 2/9

P(ì˜¤ëŠ˜, ê¸´ê¸‰, ëŒ€ì¶œ | ìŠ¤íŒ¸) = 0 Ã— (1/9) Ã— (2/9) = 0  â† ë¬¸ì œ!

ì •ìƒ ë©”ì¼:
- ëª¨ë“  ë‹¨ì–´ê°€ 0íšŒ â†’ ì „ì²´ í™•ë¥  = 0

â†’ ë‘˜ ë‹¤ 0ì´ë©´ ì–´ë–»ê²Œ ë¶„ë¥˜í•˜ë‚˜?
```

### ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©ì´ë€?

**í•µì‹¬ ì•„ì´ë””ì–´:**
```
"í•œ ë²ˆë„ ì•ˆ ë‚˜íƒ€ë‚œ ë‹¨ì–´ë„ ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ ì¡°ê¸ˆì€ ìˆë‹¤"
```

**ë°©ë²•:**
```
ëª¨ë“  ë‹¨ì–´ì˜ ë¹ˆë„ì— 1ì„ ë”í•˜ê¸° (ë˜ëŠ” Î±ë¥¼ ë”í•˜ê¸°)

ê¸°ì¡´: P(ë‹¨ì–´ | í´ë˜ìŠ¤) = count / total
ìŠ¤ë¬´ë”©: P(ë‹¨ì–´ | í´ë˜ìŠ¤) = (count + Î±) / (total + Î± Ã— ì–´íœ˜í¬ê¸°)

ì¼ë°˜ì ìœ¼ë¡œ Î± = 1 (Laplace Smoothing)
ë‹¤ë¥¸ ê°’ë„ ê°€ëŠ¥ (Additive Smoothing)
```

### ë™ì˜ìƒ ì˜ˆì œ: "ì˜¤ëŠ˜ ê¸ê¸‰ ëŒ€ì¶œ" with Smoothing

**í›ˆë ¨ ë°ì´í„° ë³µìŠµ:**

ìŠ¤íŒ¸ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„:
```
ë¬´ë£Œ: 2íšŒ, ëŒ€ì¶œ: 2íšŒ, ì‹ ì²­: 2íšŒ, ê¸´ê¸‰: 1íšŒ, ê°€ëŠ¥: 1íšŒ, ì²´í—˜: 1íšŒ
ì´ ë‹¨ì–´ ìˆ˜: 9ê°œ
```

ì •ìƒ ë©”ì¼ ë‹¨ì–´ ë¹ˆë„:
```
íšŒì˜: 1íšŒ, ì¼ì •: 1íšŒ, ê³µìœ : 1íšŒ, í”„ë¡œì íŠ¸: 1íšŒ, ì§„í–‰: 1íšŒ,
ìƒí™©: 1íšŒ, ë‚´ì¼: 1íšŒ, ì ì‹¬: 1íšŒ, ì•½ì†: 1íšŒ
ì´ ë‹¨ì–´ ìˆ˜: 9ê°œ
```

ì „ì²´ ì–´íœ˜ í¬ê¸°:
```
{ë¬´ë£Œ, ëŒ€ì¶œ, ì‹ ì²­, ê¸´ê¸‰, ê°€ëŠ¥, ì²´í—˜, íšŒì˜, ì¼ì •, ê³µìœ ,
 í”„ë¡œì íŠ¸, ì§„í–‰, ìƒí™©, ë‚´ì¼, ì ì‹¬, ì•½ì†, ì˜¤ëŠ˜} = 16ê°œ
```

**ìŠ¤ë¬´ë”© ì—†ì´:**

```
ìŠ¤íŒ¸ì¼ ë•Œ:
P(ì˜¤ëŠ˜ | ìŠ¤íŒ¸) = 0/9 = 0  â† ë¬¸ì œ!
P(ê¸´ê¸‰ | ìŠ¤íŒ¸) = 1/9 â‰ˆ 0.111
P(ëŒ€ì¶œ | ìŠ¤íŒ¸) = 2/9 â‰ˆ 0.222

P(ì˜¤ëŠ˜, ê¸´ê¸‰, ëŒ€ì¶œ | ìŠ¤íŒ¸) = 0 Ã— 0.111 Ã— 0.222 = 0

ì •ìƒì¼ ë•Œ:
P(ì˜¤ëŠ˜ | ì •ìƒ) = 0/9 = 0
P(ê¸´ê¸‰ | ì •ìƒ) = 0/9 = 0
P(ëŒ€ì¶œ | ì •ìƒ) = 0/9 = 0

P(ì˜¤ëŠ˜, ê¸´ê¸‰, ëŒ€ì¶œ | ì •ìƒ) = 0 Ã— 0 Ã— 0 = 0

â†’ ë‘˜ ë‹¤ 0! ë¶„ë¥˜ ë¶ˆê°€!
```

**ìŠ¤ë¬´ë”© ì ìš© (Î±=1):**

```
ìŠ¤íŒ¸ì¼ ë•Œ:
P(ì˜¤ëŠ˜ | ìŠ¤íŒ¸) = (0+1) / (9+1Ã—16) = 1/25 = 0.04
P(ê¸´ê¸‰ | ìŠ¤íŒ¸) = (1+1) / (9+1Ã—16) = 2/25 = 0.08
P(ëŒ€ì¶œ | ìŠ¤íŒ¸) = (2+1) / (9+1Ã—16) = 3/25 = 0.12

P(ì˜¤ëŠ˜, ê¸´ê¸‰, ëŒ€ì¶œ | ìŠ¤íŒ¸) = 0.04 Ã— 0.08 Ã— 0.12 â‰ˆ 0.000384

ì‚¬ì „ í™•ë¥  í¬í•¨:
P(ìŠ¤íŒ¸ | ë©”ì¼) âˆ 0.000384 Ã— 0.5 = 0.000192

ì •ìƒì¼ ë•Œ:
P(ì˜¤ëŠ˜ | ì •ìƒ) = (0+1) / (9+1Ã—16) = 1/25 = 0.04
P(ê¸´ê¸‰ | ì •ìƒ) = (0+1) / (9+1Ã—16) = 1/25 = 0.04
P(ëŒ€ì¶œ | ì •ìƒ) = (0+1) / (9+1Ã—16) = 1/25 = 0.04

P(ì˜¤ëŠ˜, ê¸´ê¸‰, ëŒ€ì¶œ | ì •ìƒ) = 0.04 Ã— 0.04 Ã— 0.04 = 0.000064

ì‚¬ì „ í™•ë¥  í¬í•¨:
P(ì •ìƒ | ë©”ì¼) âˆ 0.000064 Ã— 0.5 = 0.000032

ë¹„êµ:
P(ìŠ¤íŒ¸) = 0.000192 > P(ì •ìƒ) = 0.000032
â†’ ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜! âœ“
```

**íš¨ê³¼:**
- 0 í™•ë¥  ë¬¸ì œ í•´ê²°
- ìƒˆë¡œìš´ ë‹¨ì–´ì—ë„ ì‘ì€ í™•ë¥  ë¶€ì—¬
- ê¸°ì¡´ íŒ¨í„´ì€ ì—¬ì „íˆ ìœ ì§€ (ìŠ¤íŒ¸ì— ë§ì´ ë‚˜íƒ€ë‚œ ë‹¨ì–´ëŠ” ì—¬ì „íˆ ë†’ì€ í™•ë¥ )

## ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© Python êµ¬í˜„

### 1. ìŠ¤ë¬´ë”© ì ìš© ì „/í›„ ë¹„êµ

```python
"""
ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©: ë™ì˜ìƒ ì˜ˆì œ ì™„ì „ ì¬í˜„
- "ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ" ë¶„ë¥˜
- ìŠ¤ë¬´ë”© ì „í›„ ë¹„êµ
"""

import numpy as np
from collections import defaultdict

# í›ˆë ¨ ë°ì´í„° (ì´ì „ê³¼ ë™ì¼)
spam_emails = [
    "ë¬´ë£Œ ëŒ€ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤",
    "ê¸´ê¸‰ ëŒ€ì¶œ ì‹ ì²­í•˜ì„¸ìš”",
    "ë¬´ë£Œ ì²´í—˜ ì‹ ì²­"
]

normal_emails = [
    "íšŒì˜ ì¼ì • ê³µìœ í•©ë‹ˆë‹¤",
    "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©",
    "ë‚´ì¼ ì ì‹¬ ì•½ì†"
]

def tokenize(text):
    return text.split()

def count_words(emails):
    word_count = defaultdict(int)
    total_words = 0
    for email in emails:
        for word in tokenize(email):
            word_count[word] += 1
            total_words += 1
    return word_count, total_words

spam_word_count, spam_total = count_words(spam_emails)
normal_word_count, normal_total = count_words(normal_emails)

# ì „ì²´ ì–´íœ˜
vocab = set(spam_word_count.keys()) | set(normal_word_count.keys())
vocab_size = len(vocab)

print("=== í›ˆë ¨ ë°ì´í„° ì •ë³´ ===")
print(f"ìŠ¤íŒ¸ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: {spam_total}")
print(f"ì •ìƒ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: {normal_total}")
print(f"ì „ì²´ ì–´íœ˜ í¬ê¸°: {vocab_size}")
print(f"ì–´íœ˜: {sorted(vocab)}")

# ì‚¬ì „ í™•ë¥ 
P_spam = len(spam_emails) / (len(spam_emails) + len(normal_emails))
P_normal = 1 - P_spam

def classify_with_comparison(email):
    words = tokenize(email)
    print(f"\n{'='*70}")
    print(f"ë¶„ë¥˜í•  ë©”ì¼: '{email}'")
    print(f"ë‹¨ì–´: {words}")
    print(f"{'='*70}")

    # === ìŠ¤ë¬´ë”© ì—†ì´ ===
    print("\nã€ 1. ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì—†ì´ (Î±=0) ã€‘\n")

    print("--- ìŠ¤íŒ¸ í™•ë¥  ---")
    prob_spam_no_smooth = P_spam
    for word in words:
        count = spam_word_count.get(word, 0)
        prob = count / spam_total if spam_total > 0 else 0
        prob_spam_no_smooth *= prob
        print(f"  P({word} | ìŠ¤íŒ¸) = {count}/{spam_total} = {prob:.6f}")
        if prob == 0:
            print(f"    âš ï¸ í™•ë¥ ì´ 0! ì „ì²´ê°€ 0ì´ ë©ë‹ˆë‹¤!")
    print(f"P(ìŠ¤íŒ¸) Ã— âˆP(ë‹¨ì–´|ìŠ¤íŒ¸) = {P_spam:.3f} Ã— ... = {prob_spam_no_smooth:.10f}")

    print("\n--- ì •ìƒ í™•ë¥  ---")
    prob_normal_no_smooth = P_normal
    for word in words:
        count = normal_word_count.get(word, 0)
        prob = count / normal_total if normal_total > 0 else 0
        prob_normal_no_smooth *= prob
        print(f"  P({word} | ì •ìƒ) = {count}/{normal_total} = {prob:.6f}")
        if prob == 0:
            print(f"    âš ï¸ í™•ë¥ ì´ 0! ì „ì²´ê°€ 0ì´ ë©ë‹ˆë‹¤!")
    print(f"P(ì •ìƒ) Ã— âˆP(ë‹¨ì–´|ì •ìƒ) = {P_normal:.3f} Ã— ... = {prob_normal_no_smooth:.10f}")

    if prob_spam_no_smooth == 0 and prob_normal_no_smooth == 0:
        print("\nâš ï¸ ê²°ê³¼: ë‘˜ ë‹¤ 0! ë¶„ë¥˜ ë¶ˆê°€ëŠ¥!")
        result_no_smooth = "ë¶„ë¥˜ ë¶ˆê°€"
    elif prob_spam_no_smooth > prob_normal_no_smooth:
        print(f"\nâ†’ ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜ (ìŠ¤íŒ¸ í™•ë¥  ë” ë†’ìŒ)")
        result_no_smooth = "ìŠ¤íŒ¸"
    else:
        print(f"\nâ†’ ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ (ì •ìƒ í™•ë¥  ë” ë†’ìŒ)")
        result_no_smooth = "ì •ìƒ"

    # === ìŠ¤ë¬´ë”© ì ìš© ===
    print(f"\n{'='*70}")
    print("ã€ 2. ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš© (Î±=1) ã€‘\n")

    alpha = 1

    print("--- ìŠ¤íŒ¸ í™•ë¥  ---")
    log_prob_spam_smooth = np.log(P_spam)
    print(f"log P(ìŠ¤íŒ¸) = {log_prob_spam_smooth:.4f}")

    for word in words:
        count = spam_word_count.get(word, 0)
        prob = (count + alpha) / (spam_total + alpha * vocab_size)
        log_prob = np.log(prob)
        log_prob_spam_smooth += log_prob
        print(f"  P({word} | ìŠ¤íŒ¸) = ({count}+1) / ({spam_total}+1Ã—{vocab_size}) = {prob:.6f}")
        print(f"    log(P) = {log_prob:.4f}")

    print(f"ì´ log í™•ë¥  = {log_prob_spam_smooth:.4f}")
    prob_spam_smooth = np.exp(log_prob_spam_smooth)
    print(f"ì‹¤ì œ í™•ë¥  = exp({log_prob_spam_smooth:.4f}) = {prob_spam_smooth:.10e}")

    print("\n--- ì •ìƒ í™•ë¥  ---")
    log_prob_normal_smooth = np.log(P_normal)
    print(f"log P(ì •ìƒ) = {log_prob_normal_smooth:.4f}")

    for word in words:
        count = normal_word_count.get(word, 0)
        prob = (count + alpha) / (normal_total + alpha * vocab_size)
        log_prob = np.log(prob)
        log_prob_normal_smooth += log_prob
        print(f"  P({word} | ì •ìƒ) = ({count}+1) / ({normal_total}+1Ã—{vocab_size}) = {prob:.6f}")
        print(f"    log(P) = {log_prob:.4f}")

    print(f"ì´ log í™•ë¥  = {log_prob_normal_smooth:.4f}")
    prob_normal_smooth = np.exp(log_prob_normal_smooth)
    print(f"ì‹¤ì œ í™•ë¥  = exp({log_prob_normal_smooth:.4f}) = {prob_normal_smooth:.10e}")

    print(f"\n--- ë¹„êµ ---")
    print(f"log P(ìŠ¤íŒ¸|ë©”ì¼) = {log_prob_spam_smooth:.4f}")
    print(f"log P(ì •ìƒ|ë©”ì¼) = {log_prob_normal_smooth:.4f}")

    if log_prob_spam_smooth > log_prob_normal_smooth:
        diff = log_prob_spam_smooth - log_prob_normal_smooth
        print(f"â†’ ìŠ¤íŒ¸ì´ {diff:.4f} ë” ë†’ìŒ (log ìŠ¤ì¼€ì¼)")
        result_smooth = "ìŠ¤íŒ¸"
    else:
        diff = log_prob_normal_smooth - log_prob_spam_smooth
        print(f"â†’ ì •ìƒì´ {diff:.4f} ë” ë†’ìŒ (log ìŠ¤ì¼€ì¼)")
        result_smooth = "ì •ìƒ"

    # === ê²°ê³¼ ìš”ì•½ ===
    print(f"\n{'='*70}")
    print("ã€ ê²°ê³¼ ìš”ì•½ ã€‘")
    print(f"{'='*70}")
    print(f"ìŠ¤ë¬´ë”© ì—†ì´: {result_no_smooth}")
    print(f"ìŠ¤ë¬´ë”© ì ìš©: {result_smooth} âœ“")
    print(f"\nğŸ’¡ ìŠ¤ë¬´ë”©ì„ ì ìš©í•˜ë©´ 0 í™•ë¥  ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

    return result_smooth

# í…ŒìŠ¤íŠ¸: "ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ" (ë™ì˜ìƒ ì˜ˆì œ)
classify_with_comparison("ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ")

"""
ì¶œë ¥ ì˜ˆì‹œ:
=== í›ˆë ¨ ë°ì´í„° ì •ë³´ ===
ìŠ¤íŒ¸ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: 9
ì •ìƒ ë©”ì¼ ì´ ë‹¨ì–´ ìˆ˜: 9
ì „ì²´ ì–´íœ˜ í¬ê¸°: 16
ì–´íœ˜: ['ê°€ëŠ¥í•©ë‹ˆë‹¤', 'ê³µìœ í•©ë‹ˆë‹¤', 'ê¸´ê¸‰', 'ë‚´ì¼', 'ëŒ€ì¶œ', ...]

======================================================================
ë¶„ë¥˜í•  ë©”ì¼: 'ì˜¤ëŠ˜ ê¸´ê¸‰ ëŒ€ì¶œ'
ë‹¨ì–´: ['ì˜¤ëŠ˜', 'ê¸´ê¸‰', 'ëŒ€ì¶œ']
======================================================================

ã€ 1. ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì—†ì´ (Î±=0) ã€‘

--- ìŠ¤íŒ¸ í™•ë¥  ---
  P(ì˜¤ëŠ˜ | ìŠ¤íŒ¸) = 0/9 = 0.000000
    âš ï¸ í™•ë¥ ì´ 0! ì „ì²´ê°€ 0ì´ ë©ë‹ˆë‹¤!
  P(ê¸´ê¸‰ | ìŠ¤íŒ¸) = 1/9 = 0.111111
  P(ëŒ€ì¶œ | ìŠ¤íŒ¸) = 2/9 = 0.222222
P(ìŠ¤íŒ¸) Ã— âˆP(ë‹¨ì–´|ìŠ¤íŒ¸) = 0.500 Ã— ... = 0.0000000000

--- ì •ìƒ í™•ë¥  ---
  P(ì˜¤ëŠ˜ | ì •ìƒ) = 0/9 = 0.000000
    âš ï¸ í™•ë¥ ì´ 0! ì „ì²´ê°€ 0ì´ ë©ë‹ˆë‹¤!
  P(ê¸´ê¸‰ | ì •ìƒ) = 0/9 = 0.000000
    âš ï¸ í™•ë¥ ì´ 0! ì „ì²´ê°€ 0ì´ ë©ë‹ˆë‹¤!
  P(ëŒ€ì¶œ | ì •ìƒ) = 0/9 = 0.000000
    âš ï¸ í™•ë¥ ì´ 0! ì „ì²´ê°€ 0ì´ ë©ë‹ˆë‹¤!
P(ì •ìƒ) Ã— âˆP(ë‹¨ì–´|ì •ìƒ) = 0.500 Ã— ... = 0.0000000000

âš ï¸ ê²°ê³¼: ë‘˜ ë‹¤ 0! ë¶„ë¥˜ ë¶ˆê°€ëŠ¥!

======================================================================
ã€ 2. ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš© (Î±=1) ã€‘

--- ìŠ¤íŒ¸ í™•ë¥  ---
log P(ìŠ¤íŒ¸) = -0.6931
  P(ì˜¤ëŠ˜ | ìŠ¤íŒ¸) = (0+1) / (9+1Ã—16) = 0.040000
    log(P) = -3.2189
  P(ê¸´ê¸‰ | ìŠ¤íŒ¸) = (1+1) / (9+1Ã—16) = 0.080000
    log(P) = -2.5257
  P(ëŒ€ì¶œ | ìŠ¤íŒ¸) = (2+1) / (9+1Ã—16) = 0.120000
    log(P) = -2.1203
ì´ log í™•ë¥  = -8.5580
ì‹¤ì œ í™•ë¥  = exp(-8.5580) = 1.9200000e-04

--- ì •ìƒ í™•ë¥  ---
log P(ì •ìƒ) = -0.6931
  P(ì˜¤ëŠ˜ | ì •ìƒ) = (0+1) / (9+1Ã—16) = 0.040000
    log(P) = -3.2189
  P(ê¸´ê¸‰ | ì •ìƒ) = (0+1) / (9+1Ã—16) = 0.040000
    log(P) = -3.2189
  P(ëŒ€ì¶œ | ì •ìƒ) = (0+1) / (9+1Ã—16) = 0.040000
    log(P) = -3.2189
ì´ log í™•ë¥  = -10.3499
ì‹¤ì œ í™•ë¥  = exp(-10.3499) = 3.2000000e-05

--- ë¹„êµ ---
log P(ìŠ¤íŒ¸|ë©”ì¼) = -8.5580
log P(ì •ìƒ|ë©”ì¼) = -10.3499
â†’ ìŠ¤íŒ¸ì´ 1.7919 ë” ë†’ìŒ (log ìŠ¤ì¼€ì¼)

======================================================================
ã€ ê²°ê³¼ ìš”ì•½ ã€‘
======================================================================
ìŠ¤ë¬´ë”© ì—†ì´: ë¶„ë¥˜ ë¶ˆê°€
ìŠ¤ë¬´ë”© ì ìš©: ìŠ¤íŒ¸ âœ“

ğŸ’¡ ìŠ¤ë¬´ë”©ì„ ì ìš©í•˜ë©´ 0 í™•ë¥  ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
"""
```

### 2. ë‹¤ì–‘í•œ Î± ê°’ ë¹„êµ

```python
"""
ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©: ë‹¤ì–‘í•œ Î± ê°’ì˜ íš¨ê³¼
- Î±=0 (ìŠ¤ë¬´ë”© ì—†ìŒ)
- Î±=0.1, 0.5, 1.0, 2.0
- Î± ê°’ì´ í´ìˆ˜ë¡ í™•ë¥ ì´ ê· ë“±í•´ì§
"""

import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

# ê°„ë‹¨í•œ ì˜ˆì œ ë°ì´í„°
spam_words = ["ë¬´ë£Œ"] * 10 + ["ëŒ€ì¶œ"] * 8 + ["í• ì¸"] * 2  # ì´ 20ê°œ
normal_words = ["íšŒì˜"] * 5 + ["ë³´ê³ "] * 3  # ì´ 8ê°œ

spam_count = defaultdict(int, {"ë¬´ë£Œ": 10, "ëŒ€ì¶œ": 8, "í• ì¸": 2})
normal_count = defaultdict(int, {"íšŒì˜": 5, "ë³´ê³ ": 3})
spam_total = 20
normal_total = 8

# ì „ì²´ ì–´íœ˜: ë¬´ë£Œ, ëŒ€ì¶œ, í• ì¸, íšŒì˜, ë³´ê³ , ê¸´ê¸‰(ìƒˆ ë‹¨ì–´)
vocab = ["ë¬´ë£Œ", "ëŒ€ì¶œ", "í• ì¸", "íšŒì˜", "ë³´ê³ ", "ê¸´ê¸‰"]
vocab_size = len(vocab)

# ë‹¤ì–‘í•œ Î± ê°’
alphas = [0, 0.1, 0.5, 1.0, 2.0, 5.0]

print("=== ìŠ¤íŒ¸ ë©”ì¼ ë‹¨ì–´ í™•ë¥  (ë‹¤ì–‘í•œ Î±) ===\n")
print(f"{'ë‹¨ì–´':^10} | ", end="")
for alpha in alphas:
    print(f"Î±={alpha:^4} | ", end="")
print()
print("-" * 70)

for word in vocab:
    print(f"{word:^10} | ", end="")
    count = spam_count.get(word, 0)
    for alpha in alphas:
        if alpha == 0:
            prob = count / spam_total if spam_total > 0 else 0
        else:
            prob = (count + alpha) / (spam_total + alpha * vocab_size)
        print(f"{prob:^6.3f} | ", end="")
    print(f"  (ë¹ˆë„={count})")

print("\n=== ê´€ì°° ===")
print("1. Î±=0ì¼ ë•Œ:")
print("   - 'ê¸´ê¸‰' ë‹¨ì–´ì˜ í™•ë¥  = 0.000 (í›ˆë ¨ ë°ì´í„°ì— ì—†ìŒ)")
print("   - ê¸°ì¡´ ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì€ ì •í™•í•œ ë¹ˆë„ ë°˜ì˜")
print("\n2. Î±ê°€ ì¦ê°€í•˜ë©´:")
print("   - 'ê¸´ê¸‰' ë‹¨ì–´ì˜ í™•ë¥ ì´ 0ì—ì„œ ì¦ê°€")
print("   - ëª¨ë“  í™•ë¥ ì´ ê· ë“±í•´ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ë³€í™”")
print("   - ê³ ë¹ˆë„ ë‹¨ì–´('ë¬´ë£Œ')ì˜ í™•ë¥ ì€ ê°ì†Œ")
print("   - ì €ë¹ˆë„ ë‹¨ì–´('í• ì¸')ì˜ í™•ë¥ ì€ ì¦ê°€")
print("\n3. Î± ì„ íƒ:")
print("   - Î±=1.0 (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©): ì¼ë°˜ì  ì„ íƒ")
print("   - Î±ê°€ ë„ˆë¬´ í¬ë©´: í›ˆë ¨ ë°ì´í„° ì •ë³´ ì†ì‹¤")
print("   - Î±ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´: 0 í™•ë¥  ë¬¸ì œ í•´ê²° ë¶ˆì¶©ë¶„")

# ì‹œê°í™”
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, alpha in enumerate(alphas):
    ax = axes[idx]

    probs_spam = []
    probs_normal = []

    for word in vocab:
        # ìŠ¤íŒ¸
        count = spam_count.get(word, 0)
        if alpha == 0:
            prob = count / spam_total if count > 0 else 0
        else:
            prob = (count + alpha) / (spam_total + alpha * vocab_size)
        probs_spam.append(prob)

        # ì •ìƒ
        count = normal_count.get(word, 0)
        if alpha == 0:
            prob = count / normal_total if count > 0 else 0
        else:
            prob = (count + alpha) / (normal_total + alpha * vocab_size)
        probs_normal.append(prob)

    x = np.arange(len(vocab))
    width = 0.35

    ax.bar(x - width/2, probs_spam, width, label='ìŠ¤íŒ¸', alpha=0.8)
    ax.bar(x + width/2, probs_normal, width, label='ì •ìƒ', alpha=0.8)

    ax.set_xlabel('ë‹¨ì–´')
    ax.set_ylabel('í™•ë¥ ')
    ax.set_title(f'Î± = {alpha}')
    ax.set_xticks(x)
    ax.set_xticklabels(vocab, rotation=45)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('laplace_smoothing_comparison.png', dpi=300, bbox_inches='tight')
print("\nì‹œê°í™”ê°€ 'laplace_smoothing_comparison.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# Î± ê°’ì— ë”°ë¥¸ ë¶„ë¥˜ ê²°ê³¼ ë³€í™”
print("\n=== Î± ê°’ì— ë”°ë¥¸ 'ê¸´ê¸‰ ëŒ€ì¶œ' ë¶„ë¥˜ ê²°ê³¼ ===\n")

test_words = ["ê¸´ê¸‰", "ëŒ€ì¶œ"]

for alpha in alphas:
    log_prob_spam = 0
    log_prob_normal = 0

    for word in test_words:
        # ìŠ¤íŒ¸
        count = spam_count.get(word, 0)
        if alpha == 0:
            prob = count / spam_total if count > 0 else 1e-10  # ì•„ì£¼ ì‘ì€ ê°’
        else:
            prob = (count + alpha) / (spam_total + alpha * vocab_size)
        log_prob_spam += np.log(prob)

        # ì •ìƒ
        count = normal_count.get(word, 0)
        if alpha == 0:
            prob = count / normal_total if count > 0 else 1e-10
        else:
            prob = (count + alpha) / (normal_total + alpha * vocab_size)
        log_prob_normal += np.log(prob)

    prediction = "ìŠ¤íŒ¸" if log_prob_spam > log_prob_normal else "ì •ìƒ"
    diff = abs(log_prob_spam - log_prob_normal)

    print(f"Î±={alpha:4}: ìŠ¤íŒ¸={log_prob_spam:7.2f}, ì •ìƒ={log_prob_normal:7.2f}, "
          f"ì°¨ì´={diff:5.2f} â†’ {prediction}")

print("\nğŸ’¡ Î±=1.0 (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©)ì´ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„ íƒì…ë‹ˆë‹¤!")

"""
ì¶œë ¥ ì˜ˆì‹œ:
=== ìŠ¤íŒ¸ ë©”ì¼ ë‹¨ì–´ í™•ë¥  (ë‹¤ì–‘í•œ Î±) ===

   ë‹¨ì–´     | Î±= 0  | Î±=0.1 | Î±=0.5 | Î±=1.0 | Î±=2.0 | Î±=5.0 |
----------------------------------------------------------------------
   ë¬´ë£Œ     | 0.500 | 0.485 | 0.438 | 0.393 | 0.333 | 0.250 |   (ë¹ˆë„=10)
   ëŒ€ì¶œ     | 0.400 | 0.389 | 0.357 | 0.321 | 0.278 | 0.217 |   (ë¹ˆë„=8)
   í• ì¸     | 0.100 | 0.102 | 0.119 | 0.107 | 0.111 | 0.117 |   (ë¹ˆë„=2)
   íšŒì˜     | 0.000 | 0.005 | 0.024 | 0.036 | 0.056 | 0.100 |   (ë¹ˆë„=0)
   ë³´ê³      | 0.000 | 0.005 | 0.024 | 0.036 | 0.056 | 0.100 |   (ë¹ˆë„=0)
   ê¸´ê¸‰     | 0.000 | 0.005 | 0.024 | 0.036 | 0.056 | 0.100 |   (ë¹ˆë„=0)

=== ê´€ì°° ===
1. Î±=0ì¼ ë•Œ:
   - 'ê¸´ê¸‰' ë‹¨ì–´ì˜ í™•ë¥  = 0.000 (í›ˆë ¨ ë°ì´í„°ì— ì—†ìŒ)
   - ê¸°ì¡´ ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì€ ì •í™•í•œ ë¹ˆë„ ë°˜ì˜

2. Î±ê°€ ì¦ê°€í•˜ë©´:
   - 'ê¸´ê¸‰' ë‹¨ì–´ì˜ í™•ë¥ ì´ 0ì—ì„œ ì¦ê°€
   - ëª¨ë“  í™•ë¥ ì´ ê· ë“±í•´ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ë³€í™”
   - ê³ ë¹ˆë„ ë‹¨ì–´('ë¬´ë£Œ')ì˜ í™•ë¥ ì€ ê°ì†Œ
   - ì €ë¹ˆë„ ë‹¨ì–´('í• ì¸')ì˜ í™•ë¥ ì€ ì¦ê°€

3. Î± ì„ íƒ:
   - Î±=1.0 (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©): ì¼ë°˜ì  ì„ íƒ
   - Î±ê°€ ë„ˆë¬´ í¬ë©´: í›ˆë ¨ ë°ì´í„° ì •ë³´ ì†ì‹¤
   - Î±ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´: 0 í™•ë¥  ë¬¸ì œ í•´ê²° ë¶ˆì¶©ë¶„

=== Î± ê°’ì— ë”°ë¥¸ 'ê¸´ê¸‰ ëŒ€ì¶œ' ë¶„ë¥˜ ê²°ê³¼ ===

Î±= 0.0: ìŠ¤íŒ¸=-18.42, ì •ìƒ=-25.33, ì°¨ì´= 6.91 â†’ ìŠ¤íŒ¸
Î±= 0.1: ìŠ¤íŒ¸= -5.25, ì •ìƒ= -7.89, ì°¨ì´= 2.64 â†’ ìŠ¤íŒ¸
Î±= 0.5: ìŠ¤íŒ¸= -3.67, ì •ìƒ= -5.12, ì°¨ì´= 1.45 â†’ ìŠ¤íŒ¸
Î±= 1.0: ìŠ¤íŒ¸= -3.12, ì •ìƒ= -4.33, ì°¨ì´= 1.21 â†’ ìŠ¤íŒ¸
Î±= 2.0: ìŠ¤íŒ¸= -2.78, ì •ìƒ= -3.67, ì°¨ì´= 0.89 â†’ ìŠ¤íŒ¸
Î±= 5.0: ìŠ¤íŒ¸= -2.45, ì •ìƒ= -2.98, ì°¨ì´= 0.53 â†’ ìŠ¤íŒ¸

ğŸ’¡ Î±=1.0 (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©)ì´ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„ íƒì…ë‹ˆë‹¤!
"""
```

## Naive Bayes ë³€í˜• ë¹„êµ

### Gaussian vs Multinomial vs Bernoulli

```python
"""
Naive Bayes 3ê°€ì§€ ë³€í˜• ë¹„êµ
- Gaussian NB: ì—°ì†í˜• ë°ì´í„° (ì •ê·œë¶„í¬ ê°€ì •)
- Multinomial NB: ì´ì‚°í˜• ì¹´ìš´íŠ¸ (ë‹¨ì–´ ë¹ˆë„)
- Bernoulli NB: ì´ì§„ íŠ¹ì„± (ì¡´ì¬ ì—¬ë¶€)
"""

from sklearn.datasets import load_iris, fetch_20newsgroups
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score
import numpy as np

print("="*70)
print("Naive Bayes ë³€í˜• ë¹„êµ")
print("="*70)

# ============================================================
# 1. Gaussian NB - ì—°ì†í˜• ë°ì´í„° (Iris)
# ============================================================
print("\nã€ 1. Gaussian NB - ì—°ì†í˜• ë°ì´í„° ã€‘")
print("-" * 70)

iris = load_iris()
X_iris = iris.data
y_iris = iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X_iris, y_iris, test_size=0.3, random_state=42
)

gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)
acc_gnb = accuracy_score(y_test, y_pred_gnb)

print(f"ë°ì´í„°: Iris (ì—°ì†í˜• íŠ¹ì„± 4ê°œ)")
print(f"íŠ¹ì„± ì˜ˆì‹œ: {X_iris[0]}")  # [5.1 3.5 1.4 0.2]
print(f"ì •í™•ë„: {acc_gnb:.4f}")
print(f"\nğŸ’¡ Gaussian NBëŠ” ê° íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •")
print(f"   ê° í´ë˜ìŠ¤ë³„ í‰ê· ê³¼ ë¶„ì‚°ì„ í•™ìŠµ")

# ì²« ë²ˆì§¸ í´ë˜ìŠ¤ì˜ í‰ê· ê³¼ ë¶„ì‚°
print(f"\nì˜ˆ: '{iris.target_names[0]}' í´ë˜ìŠ¤ì˜ í†µê³„")
for i, feature_name in enumerate(iris.feature_names):
    print(f"  {feature_name}: í‰ê· ={gnb.theta_[0][i]:.2f}, ë¶„ì‚°={gnb.var_[0][i]:.2f}")

# ============================================================
# 2. Multinomial NB - ë‹¨ì–´ ë¹ˆë„ (ì¹´ìš´íŠ¸ ê¸°ë°˜)
# ============================================================
print("\n" + "="*70)
print("ã€ 2. Multinomial NB - ë‹¨ì–´ ë¹ˆë„ (ì¹´ìš´íŠ¸) ã€‘")
print("-" * 70)

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°
texts = [
    "python machine learning",
    "deep learning neural networks",
    "machine learning algorithms",
    "python programming language",
    "java programming language",
    "javascript web development",
]
labels = [0, 0, 0, 1, 1, 1]  # 0=ML, 1=Programming

vectorizer_count = CountVectorizer()
X_count = vectorizer_count.fit_transform(texts)

print(f"ë°ì´í„°: í…ìŠ¤íŠ¸ ë¬¸ì„œ {len(texts)}ê°œ")
print(f"íŠ¹ì„±: ë‹¨ì–´ ë¹ˆë„ (CountVectorizer)")
print(f"ì–´íœ˜: {vectorizer_count.get_feature_names_out()}")
print(f"\nì²« ë²ˆì§¸ ë¬¸ì„œ '{texts[0]}'ì˜ ë²¡í„°:")
print(f"{X_count[0].toarray()[0]}")
print(f"â†’ ê° ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜")

mnb = MultinomialNB(alpha=1.0)
mnb.fit(X_count, labels)

test_texts = ["python deep learning", "java web programming"]
test_vec = vectorizer_count.transform(test_texts)
predictions = mnb.predict(test_vec)
probas = mnb.predict_proba(test_vec)

print(f"\ní…ŒìŠ¤íŠ¸:")
for text, pred, proba in zip(test_texts, predictions, probas):
    label_name = "ML" if pred == 0 else "Programming"
    print(f"  '{text}' â†’ {label_name} (ML:{proba[0]:.2%}, Prog:{proba[1]:.2%})")

print(f"\nğŸ’¡ Multinomial NBëŠ” ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ì‚¬ìš©")
print(f"   'ë¬´ë£Œ'ê°€ 3ë²ˆ ë‚˜ì˜¤ë©´ 'ë¬´ë£Œ'ê°€ 1ë²ˆ ë‚˜ì˜¬ ë•Œë³´ë‹¤ í™•ë¥  ë†’ìŒ")

# ============================================================
# 3. Bernoulli NB - ì´ì§„ íŠ¹ì„± (ì¡´ì¬ ì—¬ë¶€)
# ============================================================
print("\n" + "="*70)
print("ã€ 3. Bernoulli NB - ì´ì§„ íŠ¹ì„± (ì¡´ì¬ ì—¬ë¶€) ã€‘")
print("-" * 70)

vectorizer_binary = CountVectorizer(binary=True)
X_binary = vectorizer_binary.fit_transform(texts)

print(f"ë°ì´í„°: ê°™ì€ í…ìŠ¤íŠ¸ ë¬¸ì„œ")
print(f"íŠ¹ì„±: ë‹¨ì–´ ì¡´ì¬ ì—¬ë¶€ (binary=True)")
print(f"\nì²« ë²ˆì§¸ ë¬¸ì„œ '{texts[0]}'ì˜ ë²¡í„°:")
print(f"{X_binary[0].toarray()[0]}")
print(f"â†’ 1=ì¡´ì¬, 0=ì—†ìŒ (ì¶œí˜„ íšŸìˆ˜ ë¬´ì‹œ)")

bnb = BernoulliNB(alpha=1.0)
bnb.fit(X_binary, labels)

test_vec_binary = vectorizer_binary.transform(test_texts)
predictions_b = bnb.predict(test_vec_binary)
probas_b = bnb.predict_proba(test_vec_binary)

print(f"\ní…ŒìŠ¤íŠ¸:")
for text, pred, proba in zip(test_texts, predictions_b, probas_b):
    label_name = "ML" if pred == 0 else "Programming"
    print(f"  '{text}' â†’ {label_name} (ML:{proba[0]:.2%}, Prog:{proba[1]:.2%})")

print(f"\nğŸ’¡ Bernoulli NBëŠ” ë‹¨ì–´ì˜ ì¡´ì¬ ì—¬ë¶€ë§Œ ì‚¬ìš©")
print(f"   'ë¬´ë£Œ'ê°€ 3ë²ˆ ë‚˜ì™€ë„ 1ë²ˆ ë‚˜ì˜¨ ê²ƒê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬")

# ============================================================
# 4. Multinomial vs Bernoulli ì§ì ‘ ë¹„êµ
# ============================================================
print("\n" + "="*70)
print("ã€ 4. Multinomial vs Bernoulli ë¹„êµ ã€‘")
print("-" * 70)

# ë‹¨ì–´ê°€ ë°˜ë³µë˜ëŠ” ê²½ìš°
test_case = "python python python programming programming programming"

# Multinomial (ë¹ˆë„ ê³ ë ¤)
vec_multi = vectorizer_count.transform([test_case])
pred_multi = mnb.predict(vec_multi)[0]
proba_multi = mnb.predict_proba(vec_multi)[0]

# Bernoulli (ì¡´ì¬ ì—¬ë¶€ë§Œ)
vec_bern = vectorizer_binary.transform([test_case])
pred_bern = bnb.predict(vec_bern)[0]
proba_bern = bnb.predict_proba(vec_bern)[0]

print(f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: '{test_case}'")
print(f"(ê° ë‹¨ì–´ê°€ 3ë²ˆì”© ë°˜ë³µ)")
print(f"\nMultinomial NB:")
print(f"  ë²¡í„°: {vec_multi.toarray()[0]}")
print(f"  ì˜ˆì¸¡: {'ML' if pred_multi==0 else 'Programming'}")
print(f"  í™•ë¥ : ML={proba_multi[0]:.2%}, Prog={proba_multi[1]:.2%}")

print(f"\nBernoulli NB:")
print(f"  ë²¡í„°: {vec_bern.toarray()[0]}")
print(f"  ì˜ˆì¸¡: {'ML' if pred_bern==0 else 'Programming'}")
print(f"  í™•ë¥ : ML={proba_bern[0]:.2%}, Prog={proba_bern[1]:.2%}")

print(f"\nğŸ’¡ ë‹¨ì–´ ë°˜ë³µì´ ë§ì€ ê²½ìš°:")
print(f"   - Multinomialì€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ê°•í•˜ê²Œ ë°˜ì˜")
print(f"   - BernoulliëŠ” ë°˜ë³µì„ ë¬´ì‹œí•˜ê³  ì¡´ì¬ ì—¬ë¶€ë§Œ ë´„")

# ============================================================
# 5. ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ê¹Œ?
# ============================================================
print("\n" + "="*70)
print("ã€ 5. ì„ íƒ ê°€ì´ë“œ ã€‘")
print("="*70)

guidelines = {
    "Gaussian NB": {
        "ì‚¬ìš© ì‹œê¸°": "ì—°ì†í˜• ìˆ˜ì¹˜ ë°ì´í„°",
        "ê°€ì •": "ê° íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„",
        "ì˜ˆì‹œ": "í‚¤, ëª¸ë¬´ê²Œ, ì˜¨ë„, ì£¼ê°€ ë“±",
        "ì¥ì ": "ì—°ì†í˜• ë°ì´í„°ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬",
        "ë‹¨ì ": "ì •ê·œë¶„í¬ ê°€ì •ì´ ë§ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ ì €í•˜"
    },
    "Multinomial NB": {
        "ì‚¬ìš© ì‹œê¸°": "ì¹´ìš´íŠ¸/ë¹ˆë„ ë°ì´í„° (ì´ì‚°í˜•)",
        "ê°€ì •": "íŠ¹ì„±ì´ ë‹¤í•­ ë¶„í¬ë¥¼ ë”°ë¦„",
        "ì˜ˆì‹œ": "ë‹¨ì–´ ë¹ˆë„, TF-IDF, ë¦¬ë·° í‰ì  ê°œìˆ˜",
        "ì¥ì ": "í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ë§¤ìš° íš¨ê³¼ì , ë¹ˆë„ ì •ë³´ í™œìš©",
        "ë‹¨ì ": "ìŒìˆ˜ ê°’ ë¶ˆê°€"
    },
    "Bernoulli NB": {
        "ì‚¬ìš© ì‹œê¸°": "ì´ì§„ íŠ¹ì„± (ìˆë‹¤/ì—†ë‹¤)",
        "ê°€ì •": "ê° íŠ¹ì„±ì´ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¦„",
        "ì˜ˆì‹œ": "ë‹¨ì–´ ì¡´ì¬ ì—¬ë¶€, ìŠ¤íŒ¸ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€",
        "ì¥ì ": "ì§§ì€ ë¬¸ì„œì— íš¨ê³¼ì , ë¹ ë¦„",
        "ë‹¨ì ": "ë¹ˆë„ ì •ë³´ ì†ì‹¤"
    }
}

for nb_type, info in guidelines.items():
    print(f"\n{nb_type}:")
    for key, value in info.items():
        print(f"  â€¢ {key}: {value}")

print(f"\nğŸ’¡ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ:")
print(f"   - ê¸´ ë¬¸ì„œ, ë‹¨ì–´ ë¹ˆë„ ì¤‘ìš” â†’ Multinomial NB")
print(f"   - ì§§ì€ ë¬¸ì„œ (íŠ¸ìœ—, ëŒ“ê¸€) â†’ Bernoulli NB")
print(f"   - ì‹¤í—˜ìœ¼ë¡œ ë‘˜ ë‹¤ ì‹œë„í•´ë³´ê³  ì„ íƒ!")

"""
ì¶œë ¥ ì˜ˆì‹œ:
======================================================================
Naive Bayes ë³€í˜• ë¹„êµ
======================================================================

ã€ 1. Gaussian NB - ì—°ì†í˜• ë°ì´í„° ã€‘
----------------------------------------------------------------------
ë°ì´í„°: Iris (ì—°ì†í˜• íŠ¹ì„± 4ê°œ)
íŠ¹ì„± ì˜ˆì‹œ: [5.1 3.5 1.4 0.2]
ì •í™•ë„: 0.9778

ğŸ’¡ Gaussian NBëŠ” ê° íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •
   ê° í´ë˜ìŠ¤ë³„ í‰ê· ê³¼ ë¶„ì‚°ì„ í•™ìŠµ

ì˜ˆ: 'setosa' í´ë˜ìŠ¤ì˜ í†µê³„
  sepal length (cm): í‰ê· =5.01, ë¶„ì‚°=0.12
  sepal width (cm): í‰ê· =3.42, ë¶„ì‚°=0.14
  petal length (cm): í‰ê· =1.46, ë¶„ì‚°=0.03
  petal width (cm): í‰ê· =0.25, ë¶„ì‚°=0.01

======================================================================
ã€ 2. Multinomial NB - ë‹¨ì–´ ë¹ˆë„ (ì¹´ìš´íŠ¸) ã€‘
----------------------------------------------------------------------
ë°ì´í„°: í…ìŠ¤íŠ¸ ë¬¸ì„œ 6ê°œ
íŠ¹ì„±: ë‹¨ì–´ ë¹ˆë„ (CountVectorizer)
ì–´íœ˜: ['algorithms' 'deep' 'development' 'java' 'javascript' 'language'
 'learning' 'machine' 'networks' 'neural' 'programming' 'python' 'web']

ì²« ë²ˆì§¸ ë¬¸ì„œ 'python machine learning'ì˜ ë²¡í„°:
[0 0 0 0 0 0 1 1 0 0 0 1 0]
â†’ ê° ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜

í…ŒìŠ¤íŠ¸:
  'python deep learning' â†’ ML (ML:87.45%, Prog:12.55%)
  'java web programming' â†’ Programming (ML:15.23%, Prog:84.77%)

ğŸ’¡ Multinomial NBëŠ” ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ì‚¬ìš©
   'ë¬´ë£Œ'ê°€ 3ë²ˆ ë‚˜ì˜¤ë©´ 'ë¬´ë£Œ'ê°€ 1ë²ˆ ë‚˜ì˜¬ ë•Œë³´ë‹¤ í™•ë¥  ë†’ìŒ
...
"""
```

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
"""
Naive Bayes í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- alpha (smoothing parameter)
- fit_prior (ì‚¬ì „ í™•ë¥  í•™ìŠµ ì—¬ë¶€)
"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import classification_report
import numpy as np
import matplotlib.pyplot as plt

# 1. ë°ì´í„° ë¡œë“œ (20 newsgroups ì¼ë¶€ ì¹´í…Œê³ ë¦¬)
categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']

print("=== 20 Newsgroups ë°ì´í„° ë¡œë“œ ===")
print(f"ì¹´í…Œê³ ë¦¬: {categories}\n")

train_data = fetch_20newsgroups(subset='train', categories=categories,
                                shuffle=True, random_state=42,
                                remove=('headers', 'footers', 'quotes'))
test_data = fetch_20newsgroups(subset='test', categories=categories,
                               shuffle=True, random_state=42,
                               remove=('headers', 'footers', 'quotes'))

print(f"í›ˆë ¨ ë¬¸ì„œ ìˆ˜: {len(train_data.data)}")
print(f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: {len(test_data.data)}")

# 2. ë²¡í„°í™” (TF-IDF)
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_train = vectorizer.fit_transform(train_data.data)
X_test = vectorizer.transform(test_data.data)
y_train = train_data.target
y_test = test_data.target

print(f"íŠ¹ì„± ìˆ˜ (ë‹¨ì–´ ìˆ˜): {X_train.shape[1]}")

# 3. alpha ê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ
print("\n=== Alpha ê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ===\n")

alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
train_scores = []
test_scores = []

for alpha in alphas:
    mnb = MultinomialNB(alpha=alpha)
    mnb.fit(X_train, y_train)

    train_score = mnb.score(X_train, y_train)
    test_score = mnb.score(X_test, y_test)

    train_scores.append(train_score)
    test_scores.append(test_score)

    print(f"Î±={alpha:6.3f}: í›ˆë ¨={train_score:.4f}, í…ŒìŠ¤íŠ¸={test_score:.4f}")

# ìµœì  alpha ì°¾ê¸°
best_alpha_idx = np.argmax(test_scores)
best_alpha = alphas[best_alpha_idx]
print(f"\nìµœì  Î±: {best_alpha} (í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_scores[best_alpha_idx]:.4f})")

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.plot(alphas, train_scores, marker='o', label='í›ˆë ¨ ì„¸íŠ¸', linewidth=2)
plt.plot(alphas, test_scores, marker='s', label='í…ŒìŠ¤íŠ¸ ì„¸íŠ¸', linewidth=2)
plt.axvline(best_alpha, color='red', linestyle='--',
            label=f'ìµœì  Î±={best_alpha}', alpha=0.7)
plt.xscale('log')
plt.xlabel('Î± (smoothing parameter)')
plt.ylabel('ì •í™•ë„')
plt.title('Alpha ê°’ì— ë”°ë¥¸ Naive Bayes ì„±ëŠ¥')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('nb_alpha_tuning.png', dpi=300, bbox_inches='tight')
print("\nì‹œê°í™”ê°€ 'nb_alpha_tuning.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 4. GridSearchCVë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
print("\n=== GridSearchCVë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ===\n")

param_grid = {
    'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0],
    'fit_prior': [True, False]
}

mnb = MultinomialNB()
grid_search = GridSearchCV(mnb, param_grid, cv=5, scoring='accuracy',
                          n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print(f"\nìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœì  CV ì ìˆ˜: {grid_search.best_score_:.4f}")

# 5. ìµœì  ëª¨ë¸ë¡œ ìµœì¢… í‰ê°€
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print("\n=== ìµœì¢… ì„±ëŠ¥ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸) ===\n")
print(classification_report(y_test, y_pred, target_names=categories))

# 6. fit_prior íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥
print("\n=== fit_prior íŒŒë¼ë¯¸í„° ì˜í–¥ ===\n")

print("fit_prior=True (ê¸°ë³¸ê°’):")
print("  ì‚¬ì „ í™•ë¥ ì„ í›ˆë ¨ ë°ì´í„°ì—ì„œ í•™ìŠµ")
mnb_true = MultinomialNB(alpha=1.0, fit_prior=True)
mnb_true.fit(X_train, y_train)
print(f"  í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥ : {mnb_true.class_prior_}")
print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {mnb_true.score(X_test, y_test):.4f}")

print("\nfit_prior=False:")
print("  ëª¨ë“  í´ë˜ìŠ¤ì˜ ì‚¬ì „ í™•ë¥ ì„ ë™ì¼í•˜ê²Œ ê°€ì • (ê· ë“± ë¶„í¬)")
mnb_false = MultinomialNB(alpha=1.0, fit_prior=False)
mnb_false.fit(X_train, y_train)
print(f"  í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥ : {mnb_false.class_prior_}")
print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {mnb_false.score(X_test, y_test):.4f}")

print("\nğŸ’¡ ì¼ë°˜ì ìœ¼ë¡œ fit_prior=Trueê°€ ë” ì¢‹ì€ ì„±ëŠ¥")
print("   ë°ì´í„°ê°€ ë¶ˆê· í˜•í•  ë•Œ íŠ¹íˆ ì¤‘ìš”!")

# 7. íŒŒë¼ë¯¸í„° ì˜í–¥ ì •ë¦¬
print("\n=== íŒŒë¼ë¯¸í„° ê°€ì´ë“œ ===")
print("""
1. alpha (Smoothing Parameter):
   â€¢ ì—­í• : ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ê°•ë„
   â€¢ ë²”ìœ„: 0 ~ ë¬´í•œëŒ€ (ë³´í†µ 0.1 ~ 10)
   â€¢ íš¨ê³¼:
     - alpha=0: ìŠ¤ë¬´ë”© ì—†ìŒ (í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•© ìœ„í—˜)
     - alphaâ†‘: í™•ë¥ ì´ ê· ë“±í•´ì§ (ê³¼ì†Œì í•© ìœ„í—˜)
     - alpha=1: ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© (ì¼ë°˜ì  ì„ íƒ)
   â€¢ ì„ íƒ: GridSearchë¡œ 0.1 ~ 10 ì‚¬ì´ íƒìƒ‰

2. fit_prior:
   â€¢ ì—­í• : ì‚¬ì „ í™•ë¥  í•™ìŠµ ì—¬ë¶€
   â€¢ ê°’:
     - True (ê¸°ë³¸): í›ˆë ¨ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ ë°˜ì˜
     - False: ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ë™ì¼í•œ í™•ë¥ ë¡œ ê°€ì •
   â€¢ ì„ íƒ:
     - ë°ì´í„°ê°€ ì‹¤ì œ ë¶„í¬ë¥¼ ë°˜ì˜ â†’ True
     - ë°ì´í„°ê°€ ë¶ˆê· í˜•í•˜ì§€ë§Œ ì‹¤ì œëŠ” ê· ë“± â†’ False
     - ì¼ë°˜ì ìœ¼ë¡œ True ê¶Œì¥

3. class_prior:
   â€¢ ì—­í• : ì‚¬ì „ í™•ë¥  ì§ì ‘ ì§€ì •
   â€¢ ì‚¬ìš©: ë„ë©”ì¸ ì§€ì‹ì´ ìˆì„ ë•Œ
   â€¢ ì˜ˆ: P(ìŠ¤íŒ¸)=0.3ì„ ì•Œê³  ìˆë‹¤ë©´ [0.7, 0.3] ì§€ì •
""")

"""
ì¶œë ¥ ì˜ˆì‹œ:
=== 20 Newsgroups ë°ì´í„° ë¡œë“œ ===
ì¹´í…Œê³ ë¦¬: ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']

í›ˆë ¨ ë¬¸ì„œ ìˆ˜: 2257
í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: 1502
íŠ¹ì„± ìˆ˜ (ë‹¨ì–´ ìˆ˜): 5000

=== Alpha ê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ===

Î±= 0.001: í›ˆë ¨=0.9956, í…ŒìŠ¤íŠ¸=0.8802
Î±= 0.010: í›ˆë ¨=0.9876, í…ŒìŠ¤íŠ¸=0.8989
Î±= 0.100: í›ˆë ¨=0.9654, í…ŒìŠ¤íŠ¸=0.9134
Î±= 0.500: í›ˆë ¨=0.9432, í…ŒìŠ¤íŠ¸=0.9187
Î±= 1.000: í›ˆë ¨=0.9321, í…ŒìŠ¤íŠ¸=0.9201
Î±= 2.000: í›ˆë ¨=0.9156, í…ŒìŠ¤íŠ¸=0.9167
Î±= 5.000: í›ˆë ¨=0.8876, í…ŒìŠ¤íŠ¸=0.9034
Î±=10.000: í›ˆë ¨=0.8654, í…ŒìŠ¤íŠ¸=0.8876

ìµœì  Î±: 1.0 (í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9201)

=== GridSearchCVë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ===

Fitting 5 folds for each of 14 candidates, totalling 70 fits

ìµœì  íŒŒë¼ë¯¸í„°: {'alpha': 1.0, 'fit_prior': True}
ìµœì  CV ì ìˆ˜: 0.9156

=== ìµœì¢… ì„±ëŠ¥ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸) ===

                        precision    recall  f1-score   support

           alt.atheism       0.89      0.92      0.91       319
  soc.religion.christian       0.92      0.91      0.92       398
         comp.graphics       0.94      0.93      0.93       389
               sci.med       0.96      0.94      0.95       396

              accuracy                           0.92      1502
             macro avg       0.93      0.93      0.93      1502
          weighted avg       0.93      0.92      0.92      1502

=== fit_prior íŒŒë¼ë¯¸í„° ì˜í–¥ ===

fit_prior=True (ê¸°ë³¸ê°’):
  ì‚¬ì „ í™•ë¥ ì„ í›ˆë ¨ ë°ì´í„°ì—ì„œ í•™ìŠµ
  í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥ : [0.236 0.298 0.253 0.213]
  í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9201

fit_prior=False:
  ëª¨ë“  í´ë˜ìŠ¤ì˜ ì‚¬ì „ í™•ë¥ ì„ ë™ì¼í•˜ê²Œ ê°€ì • (ê· ë“± ë¶„í¬)
  í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥ : [0.25 0.25 0.25 0.25]
  í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9134

ğŸ’¡ ì¼ë°˜ì ìœ¼ë¡œ fit_prior=Trueê°€ ë” ì¢‹ì€ ì„±ëŠ¥
   ë°ì´í„°ê°€ ë¶ˆê· í˜•í•  ë•Œ íŠ¹íˆ ì¤‘ìš”!
"""
```

## ì¥ë‹¨ì  ë° ì‹¤ì „ íŒ

### Naive Bayesì˜ ì¥ë‹¨ì 

**ì¥ì :**

1. **ë¹ ë¥¸ í•™ìŠµê³¼ ì˜ˆì¸¡**
   - O(n Ã— d) ì‹œê°„ ë³µì¡ë„ (n=ë°ì´í„° ê°œìˆ˜, d=íŠ¹ì„± ê°œìˆ˜)
   - ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì— ì í•©

2. **ì ì€ ë°ì´í„°ë¡œë„ ì‘ë™**
   - ê° íŠ¹ì„±ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
   - ê³ ì°¨ì›ì—ì„œë„ ì˜ ì‘ë™

3. **í™•ë¥  ì˜ˆì¸¡ ì œê³µ**
   - predict_proba()ë¡œ ì‹ ë¢°ë„ í™•ì¸ ê°€ëŠ¥
   - ì„ê³„ê°’ ì¡°ì • ìš©ì´

4. **êµ¬í˜„ì´ ê°„ë‹¨**
   - ì´í•´í•˜ê¸° ì‰¬ìš´ ìˆ˜í•™ì  ë°°ê²½
   - ë””ë²„ê¹… ìš©ì´

**ë‹¨ì :**

1. **ë…ë¦½ ê°€ì •ì˜ ë¹„í˜„ì‹¤ì„±**
   - ì‹¤ì œë¡œëŠ” íŠ¹ì„±ë“¤ì´ ìƒê´€ê´€ê³„ ìˆìŒ
   - ì˜ˆ: "ë¬´ë£Œ"ì™€ "ëŒ€ì¶œ"ì€ í•¨ê»˜ ë‚˜íƒ€ë‚¨

2. **ìˆ˜ì¹˜ ì•ˆì •ì„± ë¬¸ì œ**
   - ë§¤ìš° ì‘ì€ í™•ë¥ ì˜ ê³±ì…ˆ
   - ì–¸ë”í”Œë¡œìš° ìœ„í—˜ â†’ log ì‚¬ìš© í•„ìš”

3. **ì—°ì†í˜• ë°ì´í„° ì²˜ë¦¬**
   - Gaussian NBëŠ” ì •ê·œë¶„í¬ ê°€ì • í•„ìš”
   - ì‹¤ì œ ë¶„í¬ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

### ì‹¤ì „ íŒ

```python
"""
Naive Bayes ì‹¤ì „ íŒ ëª¨ìŒ
"""

# Tip 1: í•­ìƒ ë¡œê·¸ í™•ë¥  ì‚¬ìš©
print("=== Tip 1: ë¡œê·¸ í™•ë¥  ì‚¬ìš© ===")
print("""
ë¬¸ì œ: ì‘ì€ í™•ë¥ ì„ ì—¬ëŸ¬ ë²ˆ ê³±í•˜ë©´ ì–¸ë”í”Œë¡œìš° ë°œìƒ

ë‚˜ìœ ì˜ˆ:
P = 0.0001 Ã— 0.0001 Ã— 0.0001 Ã— ... â†’ 0.0 (ì–¸ë”í”Œë¡œìš°!)

ì¢‹ì€ ì˜ˆ:
log(P) = log(0.0001) + log(0.0001) + ... â†’ ì •í™•í•œ ê³„ì‚°

scikit-learnì€ ë‚´ë¶€ì ìœ¼ë¡œ ë¡œê·¸ ì‚¬ìš©:
  - decision_function(): ë¡œê·¸ í™•ë¥  ë°˜í™˜
  - predict_proba(): exp(log_prob)ë¡œ ë³€í™˜
""")

# Tip 2: íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”
print("\n=== Tip 2: íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš” ===")
print("""
Naive BayesëŠ” ê° íŠ¹ì„±ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
â†’ ìŠ¤ì¼€ì¼ë§ì´ ê²°ê³¼ì— ì˜í–¥ ì—†ìŒ

ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµ:
  - SVM, KNN: ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜
  - Decision Tree: ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”
  - Naive Bayes: ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš” âœ“

ë‹¨, Gaussian NBì—ì„œ ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ë„ ìˆìŒ
""")

# Tip 3: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
print("\n=== Tip 3: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ===")
print("""
ì˜µì…˜ 1: fit_prior=True (ê¸°ë³¸ê°’)
  - í›ˆë ¨ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ ë°˜ì˜
  - ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©

ì˜µì…˜ 2: class_prior ì§ì ‘ ì§€ì •
  - ë„ë©”ì¸ ì§€ì‹ í™œìš©
  - ì˜ˆ: ì‹¤ì œ ìŠ¤íŒ¸ ë¹„ìœ¨ì´ 10%ë¼ë©´ [0.9, 0.1] ì§€ì •

from sklearn.naive_bayes import MultinomialNB

# ì‹¤ì œ ìŠ¤íŒ¸ ë¹„ìœ¨ ë°˜ì˜
mnb = MultinomialNB(class_prior=[0.9, 0.1])
""")

# Tip 4: ìƒˆë¡œìš´ ë‹¨ì–´ ì²˜ë¦¬
print("\n=== Tip 4: ìƒˆë¡œìš´ ë‹¨ì–´ ì²˜ë¦¬ ===")
print("""
ë¬¸ì œ: í…ŒìŠ¤íŠ¸ ì‹œ ì²˜ìŒ ë³´ëŠ” ë‹¨ì–´
í•´ê²°: alpha > 0 (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©)

ê¶Œì¥ê°’:
  - í…ìŠ¤íŠ¸ ë¶„ë¥˜: alpha=1.0 (ë¼í”Œë¼ìŠ¤)
  - ë°ì´í„°ê°€ ë§ìœ¼ë©´: alpha=0.1 ~ 0.5
  - ë°ì´í„°ê°€ ì ìœ¼ë©´: alpha=1.0 ~ 2.0

í•­ìƒ GridSearchCVë¡œ ìµœì ê°’ ì°¾ê¸°!
""")

# Tip 5: Multinomial vs Bernoulli ì„ íƒ
print("\n=== Tip 5: Multinomial vs Bernoulli ì„ íƒ ===")
print("""
í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì„ íƒ:

ê¸´ ë¬¸ì„œ (ë‰´ìŠ¤ ê¸°ì‚¬, ë¦¬ë·°):
  â†’ Multinomial NB + CountVectorizer
  â†’ ë‹¨ì–´ ë¹ˆë„ê°€ ì¤‘ìš”í•œ ì •ë³´

ì§§ì€ ë¬¸ì„œ (íŠ¸ìœ—, ëŒ“ê¸€, SMS):
  â†’ Bernoulli NB + binary=True
  â†’ ì¡´ì¬ ì—¬ë¶€ë§Œìœ¼ë¡œ ì¶©ë¶„

ì• ë§¤í•˜ë©´:
  â†’ ë‘˜ ë‹¤ ì‹œë„í•˜ê³  Cross-Validationìœ¼ë¡œ ë¹„êµ!

from sklearn.model_selection import cross_val_score

scores_multi = cross_val_score(MultinomialNB(), X, y, cv=5)
scores_bern = cross_val_score(BernoulliNB(), X, y, cv=5)

print(f"Multinomial: {scores_multi.mean():.3f}")
print(f"Bernoulli: {scores_bern.mean():.3f}")
""")

# Tip 6: TF-IDF vs Count
print("\n=== Tip 6: TF-IDF vs Count Vectorization ===")
print("""
Count Vectorizer (ë‹¨ì–´ ë¹ˆë„):
  - Multinomial/Bernoulli NBì™€ ìì—°ìŠ¤ëŸ¬ìš´ ì¡°í•©
  - í•´ì„ì´ ì‰¬ì›€ (í™•ë¥ ì  ì˜ë¯¸ ëª…í™•)

TF-IDF:
  - ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŒ
  - ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” íš¨ê³¼
  - ë‹¨, í™•ë¥  í•´ì„ì´ ëœ ì§ê´€ì 

ê¶Œì¥: ë‘˜ ë‹¤ ì‹œë„!

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Count ê¸°ë°˜
vec_count = CountVectorizer()
X_count = vec_count.fit_transform(texts)
mnb_count = MultinomialNB().fit(X_count, y)

# TF-IDF ê¸°ë°˜
vec_tfidf = TfidfVectorizer()
X_tfidf = vec_tfidf.fit_transform(texts)
mnb_tfidf = MultinomialNB().fit(X_tfidf, y)
""")

# Tip 7: ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•
print("\n=== Tip 7: ì„±ëŠ¥ í–¥ìƒ ê¸°ë²• ===")
print("""
1. íŠ¹ì„± ì„ íƒ:
   - ë¶ˆìš©ì–´(stopwords) ì œê±°
   - ë„ˆë¬´ í”í•˜ê±°ë‚˜ í¬ê·€í•œ ë‹¨ì–´ ì œê±°
   - max_features, min_df, max_df ì¡°ì •

2. N-gram ì‚¬ìš©:
   - ë‹¨ì–´ ì¡°í•© ì •ë³´ í™œìš©
   - ngram_range=(1, 2): ë‹¨ì–´ + ë°”ì´ê·¸ë¨

3. ì•™ìƒë¸”:
   - Naive Bayes + ë‹¤ë¥¸ ëª¨ë¸ ì¡°í•©
   - VotingClassifier ì‚¬ìš©

ì˜ˆì‹œ:
vectorizer = TfidfVectorizer(
    max_features=5000,      # ìƒìœ„ 5000ê°œ ë‹¨ì–´ë§Œ
    ngram_range=(1, 2),     # ìœ ë‹ˆê·¸ë¨ + ë°”ì´ê·¸ë¨
    min_df=2,               # ìµœì†Œ 2ë²ˆ ì¶œí˜„
    max_df=0.8,             # 80% ì´ìƒ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ë©´ ì œì™¸
    stop_words='english'    # ë¶ˆìš©ì–´ ì œê±°
)
""")

# Tip 8: í™•ë¥  ë³´ì •
print("\n=== Tip 8: í™•ë¥  ë³´ì • (Calibration) ===")
print("""
Naive Bayesì˜ í™•ë¥  ì˜ˆì¸¡ì€ ì¢…ì¢… ê·¹ë‹¨ì  (0 or 1ì— ê°€ê¹Œì›€)
â†’ CalibratedClassifierCVë¡œ ë³´ì •

from sklearn.calibration import CalibratedClassifierCV

mnb = MultinomialNB()
calibrated_mnb = CalibratedClassifierCV(mnb, method='sigmoid', cv=5)
calibrated_mnb.fit(X_train, y_train)

# ë³´ì •ëœ í™•ë¥ 
proba = calibrated_mnb.predict_proba(X_test)
""")

print("\n=== ìš”ì•½: Naive Bayes ì²´í¬ë¦¬ìŠ¤íŠ¸ ===")
checklist = """
â–¡ alpha ê°’ íŠœë‹ (GridSearchCV)
â–¡ Multinomial vs Bernoulli ì„ íƒ
â–¡ Count vs TF-IDF ë¹„êµ
â–¡ N-gram ì‹œë„
â–¡ ë¶ˆìš©ì–´ ì œê±°
â–¡ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (fit_prior, class_prior)
â–¡ Cross-validationìœ¼ë¡œ í‰ê°€
â–¡ í•„ìš”ì‹œ í™•ë¥  ë³´ì •
â–¡ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµ (Logistic Regression, SVM)
"""
print(checklist)
