# Lecture 8: LLM Evaluation

# Materials

- [CME 295](https://cme295.stanford.edu/syllabus/)
- [slide](https://cme295.stanford.edu/slides/fall25-cme295-lecture8.pdf)
- [video](https://www.youtube.com/watch?v=8fNP4N46RRo&list=PLoROMvodv4rOCXd21gf0CF4xr35yINeOy&index=9)

# Table of Contents

- [Lecture 8: LLM Evaluation](#lecture-8-llm-evaluation)
- [Materials](#materials)
- [Table of Contents](#table-of-contents)
- [ê°•ì˜ ê°œìš”](#ê°•ì˜-ê°œìš”)
  - [ê°•ì˜ ëª©í‘œ](#ê°•ì˜-ëª©í‘œ)
  - [ì£¼ìš” í•™ìŠµ ë‚´ìš©](#ì£¼ìš”-í•™ìŠµ-ë‚´ìš©)
- [1. Evaluationì˜ ì¤‘ìš”ì„±](#1-evaluationì˜-ì¤‘ìš”ì„±)
  - [1.1. Evaluationì´ë€?](#11-evaluationì´ë€)
  - [1.2. ì™œ Evaluationì´ ì¤‘ìš”í•œê°€?](#12-ì™œ-evaluationì´-ì¤‘ìš”í•œê°€)
- [2. Human Evaluation](#2-human-evaluation)
  - [2.1. Ideal Scenario: Human Ratings](#21-ideal-scenario-human-ratings)
  - [2.2. Inter-Rater Agreement](#22-inter-rater-agreement)
    - [Agreement Rateì˜ ë¬¸ì œì ](#agreement-rateì˜-ë¬¸ì œì )
    - [Cohen's Kappa](#cohens-kappa)
    - [ë‹¤ë¥¸ Agreement Metrics](#ë‹¤ë¥¸-agreement-metrics)
  - [2.3. Human Evaluationì˜ í•œê³„](#23-human-evaluationì˜-í•œê³„)
- [3. Rule-Based Metrics](#3-rule-based-metrics)
  - [3.1. Rule-Based Metricsë€?](#31-rule-based-metricsë€)
  - [3.2. METEOR](#32-meteor)
  - [3.3. BLEU](#33-bleu)
  - [3.4. ROUGE](#34-rouge)
  - [3.5. Rule-Based Metricsì˜ í•œê³„](#35-rule-based-metricsì˜-í•œê³„)
- [4. LLM-as-a-Judge](#4-llm-as-a-judge)
  - [4.1. LLM-as-a-Judgeë€?](#41-llm-as-a-judgeë€)
  - [4.2. ê¸°ë³¸ Setup](#42-ê¸°ë³¸-setup)
  - [4.3. Structured Outputs](#43-structured-outputs)
  - [4.4. LLM-as-a-Judgeì˜ ì¥ì ](#44-llm-as-a-judgeì˜-ì¥ì )
  - [4.5. Variants](#45-variants)
    - [Pointwise Evaluation](#pointwise-evaluation)
    - [Pairwise Evaluation](#pairwise-evaluation)
- [5. LLM-as-a-Judgeì˜ Biases](#5-llm-as-a-judgeì˜-biases)
  - [5.1. Position Bias](#51-position-bias)
  - [5.2. Verbosity Bias](#52-verbosity-bias)
  - [5.3. Self-Enhancement Bias](#53-self-enhancement-bias)
- [6. Best Practices](#6-best-practices)
  - [6.1. ëª…í™•í•œ Guidelines](#61-ëª…í™•í•œ-guidelines)
  - [6.2. Binary Scale ì‚¬ìš©](#62-binary-scale-ì‚¬ìš©)
  - [6.3. Rationale First](#63-rationale-first)
  - [6.4. Bias ì™„í™”](#64-bias-ì™„í™”)
  - [6.5. Human Calibration](#65-human-calibration)
  - [6.6. Low Temperature](#66-low-temperature)
- [7. Evaluation Dimensions](#7-evaluation-dimensions)
  - [7.1. Task Performance](#71-task-performance)
  - [7.2. Factuality](#72-factuality)
    - [Fact Extraction](#fact-extraction)
    - [Fact Checking](#fact-checking)
    - [Aggregation](#aggregation)
- [8. Agent Evaluation](#8-agent-evaluation)
  - [8.1. Agentì˜ Inner Working](#81-agentì˜-inner-working)
  - [8.2. Tool Prediction Errors](#82-tool-prediction-errors)
    - [Tool Router Error](#tool-router-error)
    - [LLMì´ Toolì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°](#llmì´-toolì„-ì‚¬ìš©í•˜ì§€-ì•ŠëŠ”-ê²½ìš°)
  - [8.3. Tool Hallucination](#83-tool-hallucination)
- [9. ìš”ì•½](#9-ìš”ì•½)
  - [í•µì‹¬ ê°œë…](#í•µì‹¬-ê°œë…)
  - [Evaluation ë°©ë²• ë¹„êµ](#evaluation-ë°©ë²•-ë¹„êµ)
  - [ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸](#ì‹¤ì „-ì²´í¬ë¦¬ìŠ¤íŠ¸)
- [10. ì¤‘ìš” ìš©ì–´ ì •ë¦¬](#10-ì¤‘ìš”-ìš©ì–´-ì •ë¦¬)

---

# ê°•ì˜ ê°œìš”

## ê°•ì˜ ëª©í‘œ

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” LLMì˜ ì¶œë ¥ í’ˆì§ˆì„ ì–´ë–»ê²Œ í‰ê°€í•  ê²ƒì¸ê°€ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤. í‰ê°€ëŠ” LLM ê°œë°œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. **ì¸¡ì •í•  ìˆ˜ ì—†ìœ¼ë©´ ê°œì„ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.**

**í•™ìŠµ ëª©í‘œ:**
- LLM Evaluationì˜ ì¤‘ìš”ì„± ì´í•´
- Human Evaluationì˜ ì¥ë‹¨ì  íŒŒì•…
- Rule-based Metricsì˜ í•œê³„ ì¸ì‹
- LLM-as-a-Judge ê°œë…ê³¼ í™œìš©ë²• ìŠµë“
- Evaluation Biases ì´í•´ ë° ì™„í™” ë°©ë²• í•™ìŠµ
- Agent Evaluation ê¸°ë²• ì´í•´

## ì£¼ìš” í•™ìŠµ ë‚´ìš©

**1. Evaluation ë°©ë²•ë¡ **
- Human Evaluation: Inter-rater agreement
- Rule-based Metrics: METEOR, BLEU, ROUGE
- LLM-as-a-Judge: í˜„ëŒ€ì  ì ‘ê·¼ë²•

**2. LLM-as-a-Judge**
- Structured Outputs
- Pointwise vs Pairwise Evaluation
- Biasesì™€ ì™„í™” ë°©ë²•

**3. Evaluation Dimensions**
- Task Performance
- Factuality
- Safety & Alignment

**4. Agent Evaluation**
- Tool Call Evaluation
- Error Analysis

---

# 1. Evaluationì˜ ì¤‘ìš”ì„±

## 1.1. Evaluationì´ë€?

"LLMì„ í‰ê°€í•œë‹¤"ëŠ” ë§ì€ ì—¬ëŸ¬ ì˜ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**ë‹¤ì–‘í•œ í‰ê°€ ì¸¡ë©´:**

```
í‰ê°€ ë²”ìœ„:
1. ì¶œë ¥ í’ˆì§ˆ (Output Quality)
   - Coherence (ì¼ê´€ì„±)
   - Factuality (ì‚¬ì‹¤ì„±)
   - Relevance (ê´€ë ¨ì„±)
   - Usefulness (ìœ ìš©ì„±)

2. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ (System Metrics)
   - Latency (ì§€ì—°ì‹œê°„)
   - Throughput (ì²˜ë¦¬ëŸ‰)
   - Uptime (ê°€ìš©ì„±)

3. ë¹„ìš© (Cost)
   - í† í°ë‹¹ ë¹„ìš©
   - ìš´ì˜ ë¹„ìš©
```

**ì´ë²ˆ ê°•ì˜ì˜ Focus:**

ì´ ê°•ì˜ëŠ” **ì¶œë ¥ í’ˆì§ˆ(Output Quality)** í‰ê°€ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.

## 1.2. ì™œ Evaluationì´ ì¤‘ìš”í•œê°€?

**í•µì‹¬ ì›ì¹™:**

```
"You can't improve what you don't measure."
ì¸¡ì •í•  ìˆ˜ ì—†ìœ¼ë©´ ê°œì„ í•  ìˆ˜ ì—†ë‹¤.
```

**LLM ì¶œë ¥ì˜ íŠ¹ì„±:**

```python
# LLMì€ ììœ  í˜•ì‹ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
prompt = "What birthday gift should I get?"

# ê°€ëŠ¥í•œ ì¶œë ¥:
response1 = "A teddy bear is always sweet."
response2 = "Consider their hobbies and interests..."
response3 = "Here are 10 gift ideas:\n1. Books\n2. ..."

# ì–´ë–¤ ê²ƒì´ ë” ì¢‹ì€ê°€?
# â†’ Evaluationì´ í•„ìš”!
```

**í‰ê°€ê°€ ì¤‘ìš”í•œ ì´ìœ :**

1. **ëª¨ë¸ ê°œì„  ë°©í–¥ ê²°ì •**
   - ì–´ë–¤ ë¶€ë¶„ì´ ì•½í•œì§€ íŒŒì•…
   - ê°œì„  íš¨ê³¼ ì¸¡ì •

2. **ëª¨ë¸ ê°„ ë¹„êµ**
   - GPT-4 vs Claude vs LLaMA
   - Fine-tuned model vs Base model

3. **Production Readiness íŒë‹¨**
   - ì‚¬ìš©ìì—ê²Œ ë°°í¬í•´ë„ ë˜ëŠ”ê°€?
   - ì•ˆì „í•œê°€?

---

# 2. Human Evaluation

## 2.1. Ideal Scenario: Human Ratings

**ì´ìƒì ì¸ ì‹œë‚˜ë¦¬ì˜¤:**

```
Workflow:
1. Prompt â†’ LLM â†’ Response
2. Human rates the response
3. Collect all ratings
4. Aggregate â†’ Overall performance
```

**ì˜ˆì‹œ:**

```python
# Evaluation Process
prompts = [
    "What gift should I get?",
    "Explain quantum computing",
    "Write a Python function..."
]

for prompt in prompts:
    response = llm.generate(prompt)

    # Human rates
    rating = human_rater.rate(
        response=response,
        criteria="usefulness",
        scale="1-5"
    )

    store_rating(prompt, response, rating)
```

**ë¬¸ì œì :**

1. **Cost-Intensive (ë¹„ìš©ì´ ë§ì´ ë“¦)**
   - 1000ê°œ ì‘ë‹µ í‰ê°€ = ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©

2. **Slow (ëŠë¦¼)**
   - ëª¨ë¸ iterationë§ˆë‹¤ í‰ê°€ ë¶ˆê°€ëŠ¥

3. **Subjective (ì£¼ê´€ì )**
   - í‰ê°€ìë§ˆë‹¤ ë‹¤ë¥¸ ê¸°ì¤€

## 2.2. Inter-Rater Agreement

í‰ê°€ê°€ ì£¼ê´€ì ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, **í‰ê°€ì ê°„ ì¼ì¹˜ë„**ë¥¼ ì¸¡ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ:**

```
Prompt: "What birthday gift should I get?"
Response: "A teddy bear is almost always a sweet gift.
          Just pick one that feels right for you."

í‰ê°€ ê¸°ì¤€: Usefulness (ìœ ìš©ì„±)

Rater 1: "Useful (5/5)"
  â†’ í…Œë””ë² ì–´ëŠ” êµ¬ì²´ì ì¸ ì œì•ˆì´ë‹¤

Rater 2: "Not useful (2/5)"
  â†’ ì–´ë–¤ í…Œë””ë² ì–´ì¸ì§€ êµ¬ì²´ì ì´ì§€ ì•Šë‹¤

â†’ Inter-rater disagreement!
```

### Agreement Rateì˜ ë¬¸ì œì 

**ë‹¨ìˆœ Agreement Rate:**

```python
agreement_rate = (í‰ê°€ìë“¤ì´ ë™ì˜í•œ íšŸìˆ˜) / (ì „ì²´ í‰ê°€ íšŸìˆ˜)
```

**ë¬¸ì œ: Random Chanceë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ**

**ì˜ˆì‹œ:**

```python
# ë‘ í‰ê°€ìê°€ randomí•˜ê²Œ í‰ê°€í•˜ëŠ” ê²½ìš°
# Binary scale: Good (1) or Bad (0)

P(Alice says Good) = 0.5
P(Bob says Good) = 0.5

# Agreement by random chance:
P(Both agree) = P(A=1, B=1) + P(A=0, B=0)
              = 0.5 * 0.5 + 0.5 * 0.5
              = 0.25 + 0.25
              = 0.5
```

**ê²°ë¡ :** Randomìœ¼ë¡œë§Œ í‰ê°€í•´ë„ 50% agreement!

**ì¼ë°˜í™”:**

```python
def agreement_by_chance(p_a, p_b):
    """
    p_a: í‰ê°€ì Aê°€ 1ì„ ì„ íƒí•  í™•ë¥ 
    p_b: í‰ê°€ì Bê°€ 1ì„ ì„ íƒí•  í™•ë¥ 
    """
    agree_on_1 = p_a * p_b
    agree_on_0 = (1 - p_a) * (1 - p_b)
    return agree_on_1 + agree_on_0

# ì˜ˆì‹œ
print(agreement_by_chance(0.5, 0.5))  # 0.5
print(agreement_by_chance(0.7, 0.7))  # 0.58
```

### Cohen's Kappa

**ë¬¸ì œ í•´ê²°:** Random chanceë¥¼ ê³ ë ¤í•œ metric

**ê³µì‹:**

```
Îº (kappa) = (P_observed - P_chance) / (1 - P_chance)

ì—¬ê¸°ì„œ:
- P_observed: ì‹¤ì œ ê´€ì°°ëœ agreement rate
- P_chance: Random chanceì— ì˜í•œ agreement rate
```

**í•´ì„:**

```python
Îº = 1.0   # Perfect agreement
Îº = 0.0   # Agreement = random chance
Îº < 0.0   # Worse than random!
```

**ì˜ˆì‹œ:**

```python
# ì‹¤ì œ ê´€ì°°ëœ agreement
P_observed = 0.8

# Random chanceë¡œ ì¸í•œ agreement
P_chance = 0.5

# Cohen's Kappa
kappa = (0.8 - 0.5) / (1 - 0.5)
      = 0.3 / 0.5
      = 0.6
```

**Kappa í•´ì„ ê°€ì´ë“œ:**

| Kappa ê°’ | í•´ì„ |
|----------|------|
| < 0.0 | Poor (ë‚˜ì¨) |
| 0.0 - 0.2 | Slight (ì•½ê°„) |
| 0.2 - 0.4 | Fair (ë³´í†µ) |
| 0.4 - 0.6 | Moderate (ì¤‘ê°„) |
| 0.6 - 0.8 | Substantial (ìƒë‹¹í•¨) |
| 0.8 - 1.0 | Almost Perfect (ê±°ì˜ ì™„ë²½) |

### ë‹¤ë¥¸ Agreement Metrics

**Cohen's Kappaì˜ í™•ì¥:**

1. **Fleiss's Kappa**
   - 3ëª… ì´ìƒì˜ í‰ê°€ì
   - ëª¨ë“  í‰ê°€ìê°€ ëª¨ë“  í•­ëª©ì„ í‰ê°€

2. **Krippendorff's Alpha**
   - ì„ì˜ ìˆ˜ì˜ í‰ê°€ì
   - Missing data í—ˆìš©
   - ë‹¤ì–‘í•œ data type ì§€ì›

**êµ¬í˜„ ì˜ˆì‹œ:**

```python
from sklearn.metrics import cohen_kappa_score

# ë‘ í‰ê°€ìì˜ ratings
rater1 = [1, 0, 1, 1, 0, 1, 0, 0]
rater2 = [1, 0, 1, 0, 0, 1, 1, 0]

kappa = cohen_kappa_score(rater1, rater2)
print(f"Cohen's Kappa: {kappa:.3f}")
```

**ì‹¤ì „ í™œìš©:**

```python
# Inter-rater agreement ì¶”ì 
def track_agreement(ratings_per_rater):
    """
    ratings_per_rater: {rater_id: [ratings]}
    """
    kappas = []
    rater_ids = list(ratings_per_rater.keys())

    # ëª¨ë“  rater ìŒì— ëŒ€í•´ kappa ê³„ì‚°
    for i in range(len(rater_ids)):
        for j in range(i+1, len(rater_ids)):
            r1 = ratings_per_rater[rater_ids[i]]
            r2 = ratings_per_rater[rater_ids[j]]
            kappa = cohen_kappa_score(r1, r2)
            kappas.append(kappa)

    avg_kappa = sum(kappas) / len(kappas)
    return avg_kappa

# ì‚¬ìš© ì˜ˆì‹œ
ratings = {
    "alice": [1, 0, 1, 1, 0],
    "bob": [1, 0, 1, 0, 0],
    "charlie": [1, 1, 1, 0, 0]
}

avg_kappa = track_agreement(ratings)
print(f"Average Kappa: {avg_kappa:.3f}")

if avg_kappa < 0.6:
    print("âš ï¸ Agreement is low. Hold calibration session!")
```

## 2.3. Human Evaluationì˜ í•œê³„

**ìš”ì•½:**

| ì¸¡ë©´ | ë¬¸ì œì  | í•´ê²° ë°©ë²• |
|------|--------|-----------|
| Subjectivity | í‰ê°€ìë§ˆë‹¤ ë‹¤ë¥¸ ê¸°ì¤€ | Inter-rater agreement ì¶”ì  |
| Speed | ë§¤ìš° ëŠë¦¼ | - |
| Cost | ë¹„ìš©ì´ ë§ì´ ë“¦ | - |
| Scalability | ëŒ€ê·œëª¨ í‰ê°€ ë¶ˆê°€ëŠ¥ | - |

**ê²°ë¡ :**

ëª¨ë“  LLM ì¶œë ¥ì„ Humanì´ í‰ê°€í•˜ëŠ” ê²ƒì€ **ì‹¤ìš©ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.**

í•˜ì§€ë§Œ Human ratingsëŠ” ì—¬ì „íˆ ì¤‘ìš”:
- Ground truthë¡œ í™œìš©
- ë‹¤ë¥¸ í‰ê°€ ë°©ë²•ì˜ calibration

---

# 3. Rule-Based Metrics

## 3.1. Rule-Based Metricsë€?

**ìƒˆë¡œìš´ ì ‘ê·¼:**

```
ì´ì „: Prompt â†’ LLM â†’ Response â†’ Human rates
                                   â†“
                              (ë§¤ë²ˆ í‰ê°€)

ìƒˆë¡œìš´:
1. Prompt â†’ Human writes â†’ Reference (í•œ ë²ˆë§Œ)
2. Prompt â†’ LLM â†’ Response
            â†“
   Compare(Response, Reference) â†’ Score
```

**í•µì‹¬ ì•„ì´ë””ì–´:**

1. **Reference ì‘ì„±** (í•œ ë²ˆë§Œ)
   - ê° promptì— ëŒ€í•œ ì´ìƒì ì¸ ë‹µë³€
   - Humanì´ ë¯¸ë¦¬ ì‘ì„±

2. **ìë™ ë¹„êµ**
   - LLM ì‘ë‹µ vs Reference
   - Rule-based formulaë¡œ ì ìˆ˜ ê³„ì‚°

**ì¥ì :**

- ë°˜ë³µ ê°€ëŠ¥ (Repeatable)
- ë¹ ë¦„ (Fast)
- ë¹„ìš© íš¨ìœ¨ì  (Cost-effective)

## 3.2. METEOR

**METEOR: Metric for Evaluation of Translation with Explicit ORdering**

**ì‚¬ìš© ë¶„ì•¼:** ê¸°ê³„ ë²ˆì—­

**í•µì‹¬ ì•„ì´ë””ì–´:**

1. Unigram matching (ë‹¨ì–´ ë§¤ì¹­)
2. Ordering penalty (ìˆœì„œ í˜ë„í‹°)

**ê³µì‹:**

```
METEOR = F_score Ã— (1 - Penalty)

F_score = (Precision Ã— Recall) / (Î± Ã— Precision + (1-Î±) Ã— Recall)

Penalty = Î³ Ã— (C / M)^Î²

ì—¬ê¸°ì„œ:
- Precision: predictionì—ì„œ ë§¤ì¹­ëœ unigram ë¹„ìœ¨
- Recall: referenceì—ì„œ ë§¤ì¹­ëœ unigram ë¹„ìœ¨
- C: contiguous chunk ìˆ˜ (ì—°ì†ëœ ë§¤ì¹­ ë©ì–´ë¦¬ ìˆ˜)
- M: matched unigram ìˆ˜
- Î±, Î³, Î²: í•˜ì´í¼íŒŒë¼ë¯¸í„°
```

**Precision vs Recall:**

```python
reference = "The cat sat on the mat"
prediction = "cat sat on mat"

# Matched words: cat, sat, on, mat (4ê°œ)

Precision = ë§¤ì¹­ëœ ë‹¨ì–´ ìˆ˜ / predictionì˜ ë‹¨ì–´ ìˆ˜
          = 4 / 4 = 1.0

Recall = ë§¤ì¹­ëœ ë‹¨ì–´ ìˆ˜ / referenceì˜ ë‹¨ì–´ ìˆ˜
       = 4 / 6 = 0.67
```

**Ordering Penalty:**

```python
reference = "The cat sat on the mat"
prediction1 = "cat sat on mat"      # ìˆœì„œ ìœ ì§€
prediction2 = "mat on sat cat"      # ìˆœì„œ ë’¤ì„ì„

# prediction1: C=1 (í•˜ë‚˜ì˜ ì—°ì†ëœ ë©ì–´ë¦¬)
# prediction2: C=4 (4ê°œì˜ ë¶„ë¦¬ëœ ë©ì–´ë¦¬)

# Penaltyê°€ prediction2ì—ì„œ ë” ë†’ìŒ
```

**ì˜ˆì‹œ:**

```python
def simple_meteor_score(reference, prediction):
    """
    Simplified METEOR implementation
    """
    ref_words = reference.lower().split()
    pred_words = prediction.lower().split()

    # Matched words
    matched = set(ref_words) & set(pred_words)

    # Precision & Recall
    precision = len(matched) / len(pred_words) if pred_words else 0
    recall = len(matched) / len(ref_words) if ref_words else 0

    # F-score (Î±=0.9)
    alpha = 0.9
    if precision + recall == 0:
        f_score = 0
    else:
        f_score = (precision * recall) / \
                  (alpha * precision + (1-alpha) * recall)

    # Simplified penalty (ì‹¤ì œëŠ” ë” ë³µì¡)
    # C: contiguous chunks
    # ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ penalty=0ìœ¼ë¡œ ê°€ì •
    penalty = 0

    meteor = f_score * (1 - penalty)
    return meteor

# ì˜ˆì‹œ
ref = "The cat sat on the mat"
pred1 = "The cat sat on the mat"
pred2 = "cat mat"

print(f"METEOR (pred1): {simple_meteor_score(ref, pred1):.3f}")
print(f"METEOR (pred2): {simple_meteor_score(ref, pred2):.3f}")
```

**ì¥ì :**

- Synonym ê³ ë ¤ (í™•ì¥ ê°€ëŠ¥)
- Ordering ê³ ë ¤
- Translationì— íš¨ê³¼ì 

**ë‹¨ì :**

- ë§ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° (Î±, Î³, Î²)
- Stylistic variation í—ˆìš© ì•ˆí•¨
- ì—¬ì „íˆ reference í•„ìš”

## 3.3. BLEU

**BLEU: BiLingual Evaluation Understudy**

**íŠ¹ì§•:** Precision-focused metric

**í•µì‹¬ ì•„ì´ë””ì–´:**

```
BLEU = BP Ã— exp(âˆ‘(w_n Ã— log(p_n)))

ì—¬ê¸°ì„œ:
- p_n: n-gram precision
- w_n: weight (ë³´í†µ ê· ë“±)
- BP: Brevity Penalty
```

**N-gram Precision:**

```python
reference = "The cat sat on the mat"
prediction = "The cat sat"

# Unigram (1-gram)
# "The", "cat", "sat" ëª¨ë‘ ë§¤ì¹­
unigram_precision = 3/3 = 1.0

# Bigram (2-gram)
# "The cat", "cat sat" ëª¨ë‘ ë§¤ì¹­
bigram_precision = 2/2 = 1.0
```

**Brevity Penalty (BP):**

```python
# Precisionë§Œ ì‚¬ìš©í•˜ë©´ ì§§ì€ ë¬¸ì¥ì— ìœ ë¦¬
reference = "The cat sat on the mat"  # length=6
prediction = "The cat"                 # length=2

# ë¬¸ì œ: precision=1.0ì´ì§€ë§Œ ë‚´ìš©ì´ ë¶€ì¡±!

# í•´ê²°: Brevity Penalty
BP = exp(1 - len(reference)/len(prediction))
   = exp(1 - 6/2)
   = exp(-2)
   â‰ˆ 0.135

# BLEU scoreê°€ í° í­ìœ¼ë¡œ ê°ì†Œ!
```

**êµ¬í˜„ ì˜ˆì‹œ:**

```python
from collections import Counter

def ngrams(words, n):
    """Generate n-grams from words"""
    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]

def bleu_score(reference, prediction, max_n=4):
    """
    Simplified BLEU implementation
    """
    ref_words = reference.split()
    pred_words = prediction.split()

    # Brevity Penalty
    ref_len = len(ref_words)
    pred_len = len(pred_words)

    if pred_len == 0:
        return 0.0

    if pred_len < ref_len:
        bp = math.exp(1 - ref_len / pred_len)
    else:
        bp = 1.0

    # N-gram precisions
    precisions = []
    for n in range(1, max_n + 1):
        ref_ngrams = Counter(ngrams(ref_words, n))
        pred_ngrams = Counter(ngrams(pred_words, n))

        # Clipped counts
        clipped_counts = sum(min(pred_ngrams[ng], ref_ngrams[ng])
                            for ng in pred_ngrams)
        total_counts = sum(pred_ngrams.values())

        if total_counts == 0:
            precision = 0
        else:
            precision = clipped_counts / total_counts

        precisions.append(precision)

    # Geometric mean
    if min(precisions) == 0:
        return 0.0

    geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))

    return bp * geo_mean

# ì˜ˆì‹œ
ref = "The cat sat on the mat"
pred1 = "The cat sat on the mat"
pred2 = "cat mat"

print(f"BLEU (pred1): {bleu_score(ref, pred1):.3f}")
print(f"BLEU (pred2): {bleu_score(ref, pred2):.3f}")
```

**ì¥ì :**

- ë„ë¦¬ ì‚¬ìš©ë¨
- ë²ˆì—­ í’ˆì§ˆê³¼ ìƒê´€ê´€ê³„
- ë¹ ë¥¸ ê³„ì‚°

**ë‹¨ì :**

- Recall ë¬´ì‹œ
- ì§§ì€ ë¬¸ì¥ì— ìœ ë¦¬ (BPë¡œ ì™„í™”)
- Stylistic variation í—ˆìš© ì•ˆí•¨

## 3.4. ROUGE

**ROUGE: Recall-Oriented Understudy for Gisting Evaluation**

**ì‚¬ìš© ë¶„ì•¼:** ìš”ì•½ (Summarization)

**íŠ¹ì§•:** Recall-focused metric

**Variants:**

1. **ROUGE-N:** N-gram overlap

```python
reference = "The cat sat on the mat"
summary = "The cat sat"

# ROUGE-1 (unigram recall)
matched_unigrams = 3  # "The", "cat", "sat"
total_ref_unigrams = 6

ROUGE-1 = 3 / 6 = 0.5
```

2. **ROUGE-L:** Longest Common Subsequence

```python
reference = "The cat sat on the mat"
summary = "cat on mat"

# LCS: "cat on mat" (ê¸¸ì´ 3)
# (ë‹¨ì–´ ìˆœì„œ ìœ ì§€)

ROUGE-L = LCS_length / ref_length = 3 / 6 = 0.5
```

3. **ROUGE-W:** Weighted LCS (ì—°ì†ì„± ì„ í˜¸)

**êµ¬í˜„ ì˜ˆì‹œ:**

```python
def rouge_n(reference, summary, n=1):
    """
    ROUGE-N score (recall-based)
    """
    ref_words = reference.split()
    sum_words = summary.split()

    ref_ngrams = Counter(ngrams(ref_words, n))
    sum_ngrams = Counter(ngrams(sum_words, n))

    # Overlapping n-grams
    overlap = sum(min(sum_ngrams[ng], ref_ngrams[ng])
                  for ng in sum_ngrams)
    total = sum(ref_ngrams.values())

    if total == 0:
        return 0.0

    return overlap / total

# ì˜ˆì‹œ
ref = "The cat sat on the mat"
sum1 = "The cat sat on the mat"
sum2 = "cat mat"

print(f"ROUGE-1 (sum1): {rouge_n(ref, sum1, 1):.3f}")
print(f"ROUGE-1 (sum2): {rouge_n(ref, sum2, 1):.3f}")
print(f"ROUGE-2 (sum1): {rouge_n(ref, sum1, 2):.3f}")
```

## 3.5. Rule-Based Metricsì˜ í•œê³„

**í•µì‹¬ ë¬¸ì œ:**

**1. Stylistic Variationì„ í—ˆìš©í•˜ì§€ ì•ŠìŒ**

```python
# ê°™ì€ ì˜ë¯¸, ë‹¤ë¥¸ í‘œí˜„
reference = "A plush teddy bear can comfort a child during bedtime."

variation1 = "Soft stuffed bears often help kids feel safe as they fall asleep."

variation2 = "Many youngsters rest more easily at night when they cuddle a gentle toy companion."

# METEOR, BLEU, ROUGE ëª¨ë‘ ë‚®ì€ ì ìˆ˜!
# í•˜ì§€ë§Œ ì˜ë¯¸ëŠ” ë™ì¼í•¨
```

**2. Correlationì´ ë†’ì§€ ì•ŠìŒ**

```python
# Human ratingsì™€ì˜ ìƒê´€ê´€ê³„
correlation_with_human = {
    "BLEU": 0.4,      # ë‚®ìŒ
    "METEOR": 0.55,   # ì¤‘ê°„
    "ROUGE": 0.5      # ì¤‘ê°„
}

# ì™„ë²½í•œ ìƒê´€ê´€ê³„ = 1.0
# ì‹¤ì œë¡œëŠ” ê·¸ë ‡ê²Œ ë†’ì§€ ì•ŠìŒ
```

**3. ì—¬ì „íˆ Referenceê°€ í•„ìš”**

```python
# Reference ì‘ì„± ë¹„ìš©
num_prompts = 1000
time_per_reference = 5  # minutes

total_time = num_prompts * time_per_reference
           = 5000 minutes
           = 83 hours

# ì—¬ì „íˆ ë¹„ìš©ì´ ë§ì´ ë“¦!
```

**ë¹„êµí‘œ:**

| Metric | Focus | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‚¬ë¡€ |
|--------|-------|------|------|-----------|
| METEOR | Precision+Recall+Order | ìˆœì„œ ê³ ë ¤ | ë§ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° | ë²ˆì—­ |
| BLEU | Precision | ë„ë¦¬ ì‚¬ìš©ë¨ | Recall ë¬´ì‹œ | ë²ˆì—­ |
| ROUGE | Recall | ìš”ì•½ì— ì í•© | Precision ë¬´ì‹œ | ìš”ì•½ |

**ê²°ë¡ :**

Rule-based metricsëŠ” **stylistic variationì„ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤.**

ë” ë‚˜ì€ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤! â†’ **LLM-as-a-Judge**

---

# 4. LLM-as-a-Judge

## 4.1. LLM-as-a-Judgeë€?

**í•µì‹¬ ì•„ì´ë””ì–´:**

```
ìš°ë¦¬ëŠ” 7ê°œ ê°•ì˜ ë™ì•ˆ LLMì— ëŒ€í•´ ë°°ì› ìŠµë‹ˆë‹¤:
- ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ pre-training
- Human preferenceë¡œ fine-tuning
- Human knowledge ë‚´ì¬í™”

ê·¸ë ‡ë‹¤ë©´... LLMì„ í‰ê°€ì— ì‚¬ìš©í•˜ë©´ ì–´ë–¨ê¹Œ?
```

**Setup:**

```
Input:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt              â”‚  (ì›ë³¸ ì§ˆë¬¸)
â”‚ Response            â”‚  (LLM ì‘ë‹µ)
â”‚ Evaluation Criteria â”‚  (í‰ê°€ ê¸°ì¤€)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    LLM-as-a-Judge
         â†“
Output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Score               â”‚  (ì ìˆ˜)
â”‚ Rationale           â”‚  (ì´ìœ )
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì°¨ë³„ì :**

Rule-based metricsì™€ ë‹¬ë¦¬:
- Reference ë¶ˆí•„ìš”!
- Rationale ì œê³µ! (ì„¤ëª… ê°€ëŠ¥)

## 4.2. ê¸°ë³¸ Setup

**Prompt Template:**

```python
prompt_template = """
You are an expert evaluator. Your task is to evaluate the quality of a response.

**Evaluation Criteria:** {criteria}

**User Prompt:** {user_prompt}

**Model Response:** {model_response}

Please provide:
1. Rationale: Explain your reasoning step-by-step
2. Score: Choose either "pass" or "fail"

Output format:
Rationale: <your explanation>
Score: <pass/fail>
"""
```

**ì˜ˆì‹œ:**

```python
# Usefulness í‰ê°€
criteria = "Usefulness: Does the response provide helpful information to answer the question?"

user_prompt = "What birthday gift should I get?"

model_response = "A teddy bear is almost always a sweet gift. Just pick one that feels right for you."

# LLM-as-a-Judge call
judge_input = prompt_template.format(
    criteria=criteria,
    user_prompt=user_prompt,
    model_response=model_response
)

judge_output = judge_llm.generate(judge_input)
print(judge_output)
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Rationale: The response provides a concrete suggestion (teddy bear)
which gives the user a starting point. However, it lacks specifics
about age appropriateness, interests, or budget considerations.
The advice to "pick one that feels right" is somewhat generic.

Score: pass
```

**í•µì‹¬ íŠ¸ë¦­: Rationale First!**

```python
# âœ… Good: Rationale ë¨¼ì €
output_format = """
Rationale: <explanation>
Score: <pass/fail>
"""

# âŒ Bad: Score ë¨¼ì €
output_format = """
Score: <pass/fail>
Rationale: <explanation>
"""
```

**ì™œ Rationaleì„ ë¨¼ì € ì¶œë ¥í•´ì•¼ í•˜ë‚˜?**

Lecture 6 (Reasoning)ì—ì„œ ë°°ìš´ ë‚´ìš©:
- Chain-of-Thought (CoT)
- Reasoning models (o1, o3)

â†’ ìƒê° ê³¼ì •ì„ ë¨¼ì € externalizeí•˜ë©´ ì„±ëŠ¥ í–¥ìƒ!

```python
# CoTì™€ ê°™ì€ ì›ë¦¬
# Think â†’ Answer

# Judgeì—ì„œë„ ë™ì¼
# Explain reasoning â†’ Give score
```

## 4.3. Structured Outputs

**ë¬¸ì œ:**

```python
# LLM outputì€ probabilistic
judge_output = judge_llm.generate(prompt)

# ì›í•˜ëŠ” í˜•ì‹:
# Rationale: ...
# Score: pass

# ì‹¤ì œ output:
# "I think this is good..."  (íŒŒì‹± ë¶ˆê°€!)
```

**í•´ê²°: Constrained Decoding**

Lecture 3ì—ì„œ ë°°ìš´ ë‚´ìš©:
- Constrained-guided decoding
- Valid tokensë§Œ sampling

**Provider API:**

```python
from pydantic import BaseModel

# 1. Response í˜•ì‹ ì •ì˜
class JudgeResponse(BaseModel):
    rationale: str
    score: str  # "pass" or "fail"

# 2. Structured output ì‚¬ìš©
judge_output = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": judge_prompt}],
    response_format=JudgeResponse  # âœ… í˜•ì‹ ë³´ì¥!
)

# 3. íŒŒì‹± ë³´ì¥ë¨
rationale = judge_output.rationale
score = judge_output.score
```

**OpenAI ì˜ˆì‹œ:**

```python
import openai
from pydantic import BaseModel

class EvaluationResult(BaseModel):
    rationale: str
    score: str  # "pass" or "fail"

response = openai.beta.chat.completions.parse(
    model="gpt-4-turbo",
    messages=[
        {"role": "system", "content": "You are an expert evaluator."},
        {"role": "user", "content": judge_prompt}
    ],
    response_format=EvaluationResult
)

result = response.choices[0].message.parsed
print(f"Score: {result.score}")
print(f"Rationale: {result.rationale}")
```

**Anthropic ì˜ˆì‹œ:**

```python
import anthropic

# Tool/function callingìœ¼ë¡œ êµ¬ì¡°í™”
tools = [{
    "name": "submit_evaluation",
    "description": "Submit evaluation result",
    "input_schema": {
        "type": "object",
        "properties": {
            "rationale": {"type": "string"},
            "score": {"type": "string", "enum": ["pass", "fail"]}
        },
        "required": ["rationale", "score"]
    }
}]

response = anthropic.messages.create(
    model="claude-3-opus-20240229",
    messages=[{"role": "user", "content": judge_prompt}],
    tools=tools,
    tool_choice={"type": "tool", "name": "submit_evaluation"}
)

# Structured output ë³´ì¥
tool_use = response.content[0]
result = tool_use.input
```

## 4.4. LLM-as-a-Judgeì˜ ì¥ì 

**1. Reference ë¶ˆí•„ìš”**

```python
# Rule-based metrics
bleu_score(reference, prediction)  # reference í•„ìš” âŒ

# LLM-as-a-Judge
judge_score(prompt, response, criteria)  # reference ë¶ˆí•„ìš” âœ…
```

**2. Interpretable (í•´ì„ ê°€ëŠ¥)**

```python
# Rule-based
score = 0.4523  # ë¬´ìŠ¨ ì˜ë¯¸? ğŸ¤·

# LLM-as-a-Judge
{
    "score": "fail",
    "rationale": "The response lacks specific details about pricing
                  and doesn't address the budget constraint mentioned
                  in the question."  # ëª…í™•í•œ ì´ìœ ! âœ…
}
```

**3. Flexible Criteria**

```python
# ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ ê°€ëŠ¥
criteria_list = [
    "Usefulness",
    "Factuality",
    "Safety",
    "Coherence",
    "Politeness",
    "Conciseness"
]

for criteria in criteria_list:
    score = judge(prompt, response, criteria)
```

**4. No Training Required**

```python
# Rule-based metrics
# â†’ Hyperparameter tuning í•„ìš” (Î±, Î², Î³...)

# LLM-as-a-Judge
# â†’ Zero-shotìœ¼ë¡œ ì‘ë™! (LLMì˜ ì‚¬ì „ ì§€ì‹ í™œìš©)
```

## 4.5. Variants

### Pointwise Evaluation

**ë‹¨ì¼ ì‘ë‹µ í‰ê°€:**

```
Input:  1ê°œì˜ response
Output: "pass" or "fail"
```

**ì˜ˆì‹œ:**

```python
def pointwise_judge(prompt, response, criteria):
    """
    ë‹¨ì¼ ì‘ë‹µì„ ì ˆëŒ€ì ìœ¼ë¡œ í‰ê°€
    """
    judge_prompt = f"""
    Evaluate this response based on: {criteria}

    Prompt: {prompt}
    Response: {response}

    Output:
    Rationale: <explanation>
    Score: pass/fail
    """

    result = judge_llm.generate(judge_prompt)
    return parse_result(result)

# ì‚¬ìš©
score = pointwise_judge(
    prompt="What is the capital of France?",
    response="The capital of France is Paris.",
    criteria="Factual accuracy"
)
```

### Pairwise Evaluation

**ë‘ ì‘ë‹µ ë¹„êµ:**

```
Input:  2ê°œì˜ responses (A, B)
Output: "A is better" or "B is better"
```

**ì˜ˆì‹œ:**

```python
def pairwise_judge(prompt, response_a, response_b, criteria):
    """
    ë‘ ì‘ë‹µì„ ìƒëŒ€ì ìœ¼ë¡œ ë¹„êµ
    """
    judge_prompt = f"""
    Compare these two responses based on: {criteria}

    Prompt: {prompt}

    Response A: {response_a}
    Response B: {response_b}

    Which response is better?

    Output:
    Rationale: <explanation>
    Winner: A/B
    """

    result = judge_llm.generate(judge_prompt)
    return parse_result(result)

# ì‚¬ìš©
winner = pairwise_judge(
    prompt="Explain quantum computing",
    response_a="Quantum computing uses qubits...",
    response_b="Quantum computers are fast...",
    criteria="Clarity and accuracy"
)
```

**Pairwiseì˜ í™œìš©: Preference Data ìƒì„±**

Lecture 5 (Fine-tuning)ì—ì„œ ë°°ìš´ ë‚´ìš©:
- DPO, PPO ë“±ì€ preference data í•„ìš”
- (prompt, chosen, rejected) ìŒ

```python
# Synthetic preference data generation
prompts = load_prompts()

for prompt in prompts:
    # ë‘ ëª¨ë¸ì—ì„œ ì‘ë‹µ ìƒì„±
    response_a = model_a.generate(prompt)
    response_b = model_b.generate(prompt)

    # LLM-as-a-Judgeë¡œ preference íŒë‹¨
    winner = pairwise_judge(prompt, response_a, response_b, "quality")

    if winner == "A":
        preference_data.append({
            "prompt": prompt,
            "chosen": response_a,
            "rejected": response_b
        })
    else:
        preference_data.append({
            "prompt": prompt,
            "chosen": response_b,
            "rejected": response_a
        })

# DPO trainingì— ì‚¬ìš©!
train_dpo(model, preference_data)
```

**Pointwise vs Pairwise ë¹„êµ:**

| ì¸¡ë©´ | Pointwise | Pairwise |
|------|-----------|----------|
| ì…ë ¥ | 1ê°œ response | 2ê°œ responses |
| ì¶œë ¥ | ì ˆëŒ€ì  í‰ê°€ | ìƒëŒ€ì  ë¹„êµ |
| ì‚¬ìš© ì‚¬ë¡€ | Quality check | Model comparison |
| Preference data | âŒ | âœ… |
| ì†ë„ | ë¹ ë¦„ | ëŠë¦¼ (2ë°° ì…ë ¥) |

---

# 5. LLM-as-a-Judgeì˜ Biases

LLM-as-a-Judgeë„ ì™„ë²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì—¬ëŸ¬ biasê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

## 5.1. Position Bias

**ì •ì˜:** ì‘ë‹µì˜ ìœ„ì¹˜ì— ë”°ë¼ í‰ê°€ê°€ ë‹¬ë¼ì§€ëŠ” í˜„ìƒ

**ì˜ˆì‹œ:**

```python
# Pairwise evaluation
prompt = "Explain AI"
response_a = "AI is ..."  # ì¢‹ì€ ì‘ë‹µ
response_b = "AI means ..."  # ë‚˜ìœ ì‘ë‹µ

# Case 1: Aë¥¼ ë¨¼ì € ì œì‹œ
result1 = pairwise_judge(prompt, response_a, response_b)
# Output: "A is better" âœ…

# Case 2: Bë¥¼ ë¨¼ì € ì œì‹œ
result2 = pairwise_judge(prompt, response_b, response_a)
# Output: "B is better" âŒ (Position bias!)
```

**ì›ì¸:**

- LLMì´ ì²˜ìŒ ë³¸ ê²ƒì— bias
- "Primacy effect" (ì²« ë²ˆì§¸ ê²ƒ ì„ í˜¸)

**ì™„í™” ë°©ë²•:**

**1. Position Swapping + Majority Voting**

```python
def robust_pairwise_judge(prompt, response_a, response_b, criteria):
    """
    Position biasë¥¼ ì™„í™”í•œ pairwise judge
    """
    # 1st call: A then B
    result1 = pairwise_judge(prompt, response_a, response_b, criteria)

    # 2nd call: B then A
    result2 = pairwise_judge(prompt, response_b, response_a, criteria)

    # Majority voting
    if result1 == "A" and result2 == "B":
        # Consistent: A is better
        return "A"
    elif result1 == "B" and result2 == "A":
        # Consistent: B is better
        return "B"
    else:
        # Inconsistent: Position bias detected
        # Tie-breaking strategy (ì˜ˆ: ì¬í‰ê°€)
        return "Uncertain"

# ì‚¬ìš©
winner = robust_pairwise_judge(
    prompt="What is AI?",
    response_a="...",
    response_b="...",
    criteria="Clarity"
)
```

**2. ë‹¤ì¤‘ í‰ê°€ + í†µê³„**

```python
def multi_trial_judge(prompt, response_a, response_b, criteria, n_trials=5):
    """
    ì—¬ëŸ¬ ë²ˆ í‰ê°€í•˜ì—¬ robustí•œ ê²°ê³¼ ë„ì¶œ
    """
    votes = []

    for _ in range(n_trials):
        # Random order
        if random.random() < 0.5:
            result = pairwise_judge(prompt, response_a, response_b, criteria)
            votes.append("A" if result == "A" else "B")
        else:
            result = pairwise_judge(prompt, response_b, response_a, criteria)
            votes.append("B" if result == "A" else "A")

    # Majority vote
    a_count = votes.count("A")
    b_count = votes.count("B")

    if a_count > b_count:
        return "A"
    else:
        return "B"
```

## 5.2. Verbosity Bias

**ì •ì˜:** ë” ê¸´(verbose) ì‘ë‹µì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥

**ì˜ˆì‹œ:**

```python
prompt = "What is 2+2?"

# ì§§ê³  ì •í™•í•œ ì‘ë‹µ
response_short = "4"

# ê¸¸ì§€ë§Œ ë¶ˆí•„ìš”í•œ ì‘ë‹µ
response_verbose = """
To answer this question, let's break it down step by step:
1. We start with the number 2
2. We add another 2 to it
3. The operation of addition combines these values
4. Using basic arithmetic principles
5. The result is 4

Additionally, this demonstrates fundamental mathematical concepts...
(continues for 3 more paragraphs)
"""

# LLM-as-a-Judgeê°€ verboseë¥¼ ì„ í˜¸í•  ìˆ˜ ìˆìŒ!
result = pairwise_judge(prompt, response_short, response_verbose, "quality")
# Might prefer verbose âŒ
```

**ì™„í™” ë°©ë²•:**

**1. Explicit Guidelines**

```python
criteria = """
Quality: Evaluate based on correctness and conciseness.
**Important:** Do NOT prefer responses simply because they are longer.
Focus on whether the response correctly and efficiently answers the question.
"""

result = pairwise_judge(prompt, response_a, response_b, criteria)
```

**2. In-Context Examples**

```python
judge_prompt = f"""
Here are examples of good evaluations:

Example 1:
Q: What is 2+2?
Response A: "4"
Response B: "The answer is 4. Let me explain..."
Better: A (concise and correct)

Example 2:
Q: Explain quantum entanglement
Response A: "It's complicated"
Response B: "Quantum entanglement is a phenomenon where..."
Better: B (provides actual explanation)

Now evaluate:
Q: {prompt}
Response A: {response_a}
Response B: {response_b}
"""
```

**3. Length Penalty**

```python
def length_adjusted_score(prompt, response, criteria):
    """
    ê¸¸ì´ë¥¼ ê³ ë ¤í•œ ì ìˆ˜ ì¡°ì •
    """
    # Base score
    score = pointwise_judge(prompt, response, criteria)

    # Length penalty
    words = len(response.split())
    expected_length = estimate_expected_length(prompt)

    if words > expected_length * 2:
        # Too verbose: apply penalty
        penalty = 0.1 * (words / expected_length - 2)
        score = score - penalty

    return score
```

## 5.3. Self-Enhancement Bias

**ì •ì˜:** ìì‹ ì´ ìƒì„±í•œ ì‘ë‹µì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥

**ì˜ˆì‹œ:**

```python
# GPT-4ê°€ ìƒì„±í•œ ì‘ë‹µë“¤
response_gpt4_a = gpt4.generate(prompt)
response_gpt4_b = gpt4.generate(prompt)

# GPT-4ë¥¼ judgeë¡œ ì‚¬ìš©
judge = gpt4

# ë¬¸ì œ: GPT-4ê°€ ìì‹ ì˜ ì¶œë ¥ì„ ì„ í˜¸í•  ìˆ˜ ìˆìŒ
result = pairwise_judge(
    prompt=prompt,
    response_a=response_gpt4_a,
    response_b=response_claude,  # ë‹¤ë¥¸ ëª¨ë¸
    criteria="quality",
    judge_model=judge  # âš ï¸ Same model!
)
# Might prefer response_gpt4_a due to self-enhancement bias
```

**ì›ì¸:**

```
ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í–ˆë‹¤ëŠ” ê²ƒì€
â†’ í•´ë‹¹ sequenceì˜ probabilityê°€ ë†’ë‹¤ê³  íŒë‹¨
â†’ Judgeë¡œ ì‚¬ìš©ë  ë•Œë„ ê·¸ ì‘ë‹µì„ "ë” ê·¸ëŸ´ë“¯í•˜ê²Œ" ì¸ì‹
```

**ì™„í™” ë°©ë²•:**

**1. ë‹¤ë¥¸ ëª¨ë¸ì„ Judgeë¡œ ì‚¬ìš©**

```python
# âœ… Good: ìƒì„± ëª¨ë¸ â‰  Judge ëª¨ë¸
generator = gpt4
judge = claude  # ë‹¤ë¥¸ ëª¨ë¸!

response_a = generator.generate(prompt)
response_b = another_model.generate(prompt)

result = pairwise_judge(
    prompt=prompt,
    response_a=response_a,
    response_b=response_b,
    criteria="quality",
    judge_model=judge  # âœ… Different model
)
```

**2. ë” ê°•ë ¥í•œ Judge ì‚¬ìš©**

```python
# ìƒì„±: ì‘ì€ ëª¨ë¸
generator = gpt_3_5_turbo

# Judge: í° ëª¨ë¸ (reasoning ëŠ¥ë ¥ ìš°ìˆ˜)
judge = gpt_4_turbo  # ë˜ëŠ” o1, claude-opus

# í° ëª¨ë¸ì´ ì‘ì€ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë” ê°ê´€ì ìœ¼ë¡œ í‰ê°€ ê°€ëŠ¥
```

**3. Ensemble Judges**

```python
def ensemble_judge(prompt, response_a, response_b, criteria):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì„ judgeë¡œ ì‚¬ìš©
    """
    judges = [gpt4, claude, gemini]
    votes = []

    for judge in judges:
        result = pairwise_judge(
            prompt, response_a, response_b, criteria,
            judge_model=judge
        )
        votes.append(result)

    # Majority voting
    return max(set(votes), key=votes.count)
```

**Bias ìš”ì•½:**

| Bias | ì„¤ëª… | ì™„í™” ë°©ë²• |
|------|------|-----------|
| Position Bias | ë¨¼ì € ì œì‹œëœ ì‘ë‹µ ì„ í˜¸ | Position swapping + voting |
| Verbosity Bias | ê¸´ ì‘ë‹µ ì„ í˜¸ | Explicit guidelines, examples |
| Self-Enhancement | ìì‹ ì˜ ì¶œë ¥ ì„ í˜¸ | ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©, í° ëª¨ë¸ ì‚¬ìš© |

---

# 6. Best Practices

## 6.1. ëª…í™•í•œ Guidelines

**ë‚˜ìœ ì˜ˆ:**

```python
criteria = "Evaluate the quality"
# ë„ˆë¬´ ëª¨í˜¸í•¨! "quality"ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜?
```

**ì¢‹ì€ ì˜ˆ:**

```python
criteria = """
Evaluate the response based on these specific dimensions:

1. Factual Accuracy:
   - Are all facts correct?
   - Are there any hallucinations?

2. Completeness:
   - Does it fully answer the question?
   - Are important details missing?

3. Clarity:
   - Is it easy to understand?
   - Is the structure logical?

Score "pass" if ALL dimensions are satisfactory.
Score "fail" if ANY dimension is unsatisfactory.
"""
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ í¬í•¨:**

```python
criteria = """
Usefulness for gift recommendations:

âœ… Pass criteria:
- Provides specific gift suggestions
- Considers recipient's characteristics (age, interests)
- Explains why the gift is appropriate

âŒ Fail criteria:
- Generic advice ("just get something nice")
- No specific suggestions
- Ignores context from the question

Example of PASS:
"For a 5-year-old who loves dinosaurs, consider a dinosaur toy set
or a picture book about dinosaurs."

Example of FAIL:
"Get them something nice."
"""
```

## 6.2. Binary Scale ì‚¬ìš©

**ë‚˜ìœ ì˜ˆ: Granular Scale**

```python
# 1-5 scale
score_options = ["1", "2", "3", "4", "5"]
# ë¬¸ì œ: 3ê³¼ 4ì˜ ì°¨ì´ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ
```

**ì¢‹ì€ ì˜ˆ: Binary Scale**

```python
# Pass/Fail
score_options = ["pass", "fail"]
# ëª…í™•í•¨: ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ”ê°€, ì•„ë‹Œê°€?
```

**ì´ìœ :**

1. **íŒë‹¨ì´ ì‰¬ì›€**
   ```python
   # Humanë„ binaryê°€ ì‰¬ì›€
   "Is this good enough?" â†’ Yes/No

   # vs
   "Rate this 1-5" â†’ 3? 4? ì• ë§¤í•¨
   ```

2. **Noise ê°ì†Œ**
   ```python
   # 5-scale
   rater1: 3
   rater2: 4
   # ì˜ê²¬ ì°¨ì´ì¸ê°€, noiseì¸ê°€?

   # Binary
   rater1: pass
   rater2: pass
   # ëª…í™•í•œ agreement
   ```

3. **Calibrationì´ ì‰¬ì›€**
   ```python
   # Binary: í•˜ë‚˜ì˜ thresholdë§Œ ì¡°ì •
   if quality_score > threshold:
       return "pass"

   # Multi-scale: ì—¬ëŸ¬ threshold ì¡°ì • í•„ìš”
   ```

## 6.3. Rationale First

**í•­ìƒ rationaleì„ ë¨¼ì € ì¶œë ¥:**

```python
# âœ… Good
output_format = """
First, provide your reasoning:
Rationale: <step-by-step explanation>

Then, provide your score:
Score: pass/fail
"""

# âŒ Bad
output_format = """
Score: pass/fail
Rationale: <explanation>
"""
```

**ì´ìœ : Chain-of-Thought íš¨ê³¼**

```python
# Analogy to CoT reasoning (Lecture 6)

# Without CoT:
Q: "What is 17 * 23?"
A: "391"  # May be wrong

# With CoT:
Q: "What is 17 * 23?"
A: "Let me break this down:
    17 * 20 = 340
    17 * 3 = 51
    340 + 51 = 391"  # More likely to be correct

# Same for Judge:
# Thinking first â†’ Better judgment
```

## 6.4. Bias ì™„í™”

**Position Bias:**

```python
def evaluate_with_position_mitigation(prompt, response_a, response_b):
    # Evaluate both orders
    result_ab = judge(prompt, response_a, response_b)
    result_ba = judge(prompt, response_b, response_a)

    # Check consistency
    if result_ab == result_ba:
        return result_ab  # Consistent
    else:
        # Inconsistent: Run again or use tie-breaker
        return "uncertain"
```

**Verbosity Bias:**

```python
guidelines = """
IMPORTANT: Do not prefer a response simply because it is longer.
Evaluate based on:
1. Correctness
2. Completeness
3. Conciseness (brevity is a virtue!)
"""
```

**Self-Enhancement Bias:**

```python
# Use different model for judging
generator_model = "gpt-3.5-turbo"
judge_model = "gpt-4-turbo"  # Stronger, different model

# Or use specialized judge models
judge_model = "prometheus-eval"  # Specialized for evaluation
```

## 6.5. Human Calibration

**ì •ê¸°ì ìœ¼ë¡œ human ratingsì™€ ë¹„êµ:**

```python
def calibration_study(judge_model, test_cases, human_ratings):
    """
    Judgeì™€ human ratings ë¹„êµ
    """
    judge_ratings = []

    for case in test_cases:
        judge_rating = judge_model.evaluate(
            prompt=case["prompt"],
            response=case["response"],
            criteria=case["criteria"]
        )
        judge_ratings.append(judge_rating)

    # Correlation analysis
    from scipy.stats import pearsonr

    correlation, p_value = pearsonr(human_ratings, judge_ratings)

    print(f"Correlation: {correlation:.3f}")
    print(f"P-value: {p_value:.3f}")

    if correlation < 0.7:
        print("âš ï¸ Low correlation! Revise judge prompt.")

    return correlation

# ì‚¬ìš©
test_cases = load_test_cases()
human_ratings = load_human_ratings()

correlation = calibration_study(judge_model, test_cases, human_ratings)
```

**Calibration workflow:**

```python
"""
1. ì†Œê·œëª¨ human rating ìˆ˜ì§‘ (100-500 examples)
2. Same examplesì— ëŒ€í•´ LLM-as-a-Judge ì‹¤í–‰
3. Correlation ë¶„ì„
4. Correlation < 0.7ì´ë©´:
   - Judge prompt ìˆ˜ì •
   - Guidelines ëª…í™•í™”
   - ë‹¤ë¥¸ judge model ì‹œë„
5. Correlation >= 0.7ì´ë©´:
   - LLM-as-a-Judge ì‹ ë¢°í•˜ê³  ëŒ€ê·œëª¨ í‰ê°€
"""
```

## 6.6. Low Temperature

**Reproducibilityë¥¼ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©:**

```python
# âœ… For evaluation: Low temperature
judge_result = judge_llm.generate(
    prompt=judge_prompt,
    temperature=0.1  # or 0.2
)
# ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€

# âŒ For evaluation: High temperature
judge_result = judge_llm.generate(
    prompt=judge_prompt,
    temperature=0.9
)
# ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼ â†’ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ
```

**ì´ìœ :**

```python
# ë‚®ì€ temperature
temperature = 0.1
# â†’ ê±°ì˜ deterministic
# â†’ ê°™ì€ ì…ë ¥ â†’ ê°™ì€ ì¶œë ¥ (ëŒ€ë¶€ë¶„)

# ë†’ì€ temperature
temperature = 0.9
# â†’ í™•ë¥ ì  sampling
# â†’ ê°™ì€ ì…ë ¥ â†’ ë‹¤ë¥¸ ì¶œë ¥ ê°€ëŠ¥
```

**Best Practices ìš”ì•½:**

| Practice | Why | How |
|----------|-----|-----|
| ëª…í™•í•œ Guidelines | ëª¨í˜¸í•¨ ì œê±° | êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨ |
| Binary Scale | íŒë‹¨ ì‰¬ì›€, Noise ê°ì†Œ | Pass/Fail |
| Rationale First | CoT íš¨ê³¼ | Reasoning â†’ Score |
| Bias ì™„í™” | ê³µì •í•œ í‰ê°€ | Position swapping ë“± |
| Human Calibration | Ground truth í™•ì¸ | Correlation ë¶„ì„ |
| Low Temperature | ì¬í˜„ì„± | temp=0.1-0.2 |

---

# 7. Evaluation Dimensions

## 7.1. Task Performance

**í‰ê°€í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì°¨ì›:**

```python
evaluation_dimensions = {
    # Task Performance
    "usefulness": "Does the response help the user?",
    "relevance": "Is the response relevant to the question?",
    "completeness": "Does it fully answer the question?",

    # Response Format
    "coherence": "Is the response logically structured?",
    "tone": "Is the tone appropriate?",
    "style": "Does it match the desired style?",

    # Safety & Alignment
    "safety": "Is the response safe and ethical?",
    "factuality": "Are all facts correct?",
    "bias": "Is the response free from harmful biases?"
}
```

**ì˜ˆì‹œ: Usefulness í‰ê°€**

```python
criteria = """
Usefulness: Does the response provide actionable information
that helps the user achieve their goal?

Pass criteria:
- Provides specific, actionable advice
- Addresses the user's actual question
- Gives relevant details

Fail criteria:
- Too vague or generic
- Doesn't address the question
- Missing important information
"""

result = pointwise_judge(
    prompt="How do I bake a cake?",
    response="Mix ingredients and bake.",
    criteria=criteria
)
# Likely: "fail" (too vague)
```

## 7.2. Factuality

**FactualityëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.**

**ì™œ íŠ¹ë³„í•œê°€?**

```python
# ë‹¤ë¥¸ ì°¨ì›ë“¤
usefulness: "Is this useful?" â†’ Yes/No (ì£¼ê´€ì )
coherence: "Is this coherent?" â†’ Yes/No (ì£¼ê´€ì )

# Factuality
factuality: "Is this factually correct?" â†’ Needs verification!
# ë‹¨ìˆœíˆ "ë§ë‹¤/í‹€ë¦¬ë‹¤"ë¥¼ íŒë‹¨í•  ìˆ˜ ì—†ìŒ
# ì™¸ë¶€ ì§€ì‹ í•„ìš”
```

**ë¬¸ì œ:**

```python
text = """
Teddy bears, first created in the 1920s, were named after
President Theodore Roosevelt after he proudly wanted to shoot
a captured bear on a hunting trip.
"""

# ì§ˆë¬¸: ì´ í…ìŠ¤íŠ¸ëŠ” ì–¼ë§ˆë‚˜ ì‚¬ì‹¤ì ì¸ê°€?

# ë¬¸ì œ:
# 1. ì—¬ëŸ¬ factê°€ ì„ì—¬ ìˆìŒ
# 2. ì¼ë¶€ëŠ” ë§ê³  ì¼ë¶€ëŠ” í‹€ë¦¼
# 3. Binary (pass/fail)ë¡œëŠ” nuance í¬ì°© ëª»í•¨
```

**í•´ê²°ì±…: 3-Step Factuality Evaluation**

### Fact Extraction

**Step 1: í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ factë¡œ ë¶„í•´**

```python
def extract_facts(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ fact ì¶”ì¶œ
    """
    prompt = f"""
    Extract individual factual claims from this text.
    Each claim should be atomic (one fact per claim).

    Text: {text}

    Output format:
    1. <fact 1>
    2. <fact 2>
    ...
    """

    facts = llm.generate(prompt)
    return parse_facts(facts)

# ì˜ˆì‹œ
text = """
Teddy bears, first created in the 1920s, were named after
President Theodore Roosevelt after he proudly wanted to shoot
a captured bear on a hunting trip.
"""

facts = extract_facts(text)
print(facts)
```

**ì¶œë ¥:**

```
Facts:
1. Teddy bears were first created in the 1920s
2. Teddy bears were named after President Theodore Roosevelt
3. The name came from a hunting trip incident
4. President Roosevelt proudly wanted to shoot a captured bear
```

### Fact Checking

**Step 2: ê° factë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€ì¦**

```python
def check_fact(fact, use_rag=True, use_web_search=True):
    """
    ê°œë³„ fact ê²€ì¦
    """
    evidence = []

    # RAG: Knowledge base ê²€ìƒ‰
    if use_rag:
        relevant_docs = rag_system.retrieve(fact)
        evidence.extend(relevant_docs)

    # Web search
    if use_web_search:
        search_results = web_search(fact)
        evidence.extend(search_results)

    # Judge: Evidence ê¸°ë°˜ ê²€ì¦
    prompt = f"""
    Fact to verify: {fact}

    Evidence:
    {format_evidence(evidence)}

    Is this fact correct?
    Rationale: <explanation>
    Verdict: correct/incorrect
    """

    result = judge_llm.generate(prompt)
    return parse_result(result)

# ì˜ˆì‹œ
facts = [
    "Teddy bears were first created in the 1920s",
    "Teddy bears were named after President Theodore Roosevelt",
    "The name came from a hunting trip incident",
    "President Roosevelt proudly wanted to shoot a captured bear"
]

fact_results = []
for fact in facts:
    result = check_fact(fact)
    fact_results.append(result)
    print(f"{fact}: {result['verdict']}")
```

**ì¶œë ¥:**

```
Teddy bears were first created in the 1920s: incorrect
  (Actually 1900s)

Teddy bears were named after President Theodore Roosevelt: correct

The name came from a hunting trip incident: correct

President Roosevelt proudly wanted to shoot a captured bear: incorrect
  (He actually refused to shoot it)
```

### Aggregation

**Step 3: ê°œë³„ fact ê²°ê³¼ë¥¼ ì¢…í•©**

```python
def aggregate_factuality_score(fact_results, weights=None):
    """
    ê°œë³„ fact ê²€ì¦ ê²°ê³¼ë¥¼ ì¢…í•©
    """
    if weights is None:
        # Equal weights
        weights = [1.0] * len(fact_results)

    # Normalize weights
    total_weight = sum(weights)
    weights = [w / total_weight for w in weights]

    # Weighted sum
    score = sum(
        w * (1 if result['verdict'] == 'correct' else 0)
        for w, result in zip(weights, fact_results)
    )

    return score

# ì˜ˆì‹œ: Equal weights
fact_results = [
    {'verdict': 'incorrect'},  # 1920s â†’ 1900s
    {'verdict': 'correct'},    # Named after Roosevelt
    {'verdict': 'correct'},    # Hunting trip
    {'verdict': 'incorrect'}   # Proudly wanted â†’ refused
]

score = aggregate_factuality_score(fact_results)
print(f"Factuality Score: {score:.2f}")
# Output: 0.50 (2 correct out of 4)
```

**ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜:**

```python
# ì¼ë¶€ factê°€ ë” ì¤‘ìš”í•  ìˆ˜ ìˆìŒ
facts_with_importance = [
    {"fact": "Named after Roosevelt", "importance": 3},  # í•µì‹¬
    {"fact": "1920s", "importance": 1},  # ëœ ì¤‘ìš”
    {"fact": "Hunting trip", "importance": 2},
    {"fact": "Proudly wanted to shoot", "importance": 1}
]

weights = [f["importance"] for f in facts_with_importance]
score = aggregate_factuality_score(fact_results, weights)
print(f"Weighted Factuality Score: {score:.2f}")
```

**ì „ì²´ Pipeline:**

```python
def evaluate_factuality(text):
    """
    Complete factuality evaluation pipeline
    """
    # Step 1: Extract facts
    facts = extract_facts(text)
    print(f"Extracted {len(facts)} facts")

    # Step 2: Check each fact
    fact_results = []
    for i, fact in enumerate(facts):
        print(f"Checking fact {i+1}/{len(facts)}...")
        result = check_fact(fact)
        fact_results.append(result)

    # Step 3: Aggregate
    score = aggregate_factuality_score(fact_results)

    # Detailed report
    report = {
        "overall_score": score,
        "total_facts": len(facts),
        "correct_facts": sum(1 for r in fact_results if r['verdict'] == 'correct'),
        "details": [
            {
                "fact": fact,
                "verdict": result['verdict'],
                "rationale": result['rationale']
            }
            for fact, result in zip(facts, fact_results)
        ]
    }

    return report

# ì‚¬ìš©
text = """
Teddy bears, first created in the 1920s, were named after
President Theodore Roosevelt after he proudly wanted to shoot
a captured bear on a hunting trip.
"""

report = evaluate_factuality(text)
print(json.dumps(report, indent=2))
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```json
{
  "overall_score": 0.5,
  "total_facts": 4,
  "correct_facts": 2,
  "details": [
    {
      "fact": "Teddy bears were first created in the 1920s",
      "verdict": "incorrect",
      "rationale": "Teddy bears were actually first created around 1902-1903, not in the 1920s."
    },
    {
      "fact": "Teddy bears were named after President Theodore Roosevelt",
      "verdict": "correct",
      "rationale": "This is correct. The toy was named after Theodore 'Teddy' Roosevelt."
    },
    {
      "fact": "The name came from a hunting trip incident",
      "verdict": "correct",
      "rationale": "Correct. The name originated from a 1902 hunting trip in Mississippi."
    },
    {
      "fact": "President Roosevelt proudly wanted to shoot a captured bear",
      "verdict": "incorrect",
      "rationale": "Incorrect. Roosevelt actually refused to shoot the bear, considering it unsportsmanlike."
    }
  ]
}
```

---

# 8. Agent Evaluation

## 8.1. Agentì˜ Inner Working

Lecture 7ì—ì„œ ë°°ìš´ ë‚´ìš©:
- ReAct framework
- Observe, Plan, Act

**Agent êµ¬ì¡°:**

```python
def agent_loop(query):
    """
    Agentic workflow
    """
    state = {"query": query, "observations": []}

    while not is_goal_achieved(state):
        # 1. Observe
        observation = observe(state)
        state["observations"].append(observation)

        # 2. Plan
        plan = llm.plan(state)

        # 3. Act
        if plan["action_type"] == "tool_call":
            # Tool selection & execution
            tool_name = plan["tool_name"]
            tool_args = plan["tool_args"]

            result = execute_tool(tool_name, tool_args)
            state["tool_results"].append(result)
        elif plan["action_type"] == "final_answer":
            return plan["answer"]

    return state["answer"]
```

**Evaluation ì§ˆë¬¸:**

```
Agentê°€ ì‹¤íŒ¨í•˜ë©´ ì–´ë””ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆë‚˜?
1. Tool selection?
2. Argument extraction?
3. Tool execution?
4. Result interpretation?
```

## 8.2. Tool Prediction Errors

**Tool call decomposition:**

```
User Query
    â†“
[1] Tool Selection + Argument Extraction
    â†“
[2] Tool Execution
    â†“
[3] Result Interpretation
    â†“
Final Answer
```

**Step 1ì—ì„œì˜ ì˜¤ë¥˜: Tool Prediction Errors**

### Tool Router Error

**ë¬¸ì œ: í•„ìš”í•œ toolì´ function listì— í¬í•¨ë˜ì§€ ì•ŠìŒ**

**ì˜ˆì‹œ:**

```python
# Available tools
all_tools = [
    "find_teddy_bear",  # âœ… í•„ìš”í•œ tool
    "get_weather",
    "search_web",
    "calculate"
]

# Query
query = "Find a teddy bear near me"

# Tool router
selected_tools = tool_router.select(query, all_tools, max_tools=3)
# Output: ["get_weather", "search_web", "calculate"]
# âŒ find_teddy_bearê°€ ì„ íƒë˜ì§€ ì•ŠìŒ!

# Result: LLM cannot call the right tool
# â†’ Punt (error response)
response = "Sorry, I cannot help with that."
```

**ì›ì¸:**

```python
# Tool routerì˜ recall ë¬¸ì œ
# Recall: í•„ìš”í•œ toolì„ ì„ íƒí•˜ëŠ” ë¹„ìœ¨

Recall = (ì„ íƒí•œ relevant tools) / (ëª¨ë“  relevant tools)

# Tool routerê°€ recallì´ ë‚®ìœ¼ë©´
# â†’ í•„ìš”í•œ toolì„ ë†“ì¹¨
```

**í•´ê²°:**

```python
# 1. Tool router ê°œì„ 
# - Retrieval model fine-tuning
# - Better embeddings
# - Query expansion

def improved_tool_router(query, all_tools, max_tools=5):
    """
    Recall-oriented tool router
    """
    # Query expansion
    expanded_queries = [
        query,
        llm.rephrase(query),
        llm.extract_intent(query)
    ]

    # Retrieve for each query
    candidates = set()
    for q in expanded_queries:
        tools = retrieve_tools(q, all_tools, top_k=3)
        candidates.update(tools)

    # Re-rank
    ranked_tools = rerank(query, list(candidates))

    return ranked_tools[:max_tools]
```

### LLMì´ Toolì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°

**ë¬¸ì œ: Toolì´ function listì— ìˆì§€ë§Œ LLMì´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ**

**ì˜ˆì‹œ:**

```python
# Available tools (ì´ë¯¸ routerì—ì„œ ì„ íƒë¨)
available_tools = [
    {
        "name": "find_teddy_bear",
        "description": "Find teddy bear stores near a location",
        "parameters": {"location": "string"}
    }
]

# Query
query = "Find a teddy bear near me"

# LLM response
response = llm.generate(query, tools=available_tools)
# Output: "You can try looking at toy stores nearby."
# âŒ find_teddy_bear toolì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ!
```

**ì›ì¸ & í•´ê²°:**

**1. LLMì´ tool ì‚¬ìš©ë²•ì„ ëª¨ë¦„**

```python
# í•´ê²°: Supervised Fine-Tuning (SFT)

# Training data
sft_examples = [
    {
        "input": "Find a teddy bear near me",
        "output": {
            "tool": "find_teddy_bear",
            "args": {"location": "user_location"}
        }
    },
    # More examples...
]

# Fine-tune
fine_tune(llm, sft_examples)
```

**2. Promptê°€ ë¶ˆëª…í™•**

```python
# âŒ Bad prompt
prompt = f"""
Here are some tools: {tools}

User: {query}
"""

# âœ… Good prompt
prompt = f"""
You are an assistant with access to tools.

Available tools:
{format_tools(tools)}

IMPORTANT: You MUST use available tools to answer the user's question.
Do NOT provide answers without using tools.

User: {query}

Think step-by-step:
1. What tool should I use?
2. What arguments do I need?
3. Call the tool

Tool call:
"""
```

**3. Few-shot examples ë¶€ì¡±**

```python
# âœ… In-context learning
prompt = f"""
You have access to these tools:
{format_tools(tools)}

Here are examples of correct tool usage:

Example 1:
User: "What's the weather in Paris?"
Tool call: get_weather(location="Paris")

Example 2:
User: "Find a teddy bear near me"
Tool call: find_teddy_bear(location="user_location")

Now, handle this query:
User: {query}
Tool call:
"""
```

## 8.3. Tool Hallucination

**ë¬¸ì œ: LLMì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” toolì„ í˜¸ì¶œ**

**ì˜ˆì‹œ:**

```python
# Available tools
available_tools = [
    "find_teddy_bear",  # âœ… Exists
    "get_weather"
]

# Query
query = "Find a bear near me"

# LLM response
response = llm.generate(query, tools=available_tools)
# Output: find_bear(location="user_location")
#         ^^^^^^^^ âŒ This tool doesn't exist!

# Actual API: find_teddy_bear
# LLM hallucinated: find_bear
```

**ì›ì¸:**

**1. Model capacity ë¶€ì¡±**

```python
# ì•½í•œ ëª¨ë¸ì´ tool APIë¥¼ ì •í™•íˆ followí•˜ì§€ ëª»í•¨
# â†’ ë¹„ìŠ·í•œ ì´ë¦„ì„ ë§Œë“¤ì–´ëƒ„

# í•´ê²°: ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
weak_model = "gpt-3.5-turbo"   # May hallucinate
strong_model = "gpt-4-turbo"   # Less likely to hallucinate

response = strong_model.generate(query, tools=available_tools)
```

**2. Tool APIê°€ ë¶ˆëª…í™•**

```python
# âŒ Bad tool API
{
    "name": "ftb",  # Unclear abbreviation
    "description": "finds bears",  # Vague
    "parameters": {"loc": "string"}  # Unclear param name
}

# âœ… Good tool API
{
    "name": "find_teddy_bear",  # Clear, descriptive
    "description": "Searches for teddy bear stores near a given location",  # Detailed
    "parameters": {
        "location": {
            "type": "string",
            "description": "The location to search near (e.g., 'New York', 'user_location')"
        }
    }
}
```

**3. Global instructions ë¶ˆëª…í™•**

```python
# âŒ Bad global instructions
system_prompt = "You can call functions."

# âœ… Good global instructions
system_prompt = """
You are an assistant with access to specific tools.

CRITICAL RULES:
1. You MUST ONLY use tools that are explicitly provided
2. Do NOT make up tool names or modify existing ones
3. Follow the exact API specification for each tool
4. If a suitable tool doesn't exist, say "I don't have a tool for that"

Available tools:
{format_tools(available_tools)}
"""
```

**Tool hallucination ê°ì§€:**

```python
def validate_tool_call(tool_call, available_tools):
    """
    Validate that tool call uses existing tools
    """
    tool_name = tool_call["name"]
    tool_args = tool_call["arguments"]

    # Check if tool exists
    if tool_name not in [t["name"] for t in available_tools]:
        return {
            "valid": False,
            "error": f"Tool '{tool_name}' does not exist",
            "suggestion": find_similar_tool(tool_name, available_tools)
        }

    # Check arguments
    tool_spec = get_tool_spec(tool_name, available_tools)
    required_args = tool_spec["parameters"]["required"]

    missing_args = set(required_args) - set(tool_args.keys())
    if missing_args:
        return {
            "valid": False,
            "error": f"Missing required arguments: {missing_args}"
        }

    return {"valid": True}

# ì‚¬ìš©
tool_call = {"name": "find_bear", "arguments": {"location": "NYC"}}
validation = validate_tool_call(tool_call, available_tools)

if not validation["valid"]:
    print(f"âŒ Invalid tool call: {validation['error']}")
    if "suggestion" in validation:
        print(f"Did you mean: {validation['suggestion']}?")
```

**Agent Evaluation ìš”ì•½:**

| Error Type | Description | Solution |
|------------|-------------|----------|
| Tool Router Error | í•„ìš”í•œ tool ë¯¸ì„ íƒ | Improve tool router (recall) |
| LLM doesn't use tool | Tool ìˆì§€ë§Œ ì‚¬ìš© ì•ˆí•¨ | SFT, better prompts, examples |
| Tool Hallucination | ì¡´ì¬í•˜ì§€ ì•ŠëŠ” tool í˜¸ì¶œ | Strong model, clear APIs |

---

# 9. ìš”ì•½

## í•µì‹¬ ê°œë…

**Evaluationì˜ ì§„í™”:**

```
Human Evaluation (Ideal but impractical)
    â†“
Rule-Based Metrics (Fast but limited)
    â†“
LLM-as-a-Judge (Best of both worlds)
```

**LLM-as-a-Judgeì˜ í•µì‹¬:**

1. **Reference ë¶ˆí•„ìš”**
   - LLMì˜ ë‚´ì¬ëœ ì§€ì‹ í™œìš©

2. **Interpretable**
   - Rationale ì œê³µ

3. **Flexible**
   - ë‹¤ì–‘í•œ criteria í‰ê°€ ê°€ëŠ¥

4. **Scalable**
   - ëŒ€ê·œëª¨ í‰ê°€ ê°€ëŠ¥

**ì£¼ì˜ì‚¬í•­:**

1. **Biases ì¡´ì¬**
   - Position, Verbosity, Self-Enhancement

2. **Human calibration í•„ìš”**
   - Correlation í™•ì¸

3. **Best practices ì¤‘ìš”**
   - Binary scale, Rationale first, Low temperature

## Evaluation ë°©ë²• ë¹„êµ

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œê¸° |
|------|------|------|-----------|
| Human Evaluation | - Ground truth<br>- ê°€ì¥ ì •í™• | - ëŠë¦¼<br>- ë¹„ìŒˆ<br>- í™•ì¥ ë¶ˆê°€ | - Calibration<br>- ìµœì¢… ê²€ì¦ |
| Rule-Based (BLEU, METEOR) | - ë¹ ë¦„<br>- ì¬í˜„ ê°€ëŠ¥<br>- ë¹„ìš© ì—†ìŒ | - Reference í•„ìš”<br>- Stylistic variation ë¶ˆê°€<br>- ë‚®ì€ correlation | - ë²ˆì—­/ìš”ì•½<br>- ê°„ë‹¨í•œ benchmark |
| LLM-as-a-Judge | - Reference ë¶ˆí•„ìš”<br>- í™•ì¥ ê°€ëŠ¥<br>- Interpretable<br>- Flexible | - Biases ì¡´ì¬<br>- API ë¹„ìš©<br>- Calibration í•„ìš” | - ëŒ€ë¶€ë¶„ì˜ ê²½ìš°<br>- í˜„ëŒ€ì  LLM í‰ê°€ |

**ì–¸ì œ ë¬´ì—‡ì„ ì‚¬ìš©í•˜ë‚˜:**

```python
# 1. ì´ˆê¸° ê°œë°œ ë‹¨ê³„
# â†’ LLM-as-a-Judge (ë¹ ë¥¸ iteration)

# 2. ëª¨ë¸ ë¹„êµ
# â†’ LLM-as-a-Judge pairwise

# 3. ìµœì¢… ê²€ì¦
# â†’ Human Evaluation (ì†Œê·œëª¨)

# 4. ì§€ì†ì  ëª¨ë‹ˆí„°ë§
# â†’ LLM-as-a-Judge + periodic human calibration

# 5. Benchmark ì œì¶œ
# â†’ Rule-based metrics (BLEU ë“±) + LLM-as-a-Judge
```

## ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

**LLM-as-a-Judge êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸:**

```python
evaluation_checklist = {
    "Prompt Design": [
        "âœ“ ëª…í™•í•œ criteria ì •ì˜",
        "âœ“ êµ¬ì²´ì ì¸ pass/fail ì˜ˆì‹œ í¬í•¨",
        "âœ“ Binary scale ì‚¬ìš©",
        "âœ“ Rationaleì„ ë¨¼ì € ìš”ì²­"
    ],

    "Technical Setup": [
        "âœ“ Structured outputs ì‚¬ìš©",
        "âœ“ Low temperature (0.1-0.2)",
        "âœ“ ìƒì„± ëª¨ë¸ê³¼ ë‹¤ë¥¸ judge ëª¨ë¸ ì‚¬ìš©",
        "âœ“ Error handling êµ¬í˜„"
    ],

    "Bias Mitigation": [
        "âœ“ Position bias: swapping êµ¬í˜„",
        "âœ“ Verbosity bias: guidelinesì— ëª…ì‹œ",
        "âœ“ Self-enhancement: ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©"
    ],

    "Validation": [
        "âœ“ Human ratings ìˆ˜ì§‘ (100+ examples)",
        "âœ“ Correlation ë¶„ì„ (target: >0.7)",
        "âœ“ Inter-rater agreement ì¸¡ì •",
        "âœ“ ì •ê¸°ì  calibration"
    ],

    "Factuality (íŠ¹ë³„ ì²˜ë¦¬)": [
        "âœ“ Fact extraction ë‹¨ê³„",
        "âœ“ Fact checking with RAG/Web search",
        "âœ“ Weighted aggregation"
    ]
}
```

**Agent Evaluation ì²´í¬ë¦¬ìŠ¤íŠ¸:**

```python
agent_eval_checklist = {
    "Tool Selection": [
        "âœ“ Tool router recall ì¸¡ì •",
        "âœ“ í•„ìš”í•œ toolì´ ì„ íƒë˜ëŠ”ì§€ í™•ì¸",
        "âœ“ Tool router ê°œì„  (í•„ìš” ì‹œ)"
    ],

    "Tool Usage": [
        "âœ“ LLMì´ toolì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸",
        "âœ“ SFT ë°ì´í„° ì¤€ë¹„ (í•„ìš” ì‹œ)",
        "âœ“ Few-shot examples í¬í•¨"
    ],

    "Tool Hallucination": [
        "âœ“ Tool call validation êµ¬í˜„",
        "âœ“ Clear tool API ì‘ì„±",
        "âœ“ Global instructions ëª…í™•í™”",
        "âœ“ Strong model ì‚¬ìš©"
    ],

    "Error Analysis": [
        "âœ“ ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ë¥˜",
        "âœ“ ë³‘ëª© êµ¬ê°„ ì‹ë³„",
        "âœ“ Systematic improvement"
    ]
}
```

**Production Deployment:**

```python
class ProductionEvaluator:
    """
    Production-ready LLM-as-a-Judge
    """

    def __init__(self, judge_model, criteria_config, calibration_data=None):
        self.judge_model = judge_model
        self.criteria_config = criteria_config
        self.calibration_data = calibration_data

        # Calibration
        if calibration_data:
            self.correlation = self.run_calibration()
            if self.correlation < 0.7:
                warnings.warn("Low correlation with human ratings!")

    def evaluate(self, prompt, response, criteria):
        """
        Evaluate with bias mitigation
        """
        # Main evaluation
        result = self._evaluate_once(prompt, response, criteria)

        # Confidence score
        confidence = self._estimate_confidence(result)

        # Flag low confidence cases for human review
        if confidence < 0.8:
            self._flag_for_human_review(prompt, response, result)

        return result

    def evaluate_pairwise(self, prompt, response_a, response_b, criteria):
        """
        Pairwise evaluation with position bias mitigation
        """
        # Evaluate both orders
        result_ab = self._evaluate_once(prompt, response_a, response_b, criteria, order="AB")
        result_ba = self._evaluate_once(prompt, response_b, response_a, criteria, order="BA")

        # Check consistency
        if result_ab["winner"] == result_ba["winner"]:
            return result_ab  # Consistent
        else:
            # Inconsistent: Flag for review
            return {
                "winner": "uncertain",
                "reason": "Position bias detected",
                "flag_for_review": True
            }

    def batch_evaluate(self, cases, parallel=True):
        """
        Batch evaluation with progress tracking
        """
        from tqdm import tqdm

        results = []
        for case in tqdm(cases):
            result = self.evaluate(
                case["prompt"],
                case["response"],
                case["criteria"]
            )
            results.append(result)

        return results

    def run_calibration(self):
        """
        Calibration with human ratings
        """
        judge_scores = []
        human_scores = []

        for case in self.calibration_data:
            judge_score = self.evaluate(
                case["prompt"],
                case["response"],
                case["criteria"]
            )
            judge_scores.append(judge_score["score"])
            human_scores.append(case["human_score"])

        from scipy.stats import pearsonr
        correlation, _ = pearsonr(human_scores, judge_scores)

        return correlation

# ì‚¬ìš©
evaluator = ProductionEvaluator(
    judge_model=gpt4,
    criteria_config=load_criteria_config(),
    calibration_data=load_calibration_data()
)

# Single evaluation
result = evaluator.evaluate(prompt, response, "usefulness")

# Batch evaluation
results = evaluator.batch_evaluate(test_cases)

# Pairwise comparison
winner = evaluator.evaluate_pairwise(prompt, response_a, response_b, "quality")
```

---

# 10. ì¤‘ìš” ìš©ì–´ ì •ë¦¬

**Evaluation ê´€ë ¨:**

- **LLM Evaluation**: LLMì˜ ì¶œë ¥ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ê³¼ì •
- **Inter-Rater Agreement**: ì—¬ëŸ¬ í‰ê°€ì ê°„ì˜ í‰ê°€ ì¼ì¹˜ë„
- **Cohen's Kappa**: ë‘ í‰ê°€ì ê°„ agreementë¥¼ random chance ëŒ€ë¹„ ì¸¡ì •í•˜ëŠ” metric
- **Fleiss's Kappa**: 3ëª… ì´ìƒ í‰ê°€ìì— ëŒ€í•œ Cohen's Kappa í™•ì¥
- **Krippendorff's Alpha**: Missing dataë¥¼ í—ˆìš©í•˜ëŠ” ì¼ë°˜í™”ëœ agreement metric

**Rule-Based Metrics:**

- **METEOR**: Translation í‰ê°€ë¥¼ ìœ„í•œ precision+recall ê¸°ë°˜ metric (ordering ê³ ë ¤)
- **BLEU**: Translation í‰ê°€ë¥¼ ìœ„í•œ n-gram precision ê¸°ë°˜ metric
- **ROUGE**: Summarization í‰ê°€ë¥¼ ìœ„í•œ recall ê¸°ë°˜ metric
- **Brevity Penalty**: ì§§ì€ ë¬¸ì¥ì— ëŒ€í•œ í˜ë„í‹° (BLEUì—ì„œ ì‚¬ìš©)
- **N-gram**: ì—°ì†ëœ nê°œì˜ ë‹¨ì–´ ì‹œí€€ìŠ¤

**LLM-as-a-Judge:**

- **LLM-as-a-Judge**: LLMì„ í‰ê°€ìë¡œ ì‚¬ìš©í•˜ëŠ” í‰ê°€ ë°©ë²•
- **Pointwise Evaluation**: ë‹¨ì¼ ì‘ë‹µì˜ ì ˆëŒ€ì  í’ˆì§ˆ í‰ê°€
- **Pairwise Evaluation**: ë‘ ì‘ë‹µì˜ ìƒëŒ€ì  í’ˆì§ˆ ë¹„êµ
- **Rationale**: Judgeê°€ ì œê³µí•˜ëŠ” íŒë‹¨ ê·¼ê±° ì„¤ëª…
- **Structured Outputs**: ì •í•´ì§„ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ì„ ê°•ì œí•˜ëŠ” ê¸°ë²•

**Biases:**

- **Position Bias**: ë¨¼ì € ì œì‹œëœ ì‘ë‹µì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥
- **Verbosity Bias**: ë” ê¸´ ì‘ë‹µì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥
- **Self-Enhancement Bias**: ìì‹ ì´ ìƒì„±í•œ ì‘ë‹µì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥

**Evaluation Dimensions:**

- **Usefulness**: ì‘ë‹µì´ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì •ë„
- **Factuality**: ì‘ë‹µì˜ ì‚¬ì‹¤ì  ì •í™•ì„±
- **Relevance**: ì‘ë‹µì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë„
- **Coherence**: ì‘ë‹µì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±
- **Safety**: ì‘ë‹µì˜ ì•ˆì „ì„± ë° ìœ¤ë¦¬ì„±

**Factuality Evaluation:**

- **Fact Extraction**: í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ factë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •
- **Fact Checking**: ê°œë³„ factì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ëŠ” ê³¼ì •
- **Atomic Fact**: ë” ì´ìƒ ë¶„í•´í•  ìˆ˜ ì—†ëŠ” í•˜ë‚˜ì˜ ì‚¬ì‹¤ì  ì£¼ì¥

**Agent Evaluation:**

- **Tool Router**: ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ì í•©í•œ toolì„ ì„ íƒí•˜ëŠ” ì‹œìŠ¤í…œ
- **Tool Hallucination**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” toolì„ í˜¸ì¶œí•˜ëŠ” ì˜¤ë¥˜
- **Punt**: Agentê°€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì§€ ëª»í•˜ê³  í¬ê¸°í•˜ëŠ” ê²ƒ
- **Recall (Tool Selection)**: í•„ìš”í•œ toolì„ ì–¼ë§ˆë‚˜ ì˜ ì„ íƒí•˜ëŠ”ì§€ì˜ ë¹„ìœ¨

**Best Practices:**

- **Calibration**: Judgeì˜ í‰ê°€ë¥¼ human ratingsì™€ ë¹„êµí•˜ì—¬ ì¡°ì •
- **Temperature**: ìƒì„± ê³¼ì •ì˜ randomnessë¥¼ ì œì–´í•˜ëŠ” íŒŒë¼ë¯¸í„°
- **Binary Scale**: Pass/Failê³¼ ê°™ì€ ì´ì§„ í‰ê°€ ì²™ë„
- **Correlation**: ë‘ ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ ê°•ë„ (Judge vs Human)

---

**ë‹¤ìŒ ê°•ì˜ ì˜ˆê³ :**

Lecture 9ì—ì„œëŠ” Current Trendsë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. LLM ë¶„ì•¼ì˜ ìµœì‹  ë™í–¥ê³¼ ë¯¸ë˜ ë°©í–¥ì„ ì‚´í´ë´…ë‹ˆë‹¤.

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰
