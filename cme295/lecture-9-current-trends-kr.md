# Lecture 9: Current Trends in LLMs

# Materials

- [CME 295](https://cme295.stanford.edu/syllabus/)
- [slide](https://cme295.stanford.edu/slides/fall25-cme295-lecture9.pdf)
- [video](https://www.youtube.com/watch?v=Q86qzJ1K1Ss&list=PLoROMvodv4rOCXd21gf0CF4xr35yINeOy&index=9)

# Table of Contents

- [Lecture 9: Current Trends in LLMs](#lecture-9-current-trends-in-llms)
- [Materials](#materials)
- [Table of Contents](#table-of-contents)
- [ê°•ì˜ ê°œìš”](#ê°•ì˜-ê°œìš”)
  - [ê°•ì˜ ëª©í‘œ](#ê°•ì˜-ëª©í‘œ)
  - [ê°•ì˜ êµ¬ì„±](#ê°•ì˜-êµ¬ì„±)
- [Part 1: ì „ì²´ ì½”ìŠ¤ ë³µìŠµ](#part-1-ì „ì²´-ì½”ìŠ¤-ë³µìŠµ)
  - [1. Transformer ê¸°ì´ˆ](#1-transformer-ê¸°ì´ˆ)
    - [1.1. Tokenization](#11-tokenization)
    - [1.2. Embeddings](#12-embeddings)
    - [1.3. Self-Attention](#13-self-attention)
  - [2. Transformer ê°œì„ ì‚¬í•­](#2-transformer-ê°œì„ ì‚¬í•­)
    - [2.1. Position Embeddings](#21-position-embeddings)
    - [2.2. Multi-Head Attention ìµœì í™”](#22-multi-head-attention-ìµœì í™”)
    - [2.3. Normalization](#23-normalization)
  - [3. Large Language Models](#3-large-language-models)
    - [3.1. Mixture of Experts (MoE)](#31-mixture-of-experts-moe)
    - [3.2. Sampling ì „ëµ](#32-sampling-ì „ëµ)
  - [4. LLM Training](#4-llm-training)
    - [4.1. Scaling Laws](#41-scaling-laws)
    - [4.2. Flash Attention](#42-flash-attention)
    - [4.3. Parallelism](#43-parallelism)
  - [5. LLM Tuning](#5-llm-tuning)
    - [5.1. 3ë‹¨ê³„ Training Pipeline](#51-3ë‹¨ê³„-training-pipeline)
    - [5.2. Reward Modeling](#52-reward-modeling)
    - [5.3. RL ê¸°ë°˜ Tuning](#53-rl-ê¸°ë°˜-tuning)
  - [6. LLM Reasoning](#6-llm-reasoning)
    - [6.1. Chain of Thought](#61-chain-of-thought)
    - [6.2. GRPO vs PPO](#62-grpo-vs-ppo)
    - [6.3. GRPOì˜ ë¬¸ì œì ê³¼ ê°œì„ ](#63-grpoì˜-ë¬¸ì œì ê³¼-ê°œì„ )
  - [7. Agentic LLMs](#7-agentic-llms)
    - [7.1. RAG (Retrieval Augmented Generation)](#71-rag-retrieval-augmented-generation)
    - [7.2. Tool Calling](#72-tool-calling)
  - [8. LLM Evaluation](#8-llm-evaluation)
    - [8.1. ì „í†µì ì¸ Metrics](#81-ì „í†µì ì¸-metrics)
    - [8.2. LLM-as-a-Judge](#82-llm-as-a-judge)
    - [8.3. Benchmarks](#83-benchmarks)
- [Part 2: 2025ë…„ í˜„ì¬ íŠ¸ë Œë“œ](#part-2-2025ë…„-í˜„ì¬-íŠ¸ë Œë“œ)
  - [1. Vision Transformer (ViT)](#1-vision-transformer-vit)
    - [1.1. ë™ê¸°: Transformerë¥¼ Visionì— ì ìš©í•  ìˆ˜ ìˆì„ê¹Œ?](#11-ë™ê¸°-transformerë¥¼-visionì—-ì ìš©í• -ìˆ˜-ìˆì„ê¹Œ)
    - [1.2. ViT ì•„í‚¤í…ì²˜](#12-vit-ì•„í‚¤í…ì²˜)
    - [1.3. Image Patching](#13-image-patching)
    - [1.4. End-to-End ì˜ˆì‹œ](#14-end-to-end-ì˜ˆì‹œ)
    - [1.5. Inductive Bias](#15-inductive-bias)
  - [2. Multimodal LLMs](#2-multimodal-llms)
    - [2.1. Vision-Language Models (VLM)](#21-vision-language-models-vlm)
    - [2.2. ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹](#22-ë‘-ê°€ì§€-ì ‘ê·¼-ë°©ì‹)
    - [2.3. LAVA ëª¨ë¸](#23-lava-ëª¨ë¸)
  - [3. Diffusion-based LLMs](#3-diffusion-based-llms)
    - [3.1. ë™ê¸°: Autoregressiveì˜ í•œê³„](#31-ë™ê¸°-autoregressiveì˜-í•œê³„)
    - [3.2. Diffusionì´ë€?](#32-diffusionì´ë€)
    - [3.3. Textì— Diffusion ì ìš©í•˜ê¸°](#33-textì—-diffusion-ì ìš©í•˜ê¸°)
    - [3.4. ìµœê·¼ ë°œì „](#34-ìµœê·¼-ë°œì „)
  - [4. Transformerì˜ í™•ì¥](#4-transformerì˜-í™•ì¥)
- [Part 3: ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„](#part-3-ìš”ì•½-ë°-ë‹¤ìŒ-ë‹¨ê³„)
  - [1. ì½”ìŠ¤ ì „ì²´ ìš”ì•½](#1-ì½”ìŠ¤-ì „ì²´-ìš”ì•½)
  - [2. í•µì‹¬ Takeaways](#2-í•µì‹¬-takeaways)
  - [3. Final ì‹œí—˜ ë²”ìœ„](#3-final-ì‹œí—˜-ë²”ìœ„)
- [ìš©ì–´ ì •ë¦¬](#ìš©ì–´-ì •ë¦¬)
  - [Vision ê´€ë ¨](#vision-ê´€ë ¨)
  - [Diffusion ê´€ë ¨](#diffusion-ê´€ë ¨)
  - [Multimodal ê´€ë ¨](#multimodal-ê´€ë ¨)

---

# ê°•ì˜ ê°œìš”

## ê°•ì˜ ëª©í‘œ

ì´ë²ˆ ê°•ì˜ëŠ” CME 295ì˜ ë§ˆì§€ë§‰ ê°•ì˜ë¡œ, ë‹¤ìŒ ì„¸ ê°€ì§€ ëª©í‘œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

1. **ì „ì²´ ì½”ìŠ¤ ë³µìŠµ**: Lecture 1-8ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•˜ê³  ì—°ê²°
2. **2025ë…„ íŠ¸ë Œë“œ**: í˜„ì¬ì™€ ê°€ê¹Œìš´ ë¯¸ë˜ì˜ íŠ¸ë Œë”© í† í”½ ì†Œê°œ
3. **ë‹¤ìŒ ë‹¨ê³„**: Final ì‹œí—˜ ì¤€ë¹„ ë° í–¥í›„ í•™ìŠµ ë°©í–¥

## ê°•ì˜ êµ¬ì„±

**Part 1: ì „ì²´ ì½”ìŠ¤ ë³µìŠµ (Recap)**
- Transformer ê¸°ì´ˆë¶€í„° ìµœì‹  ê¸°ë²•ê¹Œì§€
- Lecture 1-8ì˜ í•µì‹¬ ê°œë… ì •ë¦¬
- Final ì‹œí—˜ ë²”ìœ„

**Part 2: 2025ë…„ íŠ¸ë Œë“œ**
- Vision Transformer (ViT)
- Multimodal LLMs
- Diffusion-based LLMs
- Transformerì˜ ë‹¤ì–‘í•œ ì‘ìš©

**Part 3: ë§ˆë¬´ë¦¬**
- ì „ì²´ ìš”ì•½
- Final ì‹œí—˜ ì•ˆë‚´
- í–¥í›„ í•™ìŠµ ë°©í–¥

---

# Part 1: ì „ì²´ ì½”ìŠ¤ ë³µìŠµ

ì´ ì„¹ì…˜ì—ì„œëŠ” Lecture 1ë¶€í„° Lecture 8ê¹Œì§€ ë°°ìš´ ëª¨ë“  ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤. Final ì‹œí—˜ ë²”ìœ„ëŠ” **Lecture 5-8**ì´ì§€ë§Œ, ì „ì²´ì ì¸ ë§¥ë½ì„ ì´í•´í•˜ê¸° ìœ„í•´ ëª¨ë“  ë‚´ìš©ì„ ë³µìŠµí•©ë‹ˆë‹¤.

## 1. Transformer ê¸°ì´ˆ

Lecture 1ì—ì„œëŠ” Transformerì˜ ê¸°ë³¸ ê°œë…ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 1.1. Tokenization

**í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì²« ë‹¨ê³„**

```python
# í…ìŠ¤íŠ¸ë¥¼ atomic unitsë¡œ ë¶„í• 
Text: "The cat sat on the mat"

# Subword tokenization
Tokens: ["The", "cat", "sat", "on", "the", "mat"]
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- Subword level tokenizerê°€ ê°€ì¥ ì¼ë°˜ì 
- ë‹¨ì–´ì˜ rootë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆì–´ íš¨ìœ¨ì 
- ì˜ˆ: "playing", "played", "player" â†’ "play", "ing", "ed", "er"

### 1.2. Embeddings

**í† í°ì„ ë²¡í„°ë¡œ í‘œí˜„í•˜ê¸°**

**ì´ˆê¸° ë°©ë²•: Word2Vec**
```python
# ë¬¸ë§¥ ì—†ëŠ” ê³ ì • ì„ë² ë”©
"bank" â†’ [0.1, -0.3, 0.5, ...]  # í•­ìƒ ë™ì¼

ë¬¸ì œì :
"I went to the bank to deposit money"  # ì€í–‰
"I sat by the river bank"              # ê°•ë‘‘
â†’ ë™ì¼í•œ ì„ë² ë”©! (ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ)
```

**RNNì˜ ë“±ì¥**
- ìˆœì°¨ì ìœ¼ë¡œ í† í° ì²˜ë¦¬
- ë‚´ë¶€ ìƒíƒœë¡œ ë¬¸ë§¥ ìœ ì§€
- **ë¬¸ì œì **: Long-range dependency (ë¨¼ ê±°ë¦¬ í† í° ì •ë³´ ì†ì‹¤)

```python
# RNNì˜ í•œê³„
"The cat, which was sitting on the mat in the living room, meowed"
        â†‘                                                    â†‘
     ì£¼ì–´                                                  ë™ì‚¬
# "cat"ê³¼ "meowed"ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
```

### 1.3. Self-Attention

**ëª¨ë“  í† í°ì´ ì§ì ‘ ì†Œí†µ!**

```
Self-Attention = softmax(QÂ·K^T / sqrt(d_k)) Â· V

í•µì‹¬ ì•„ì´ë””ì–´:
- Query: "ë‚˜ëŠ” ëˆ„êµ¬ì™€ ê´€ë ¨ìˆë‚˜?"
- Key: "ë‚˜ëŠ” ì´ëŸ° ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´"
- Value: "ì‹¤ì œ ì „ë‹¬í•  ì •ë³´"
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
ì‹œí€€ìŠ¤: "The cat sat"

ê° í† í°ì˜ attention:
"cat"ì˜ ì…ì¥ì—ì„œ:
  - "The"ì™€ì˜ similarity: 0.2
  - "cat"ì™€ì˜ similarity: 0.5 (ìê¸° ìì‹ )
  - "sat"ì™€ì˜ similarity: 0.3

Output for "cat" = 0.2*V_The + 0.5*V_cat + 0.3*V_sat
```

**ì¥ì :**
- âœ… ê±°ë¦¬ì™€ ìƒê´€ì—†ì´ ëª¨ë“  í† í° ê°„ ì§ì ‘ ì—°ê²°
- âœ… ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- âœ… Long-range dependency í•´ê²°

**ë‹¨ì :**
- âŒ O(nÂ²) ë³µì¡ë„

## 2. Transformer ê°œì„ ì‚¬í•­

Lecture 2ì—ì„œëŠ” Transformerë¥¼ ê°œì„ í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 2.1. Position Embeddings

**ì™œ í•„ìš”í•œê°€?**

Self-Attentionì€ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤!

```python
"I love transformers"
"transformers love I"
# Self-Attentionì€ ì´ ë‘˜ì„ êµ¬ë¶„í•˜ì§€ ëª»í•¨!
```

**ë°œì „ ê³¼ì •:**

```
Learned (ì ˆëŒ€ ìœ„ì¹˜)
  â†“
Sinusoidal (ê³ ì • ìˆ˜ì‹)
  â†“
T5 Bias / ALiBi (attentionì— ì§ì ‘)
  â†“
RoPE (íšŒì „ ë³€í™˜) â† í˜„ëŒ€ í‘œì¤€!
```

**RoPE (Rotary Position Embeddings)**

```python
# Queryì™€ Keyë¥¼ íšŒì „ì‹œì¼œ ìƒëŒ€ ìœ„ì¹˜ ì¸ì½”ë”©
q_rotated = R(Î¸_pos) Â· q
k_rotated = R(Î¸_pos) Â· k

# í•µì‹¬: ìƒëŒ€ ìœ„ì¹˜ì˜ í•¨ìˆ˜ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ë¨
Attention = q_rotated Â· k_rotated^T
```

**ì‚¬ìš© ëª¨ë¸:**
- GPT-3/4
- LLaMA ì‹œë¦¬ì¦ˆ
- Mistral
- ëŒ€ë¶€ë¶„ì˜ í˜„ëŒ€ LLM

### 2.2. Multi-Head Attention ìµœì í™”

**KV Cache ë©”ëª¨ë¦¬ ë¬¸ì œ**

```python
# GPT-3 (175B parameters)
num_heads = 96
d_k = 128
seq_len = 2048

KV Cache per head = 1 MB
Total = 96 MB per sample
Batch 100 = 9.6 GB!
```

**í•´ê²°ì±…: Grouped Query Attention (GQA)**

```
MHA (Multi-Head Attention):
  Head 1: Q1, K1, V1
  Head 2: Q2, K2, V2
  ...
  Head 96: Q96, K96, V96

GQA (Grouped Query Attention):
  Group 1 (Heads 1-8): Q1-Q8, K_shared, V_shared
  Group 2 (Heads 9-16): Q9-Q16, K_shared, V_shared
  ...

MQA (Multi-Query Attention):
  All Heads: Q1-Q96, K_shared (1ê°œ), V_shared (1ê°œ)
```

**ë©”ëª¨ë¦¬ ë¹„êµ:**

```
MHA: 96 MB
GQA (8 groups): 8 MB (12ë°° ê°ì†Œ!)
MQA: 1 MB (96ë°° ê°ì†Œ!)
```

**ì‚¬ìš© ì˜ˆì‹œ:**
- LLaMA 2 (70B): GQA with 8 groups
- Mistral: GQA
- ì‘ì€ ëª¨ë¸ (< 7B): MHA
- ë§¤ìš° í° ëª¨ë¸ (> 70B): GQA ë˜ëŠ” MQA

### 2.3. Normalization

**Pre-norm vs Post-norm**

```python
# Post-norm (ì›ë³¸ Transformer)
x = LayerNorm(x + Sublayer(x))

# Pre-norm (í˜„ëŒ€ í‘œì¤€)
x = x + Sublayer(LayerNorm(x))
```

**Pre-normì´ ë” ë‚˜ì€ ì´ìœ :**
1. Gradient flowê°€ ë” ì•ˆì •ì 
2. Learning rate warmup ëœ í•„ìš”
3. í•™ìŠµ ì´ˆê¸°ì— ë” ì•ˆì •ì 

**RMSNorm (ìµœì‹  ê¸°ë²•)**

```python
# LayerNorm
LayerNorm(x) = Î³ Â· (x - Î¼) / sqrt(ÏƒÂ² + Îµ) + Î²

# RMSNorm (í‰ê·  ì œê±°, Î² ì œê±°)
RMSNorm(x) = Î³ Â· x / sqrt(mean(xÂ²) + Îµ)
```

**ì¥ì :**
- 25% ë¹ ë¥¸ ê³„ì‚°
- íŒŒë¼ë¯¸í„° ê°ì†Œ (Î² ì œê±°)
- ë¹„ìŠ·í•œ ì„±ëŠ¥

**ì‚¬ìš© ëª¨ë¸:**
- LLaMA
- Mistral
- Falcon

## 3. Large Language Models

Lecture 3ì—ì„œëŠ” LLMì˜ íŠ¹ë³„í•œ êµ¬ì¡°ì™€ ê¸°ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 3.1. Mixture of Experts (MoE)

**í•µì‹¬ ì•„ì´ë””ì–´: ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•­ìƒ ì‚¬ìš©í•˜ì§€ ì•Šê¸°**

```
ì „í†µì ì¸ LLM:
Input â†’ All Parameters â†’ Output

MoE LLM:
Input â†’ Gating â†’ Expert 1 (ì„ íƒë¨)
              â†’ Expert 2
              â†’ Expert 3
              â†’ Expert 4 (ì„ íƒë¨)
              â†’ ...
Only activated experts â†’ Output
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# Token-level routing
Text: "Write Python code for sorting"

Token "Python" â†’ Expert 2, Expert 5 (Programming experts)
Token "sorting" â†’ Expert 2, Expert 7 (Algorithm experts)
Token "code" â†’ Expert 1, Expert 2 (Code experts)

# ê° í† í°ë§ˆë‹¤ ë‹¤ë¥¸ experts í™œì„±í™”
```

**ì¥ì :**
- Forward pass ì‹œ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©
- ë” í° ëª¨ë¸ êµ¬ì¶• ê°€ëŠ¥
- ë³‘ë ¬í™” ê°€ëŠ¥ (expertsë¥¼ ë‹¤ë¥¸ GPUì— ë°°ì¹˜)

**ë‹¨ì :**
- Training ë³µì¡ë„ ì¦ê°€
- Load balancing í•„ìš”

### 3.2. Sampling ì „ëµ

**ë‹¤ìŒ í† í° ì˜ˆì¸¡í•˜ê¸°**

```python
# Greedy (íƒìš•ì )
"The cat" â†’ "sat" (P=0.6, ê°€ì¥ ë†’ì€ í™•ë¥ )

# Sampling (ìƒ˜í”Œë§)
"The cat" â†’ "sat" (P=0.6) or "jumped" (P=0.2) or "meowed" (P=0.15)
```

**Temperature ì¡°ì ˆ**

```python
# Low temperature (T=0.1) - ë” deterministic
Probabilities: [0.95, 0.03, 0.01, 0.01]
Output: ê±°ì˜ í•­ìƒ ì²« ë²ˆì§¸ ì„ íƒ

# High temperature (T=1.5) - ë” creative
Probabilities: [0.4, 0.25, 0.2, 0.15]
Output: ë‹¤ì–‘í•œ ì„ íƒ ê°€ëŠ¥
```

**ì–¸ì œ ë¬´ì—‡ì„ ì‚¬ìš©í•˜ë‚˜?**

```
T = 0.0 (Greedy):
  - ì‚¬ì‹¤ ê¸°ë°˜ QA
  - ì½”ë“œ ìƒì„±
  - ë²ˆì—­

T = 0.7-1.0:
  - ì¼ë°˜ ëŒ€í™”
  - ê· í˜•ì¡íŒ ì°½ì˜ì„±

T = 1.5+:
  - ì°½ì˜ì  ê¸€ì“°ê¸°
  - ë¸Œë ˆì¸ìŠ¤í† ë°
  - ì‹œ ì‘ì„±
```

## 4. LLM Training

Lecture 4ì—ì„œëŠ” ëŒ€ê·œëª¨ LLMì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 4.1. Scaling Laws

**ë” í° ëª¨ë¸ = ë” ì¢‹ì€ ì„±ëŠ¥?**

```
ë°œê²¬ëœ ë²•ì¹™:
1. ë” ë§ì€ compute â†’ ë” ë‚®ì€ loss
2. ë” í° ë°ì´í„°ì…‹ â†’ ë” ë‚®ì€ loss
3. ë” ë§ì€ íŒŒë¼ë¯¸í„° â†’ ë” ë‚®ì€ loss
```

**Chinchilla Scaling Laws (2022)**

```
Rule of thumb:
ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ : í•™ìŠµ í† í° ìˆ˜ = 1 : 20

ì˜ˆì‹œ:
100B parameter ëª¨ë¸ â†’ 2T tokensë¡œ í•™ìŠµ
70B parameter ëª¨ë¸ â†’ 1.4T tokensë¡œ í•™ìŠµ

ë¬¸ì œ: ëŒ€ë¶€ë¶„ì˜ ì´ˆê¸° LLMë“¤ì€ undertrained!
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# GPT-3
Parameters: 175B
Training tokens: ~300B
Chinchilla optimal: 175B Ã— 20 = 3.5T tokens
â†’ Undertrained!

# LLaMA 2
Parameters: 70B
Training tokens: 2T
Chinchilla optimal: 70B Ã— 20 = 1.4T tokens
â†’ Well-trained!
```

### 4.2. Flash Attention

**ë¬¸ì œ: Attentionì€ ë©”ëª¨ë¦¬ ë³‘ëª©**

```
Standard Attention:
1. QÂ·K^T ê³„ì‚° â†’ HBMì— ì €ì¥
2. Softmax ê³„ì‚° â†’ HBMì— ì €ì¥
3. AttentionÂ·V ê³„ì‚°
â†’ HBM (í°/ëŠë¦° ë©”ëª¨ë¦¬)ì— ë§ì€ ì½ê¸°/ì“°ê¸°
```

**Flash Attentionì˜ í•´ê²°ì±…**

```
Key Ideas:
1. Tiling: ê³„ì‚°ì„ ì‘ì€ ë¸”ë¡ìœ¼ë¡œ ë¶„í• 
2. SRAM í™œìš©: ì‘ì§€ë§Œ ë¹ ë¥¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
3. Recomputation: ì €ì¥ ëŒ€ì‹  ì¬ê³„ì‚°
```

**ì˜ˆì‹œ:**

```python
# Standard Attention
Attention Matrix (seq_len=2048):
  2048 Ã— 2048 = 4M elements
  ë©”ëª¨ë¦¬: 16 MB (FP32)
  HBM ì½ê¸°/ì“°ê¸°: ë§¤ìš° ëŠë¦¼

# Flash Attention
Block size: 128 Ã— 128
  128 Ã— 128 = 16K elements
  SRAMì— fit!
  í›¨ì”¬ ë¹ ë¥¸ ì—°ì‚°

ê²°ê³¼:
- 2-4ë°° ë¹ ë¥¸ ì†ë„
- ë™ì¼í•œ ê²°ê³¼ (exact, no approximation)
```

### 4.3. Parallelism

**í•˜ë‚˜ì˜ GPUë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤!**

**Data Parallelism**

```
GPU 1: Batch 1-32    â”
GPU 2: Batch 33-64   â”œâ”€ ë™ì¼í•œ ëª¨ë¸
GPU 3: Batch 65-96   â”˜

ê° GPUê°€ gradient ê³„ì‚° â†’ í‰ê·  â†’ ì—…ë°ì´íŠ¸
```

**Model Parallelism**

```
GPU 1: Layer 1-10   â”
GPU 2: Layer 11-20  â”œâ”€ ëª¨ë¸ ë¶„í• 
GPU 3: Layer 21-30  â”˜

ë°ì´í„°ê°€ GPUë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼
```

**Pipeline Parallelism**

```
Time step 1:
  GPU 1: Process Batch 1
  GPU 2: Idle
  GPU 3: Idle

Time step 2:
  GPU 1: Process Batch 2
  GPU 2: Process Batch 1
  GPU 3: Idle

Time step 3:
  GPU 1: Process Batch 3
  GPU 2: Process Batch 2
  GPU 3: Process Batch 1
```

## 5. LLM Tuning

Lecture 5ì—ì„œëŠ” LLMì„ ìœ ìš©í•˜ê²Œ ë§Œë“œëŠ” fine-tuning ê³¼ì •ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 5.1. 3ë‹¨ê³„ Training Pipeline

```
Step 1: Pre-training
  - ëª©ì : ì–¸ì–´/ì½”ë“œ êµ¬ì¡° í•™ìŠµ
  - ë°ì´í„°: Trillions of tokens
  - Task: Next token prediction
  - ê²°ê³¼: Autocomplete ê°€ëŠ¥í•œ ëª¨ë¸

  â†“

Step 2: Supervised Fine-Tuning (SFT)
  - ëª©ì : ì›í•˜ëŠ” í–‰ë™ í•™ìŠµ
  - ë°ì´í„°: Input-output pairs
  - Task: ì£¼ì–´ì§„ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
  - ê²°ê³¼: Helpfulí•œ ëª¨ë¸

  â†“

Step 3: Preference Tuning
  - ëª©ì : ì„ í˜¸ë„ ì •ë ¬
  - ë°ì´í„°: Preference pairs
  - Task: ì¢‹ì€ ì‘ë‹µ vs ë‚˜ìœ ì‘ë‹µ
  - ê²°ê³¼: ì¸ê°„ ì„ í˜¸ë„ì™€ ì •ë ¬ëœ ëª¨ë¸
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# Pre-training
Input: "The cat sat on the"
Output: "mat"  # Next token prediction

# SFT
Input: "What is the capital of France?"
Output: "The capital of France is Paris."

# Preference Tuning
Input: "Explain quantum computing"
Preferred: "Quantum computing uses quantum bits..."
Rejected: "Idk, google it lol"
```

### 5.2. Reward Modeling

**Bradley-Terry ê³µì‹**

```
P(output_i > output_j) = exp(r_i) / (exp(r_i) + exp(r_j))

ì—¬ê¸°ì„œ:
- r_i: Output iì˜ reward score
- r_j: Output jì˜ reward score
```

**Reward Model í•™ìŠµ**

```python
# Pairwise training
Input: "Write a Python function"
Output A: "def foo():\n    pass"  # ì„ í˜¸ë¨
Output B: "idk"                    # ê±°ë¶€ë¨

# Model í•™ìŠµ
r_A = reward_model(Input, Output A)  # ë†’ì€ ì ìˆ˜
r_B = reward_model(Input, Output B)  # ë‚®ì€ ì ìˆ˜

Loss = -log(exp(r_A) / (exp(r_A) + exp(r_B)))
```

**ì¶”ë¡  ì‹œì—ëŠ”:**

```python
# Single output scoring
output = llm.generate(prompt)
score = reward_model(prompt, output)
```

### 5.3. RL ê¸°ë°˜ Tuning

**LLM as RL Agent**

```
State: ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ í† í°
Action: ë‹¤ìŒ í† í° ì˜ˆì¸¡
Environment: í† í° ê³µê°„
Reward: Reward modelì˜ ì ìˆ˜
```

**PPO (Proximal Policy Optimization)**

```
Components:
1. Policy Model (LLM): í…ìŠ¤íŠ¸ ìƒì„±
2. Reward Model: í’ˆì§ˆ í‰ê°€
3. Value Model: ì˜ˆìƒ ë¯¸ë˜ ë³´ìƒ
4. Reference Model: ì›ë˜ SFT ëª¨ë¸ (regularization)

Objective:
  Maximize reward
  - KL penalty (reference modelë¡œë¶€í„° ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šê¸°)
```

**êµ¬ì²´ì ì¸ Flow:**

```python
# PPO Training Loop
for iteration in range(num_iterations):
    # 1. Generate rollouts
    prompts = sample_prompts()
    responses = policy_model.generate(prompts)

    # 2. Get rewards
    rewards = reward_model(prompts, responses)

    # 3. Get value predictions
    values = value_model(prompts, responses)

    # 4. Compute advantages
    advantages = compute_GAE(rewards, values)

    # 5. Update policy
    policy_model.update(advantages, reference_model)

    # 6. Update value model
    value_model.update(rewards)
```

## 6. LLM Reasoning

Lecture 6ì—ì„œëŠ” LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 6.1. Chain of Thought

**í•µì‹¬ ì•„ì´ë””ì–´: ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê¸°**

```
Without CoT:
Input: "Roger has 5 balls. He buys 2 more. How many balls does he have?"
Output: "7"

With CoT:
Input: "Roger has 5 balls. He buys 2 more. How many balls does he have?"
Output: "Let me think step by step:
1. Roger starts with 5 balls
2. He buys 2 more balls
3. Total = 5 + 2 = 7
Therefore, Roger has 7 balls."
```

**ì„±ëŠ¥ í–¥ìƒ**

```
ì‹¤í—˜ ê²°ê³¼ (MATH benchmark):
Without CoT: 30% accuracy
With CoT: 65% accuracy
â†’ 2ë°° ì´ìƒ í–¥ìƒ!
```

### 6.2. GRPO vs PPO

**PPOì˜ êµ¬ì¡°**

```
Components (4ê°œ):
1. Policy Model (LLM)
2. Reward Model
3. Value Model
4. Reference Model

ë¬¸ì œì :
- Value Model í•™ìŠµ/ìœ ì§€ ë¹„ìš©
- ë³µì¡í•œ êµ¬ì¡°
```

**GRPO (Group Relative Policy Optimization)**

```
Components (2ê°œë§Œ!):
1. Policy Model (LLM)
2. Reference Model

í•µì‹¬ ì•„ì´ë””ì–´:
- Value Model ì œê±°
- ì—¬ëŸ¬ completions ìƒì„±
- Rewardë¥¼ ì„œë¡œ ë¹„êµ (ìƒëŒ€ì )
```

**êµ¬ì²´ì ì¸ ë¹„êµ:**

```python
# PPO
for each prompt:
    completion = policy.generate(prompt)
    reward = reward_model(prompt, completion)
    value = value_model(prompt, completion)
    advantage = GAE(reward, value)  # Value model ì‚¬ìš©

# GRPO
for each prompt:
    completions = [policy.generate(prompt) for _ in range(K)]
    rewards = [reward_model(prompt, c) for c in completions]
    # Completionsë¼ë¦¬ ë¹„êµ (Value model ë¶ˆí•„ìš”!)
    advantages = rewards - mean(rewards)
```

**GRPOì˜ ì¥ì :**

1. **ë” ê°„ë‹¨**: Value model ë¶ˆí•„ìš”
2. **ë” íš¨ìœ¨ì **: ëª¨ë¸ í•˜ë‚˜ ëœ í•™ìŠµ
3. **Verifiable rewardsì— ì í•©**: ìˆ˜í•™ ë¬¸ì œ ë“±

### 6.3. GRPOì˜ ë¬¸ì œì ê³¼ ê°œì„ 

**Length Bias ë¬¸ì œ**

```
ë¬¸ì œ:
GRPOëŠ” ì§§ì€ í‹€ë¦° ë‹µì„ ë” ë§ì´ penalizeí•¨
â†’ ëª¨ë¸ì´ ê¸´ í‹€ë¦° ë‹µì„ ìƒì„±í•˜ê²Œ ë¨

ì˜ˆì‹œ:
Short incorrect: "The answer is 5" (ë§¤ìš° penalize)
Long incorrect: "Let me explain... [500 tokens]... so it's 5" (ëœ penalize)
â†’ ëª¨ë¸ì´ ê¸¸ê²Œ ì“°ëŠ” ë²•ì„ í•™ìŠµ!
```

**í•´ê²°ì±…: GRPO Done Right**

```python
# Original GRPO loss
loss = -advantages / length  # Lengthë¡œ normalize â†’ ë¬¸ì œ!

# GRPO Done Right
loss = -advantages  # Normalization ì œê±°
```

**DAPO (Direct Advantage Policy Optimization)**

```
ì¶”ê°€ ê°œì„ ì‚¬í•­:
1. Length bias ì œê±°
2. ë” ì•ˆì •ì ì¸ training
3. Reasoning tasksì— íŠ¹í™”
```

## 7. Agentic LLMs

Lecture 7ì—ì„œëŠ” LLMì„ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 7.1. RAG (Retrieval Augmented Generation)

**ì™œ í•„ìš”í•œê°€?**

```
ë¬¸ì œ:
LLMì˜ ì§€ì‹ = í•™ìŠµ ë°ì´í„°ê¹Œì§€ë§Œ (Knowledge Cutoff)

ì˜ˆì‹œ:
"2024ë…„ 10ì›”ì— ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆë‚˜ìš”?"
â†’ LLM (2024ë…„ 1ì›” í•™ìŠµ): ëª¨ë¦„!
```

**RAGì˜ êµ¬ì¡°**

```
User Query
  â†“
1. Retrieval (ê²€ìƒ‰)
   - Candidate Retrieval (Bi-encoder)
   - Reranking (Cross-encoder)
  â†“
2. Augmentation (ì¦ê°•)
   - ê´€ë ¨ ë¬¸ì„œë¥¼ promptì— ì¶”ê°€
  â†“
3. Generation (ìƒì„±)
   - LLMì´ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# Step 1: Candidate Retrieval (Bi-encoder)
query = "ìµœì‹  iPhone ê°€ê²©"
query_embedding = encoder(query)  # [0.1, -0.3, ...]

documents = [
    "iPhone 15 ProëŠ” $999ì…ë‹ˆë‹¤",
    "ì‚¼ì„± GalaxyëŠ”...",
    "MacBook ProëŠ”..."
]
doc_embeddings = [encoder(d) for d in documents]

# Cosine similarity
scores = [cosine(query_embedding, d_emb) for d_emb in doc_embeddings]
top_k = select_top_k(documents, scores, k=10)

# Step 2: Reranking (Cross-encoder)
rerank_scores = [cross_encoder(query, doc) for doc in top_k]
final_docs = select_top_k(top_k, rerank_scores, k=3)

# Step 3: Generation
prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

ë¬¸ì„œ: {final_docs}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

answer = llm.generate(prompt)
```

**Bi-encoder vs Cross-encoder**

```
Bi-encoder:
Queryì™€ Documentë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”©
  + ë¹ ë¦„ (ì‚¬ì „ ê³„ì‚° ê°€ëŠ¥)
  + ëŒ€ê·œëª¨ ê²€ìƒ‰ì— ì í•©
  - ì •í™•ë„ ë‚®ìŒ

Cross-encoder:
Queryì™€ Documentë¥¼ í•¨ê»˜ ì¸ì½”ë”©
  + ì •í™•ë„ ë†’ìŒ
  + ë” ì •êµí•œ ìœ ì‚¬ë„
  - ëŠë¦¼ (ëª¨ë“  pair ê³„ì‚°)
  - ì†Œê·œëª¨ rerankingì— ì í•©
```

### 7.2. Tool Calling

**LLMì´ API ì‚¬ìš©í•˜ê¸°**

```
2-Step Process:

Step 1: Tool Selection
  LLM decides: ì–´ë–¤ API? ì–´ë–¤ arguments?

Step 2: Tool Execution â†’ Final Answer
  API ì‹¤í–‰ â†’ ê²°ê³¼ë¥¼ LLMì—ê²Œ â†’ ìµœì¢… ë‹µë³€
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# User query
"What's the weather in Seoul?"

# Step 1: LLM decides to use weather API
llm_output = {
    "tool": "get_weather",
    "arguments": {
        "location": "Seoul",
        "units": "celsius"
    }
}

# Step 2: Execute API
weather_data = get_weather(location="Seoul", units="celsius")
# Returns: {"temperature": 15, "condition": "cloudy"}

# Step 3: LLM generates final answer
prompt = f"""
User asked: "What's the weather in Seoul?"
API returned: {weather_data}
Generate a natural response:
"""

final_answer = llm.generate(prompt)
# "The weather in Seoul is currently 15Â°C and cloudy."
```

**Modern Agentic Workflow**

```
User: "ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê³  ì´ë©”ì¼ë¡œ ë³´ë‚´ì¤˜"

Agent Flow:
1. RAG: ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
   â†“
2. Tool: data_analysis API í˜¸ì¶œ
   â†“
3. Tool: generate_report API í˜¸ì¶œ
   â†“
4. Tool: send_email API í˜¸ì¶œ
   â†“
5. LLM: ìµœì¢… ì‘ë‹µ ìƒì„±
   "ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì—¬ ì´ë©”ì¼ë¡œ ë°œì†¡í–ˆìŠµë‹ˆë‹¤."
```

## 8. LLM Evaluation

Lecture 8ì—ì„œëŠ” LLMì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 8.1. ì „í†µì ì¸ Metrics

**BLEU, ROUGE, METEOR**

```python
Reference: "The cat sat on the mat"
Hypothesis: "A cat sat on a mat"

BLEU Score: 0.75 (n-gram overlap ê¸°ë°˜)

ë¬¸ì œì :
Reference: "The movie was excellent"
Hypothesis: "The film was great"
â†’ BLEU: Low score (ë‹¨ì–´ê°€ ë‹¤ë¦„)
â†’ í•˜ì§€ë§Œ ì˜ë¯¸ëŠ” ë™ì¼!
```

**í•œê³„:**
- ë‹¨ì–´ ìˆ˜ì¤€ ì¼ì¹˜ë§Œ ê³ ë ¤
- ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¬´ì‹œ
- Paraphrase ì²˜ë¦¬ ëª»í•¨

### 8.2. LLM-as-a-Judge

**í•µì‹¬ ì•„ì´ë””ì–´: LLMìœ¼ë¡œ í‰ê°€í•˜ê¸°**

```python
# Evaluation Prompt
prompt = f"""
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì‘ë‹µì„ í‰ê°€í•˜ì„¸ìš”:
- ì •í™•ì„±
- ìœ ìš©ì„±
- ì•ˆì „ì„±

ì§ˆë¬¸: {question}
ì‘ë‹µ: {response}

ë¨¼ì € ì´ìœ ë¥¼ ì„¤ëª…í•˜ê³ , ê·¸ ë‹¤ìŒ Pass/Failì„ íŒë‹¨í•˜ì„¸ìš”.

í‰ê°€:"""

judgment = judge_llm.generate(prompt)
```

**ì˜ˆì‹œ:**

```
ì§ˆë¬¸: "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?"

ì‘ë‹µ: "sorted() í•¨ìˆ˜ë‚˜ .sort() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤."

LLM Judge í‰ê°€:
"ì´ìœ : ì‘ë‹µì´ ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ì„ ì •í™•íˆ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤.
sorted()ëŠ” ìƒˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ê³ , .sort()ëŠ” in-placeë¡œ
ì •ë ¬í•©ë‹ˆë‹¤. ì •í™•í•˜ê³  ìœ ìš©í•©ë‹ˆë‹¤.

íŒë‹¨: Pass"
```

**Biases (í¸í–¥)**

```
1. Position Bias
   Input: Compare A vs B
   LLM tends to prefer: A (ì²« ë²ˆì§¸)
   í•´ê²°: ìˆœì„œë¥¼ ë°”ê¿”ì„œ 2ë²ˆ í‰ê°€

2. Verbosity Bias
   Short answer: "Paris"
   Long answer: "Paris is the capital..."
   LLM tends to prefer: Long answer

3. Self-Enhancement Bias
   LLM prefers its own outputs
   í•´ê²°: ë‹¤ë¥¸ LLMì„ judgeë¡œ ì‚¬ìš©
```

### 8.3. Benchmarks

**ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ ì¹´í…Œê³ ë¦¬**

```
1. Knowledge (ì§€ì‹)
   - MMLU: ë‹¤ì–‘í•œ ì£¼ì œ ê°ê´€ì‹
   - TriviaQA: ì‚¬ì‹¤ ì§ˆë¬¸

2. Reasoning (ì¶”ë¡ )
   - MATH: ìˆ˜í•™ ë¬¸ì œ
   - GSM8K: ì´ˆë“± ìˆ˜í•™

3. Coding (ì½”ë”©)
   - HumanEval: Python í•¨ìˆ˜ ì‘ì„±
   - MBPP: ê¸°ë³¸ í”„ë¡œê·¸ë˜ë°

4. Safety (ì•ˆì „ì„±)
   - TruthfulQA: ì§„ì‹¤ì„±
   - ToxiGen: ìœ í•´ì„± íƒì§€
```

**ëª¨ë¸ ë¦´ë¦¬ìŠ¤ ì˜ˆì‹œ:**

```
New LLM Release Announcement:
"Our model achieves state-of-the-art performance:
- MMLU: 87.5%
- GSM8K: 92.3%
- HumanEval: 85.1%
- TruthfulQA: 78.9%"
```

---

# Part 2: 2025ë…„ í˜„ì¬ íŠ¸ë Œë“œ

ì´ì œ 2025ë…„ í˜„ì¬ íŠ¸ë Œë”©í•˜ê³  ìˆëŠ” í† í”½ë“¤ì„ ì‚´í´ë´…ë‹ˆë‹¤. ì´ ë‚´ìš©ì€ Final ì‹œí—˜ ë²”ìœ„ì— **í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤**.

## 1. Vision Transformer (ViT)

### 1.1. ë™ê¸°: Transformerë¥¼ Visionì— ì ìš©í•  ìˆ˜ ìˆì„ê¹Œ?

**Self-Attentionì˜ ë³¸ì§ˆ**

```
Self-Attentionì˜ í•µì‹¬:
- Queryê°€ ìˆìŒ
- ë‹¤ë¥¸ elements (Keys, Values)ê°€ ìˆìŒ
- Queryì™€ ê´€ë ¨ìˆëŠ” elements ì°¾ê¸°

ì§€ê¸ˆê¹Œì§€ elements = Text tokens (ë²¡í„°)

ì§ˆë¬¸: Elementsë¥¼ ì´ë¯¸ì§€ì˜ ë¶€ë¶„ìœ¼ë¡œ ë°”ê¾¸ë©´?
```

**Computer Visionì˜ ì „í†µì  ì ‘ê·¼**

```
Convolutional Neural Networks (CNN):
- Sliding windowë¡œ ì´ë¯¸ì§€ ìŠ¤ìº”
- Local patterns ì¸ì‹
- Inductive bias: ì´ë¯¸ì§€ì˜ êµ¬ì¡° ê°€ì •

ì˜ˆì‹œ:
[3x3 filter]ë¥¼ ì´ë¯¸ì§€ ìœ„ë¡œ slide
  â†’ ì—£ì§€ ê°ì§€
  â†’ í…ìŠ¤ì²˜ ì¸ì‹
  â†’ ê°ì²´ ì¸ì‹
```

### 1.2. ViT ì•„í‚¤í…ì²˜

**í•µì‹¬ ì•„ì´ë””ì–´: BERTë¥¼ Visionì— ì ìš©**

```
BERT (Encoder-only):
  Text tokens â†’ Transformer Encoder â†’ CLS embedding â†’ Classification

ViT:
  Image patches â†’ Transformer Encoder â†’ CLS embedding â†’ Classification
```

**ì™œ Encoder-only?**

```
Classification Task:
- ì´ë¯¸ì§€ê°€ ë¬´ì—‡ì¸ì§€ ë¶„ë¥˜
- í…ìŠ¤íŠ¸ ìƒì„± ë¶ˆí•„ìš”
- BERTì™€ ë™ì¼í•œ íŒ¨ëŸ¬ë‹¤ì„
```

### 1.3. Image Patching

**ì´ë¯¸ì§€ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜**

```python
# Original Image: 224 x 224 x 3 (RGB)

# Step 1: Divide into patches
Patch size: 16 x 16
Number of patches: (224/16) x (224/16) = 14 x 14 = 196 patches

# Step 2: Flatten each patch
Each patch: 16 x 16 x 3 = 768 values
Flatten â†’ 768-dim vector

# Step 3: Linear projection
patch_embedding = Linear(768, d_model)

# Result: 196 patch tokens
```

**ì‹œê°í™”:**

```
Original Image:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”  â”‚
â”‚  â”‚P1â”‚P2â”‚P3â”‚P4â”‚  â”‚
â”‚  â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤  â”‚
â”‚  â”‚P5â”‚P6â”‚P7â”‚P8â”‚  â”‚
â”‚  â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤  â”‚
â”‚  â”‚..â”‚..â”‚..â”‚..â”‚  â”‚
â”‚  â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tokens:
[CLS] [P1] [P2] [P3] ... [P196]
```

### 1.4. End-to-End ì˜ˆì‹œ

**Teddy Bear ì´ë¯¸ì§€ ë¶„ë¥˜**

```python
# 1. Image â†’ Patches
image = load_image("teddy_bear.jpg")  # 224x224x3
patches = divide_into_patches(image, patch_size=16)  # 196 patches

# 2. Patch Embedding
patch_tokens = []
for patch in patches:
    flat_patch = flatten(patch)  # 768 values
    embedding = linear_projection(flat_patch)  # d_model dim
    patch_tokens.append(embedding)

# 3. Add Position Embeddings
position_embeddings = learned_positions(196)
patch_tokens = [p + pos for p, pos in zip(patch_tokens, position_embeddings)]

# 4. Add CLS token
cls_token = learned_cls_embedding()
tokens = [cls_token] + patch_tokens

# 5. Transformer Encoder
for layer in transformer_layers:
    tokens = layer(tokens)  # Self-attention + FFN

# 6. Classification
cls_output = tokens[0]  # CLS tokenì˜ output
logits = classification_head(cls_output)  # [1000 classes]

prediction = argmax(logits)
# "Teddy Bear" (Class 853)
```

### 1.5. Inductive Bias

**CNN vs ViT**

```
CNN (Strong Inductive Bias):
- Local connectivity: ì´ì›ƒ í”½ì…€ë§Œ ì—°ê²°
- Translation invariance: ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ ë™ì¼ íŒ¨í„´
- Hierarchical structure: Low â†’ High level features

ì¥ì : ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
ë‹¨ì : ìœ ì—°ì„± ì œí•œ

ViT (Low Inductive Bias):
- Global connectivity: ëª¨ë“  patchê°€ ìƒí˜¸ì‘ìš©
- Positionì€ embeddingìœ¼ë¡œ í•™ìŠµ
- Flat structure (ë ˆì´ì–´ë§Œ stacking)

ì¥ì : ë” ìœ ì—°í•¨
ë‹¨ì : ë§ì€ ë°ì´í„° í•„ìš”
```

**ì‹¤í—˜ ê²°ê³¼ (2020 ViT Paper)**

```
Small dataset (ImageNet-1K, 1.3M images):
  CNN (ResNet): 85% accuracy
  ViT: 78% accuracy
  â†’ CNNì´ ë” ì¢‹ìŒ (inductive bias ë•ë¶„)

Large dataset (JFT-300M, 300M images):
  CNN (ResNet): 87% accuracy
  ViT: 90% accuracy
  â†’ ViTê°€ ë” ì¢‹ìŒ (ì¶©ë¶„í•œ ë°ì´í„°!)
```

**í•µì‹¬ ê²°ë¡ :**

```
If you have a LOT of data:
  â†’ ViT outperforms CNN
  â†’ Low inductive biasê°€ ì¥ì ìœ¼ë¡œ ì‘ìš©

If you have limited data:
  â†’ CNN still better
  â†’ Inductive biasê°€ ì¤‘ìš”
```

## 2. Multimodal LLMs

### 2.1. Vision-Language Models (VLM)

**ëª©í‘œ: ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ ë‹µë³€**

```
Input:
  - Image: [ì‚¬ì§„]
  - Text: "ì´ ì‚¬ì§„ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?"

Output:
  - Text: "í…Œì´ë¸” ìœ„ì— ê³ ì–‘ì´ê°€ ì•‰ì•„ìˆìŠµë‹ˆë‹¤."
```

**ì±Œë¦°ì§€:**

```
ë‘ ê°€ì§€ ë‹¤ë¥¸ modality:
1. Image: Continuous, 2D structure
2. Text: Discrete, 1D sequence

ì–´ë–»ê²Œ í•¨ê»˜ ì²˜ë¦¬í• ê¹Œ?
```

### 2.2. ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹

**Method 1: Early Fusion (ë” ì¼ë°˜ì )**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image  â”‚ â†’ Vision Encoder â†’ Image Tokens
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â†“
                           [Concat with Text]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â†“
â”‚  Text   â”‚ â†’ Tokenizer  â†’ Text Tokens
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â†“
                           Decoder-only LLM
                                    â†“
                              Generated Text
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# Image processing
image = load_image("cat.jpg")
image_tokens = vision_encoder(image)  # [256 tokens]

# Text processing
text = "What's in this image?"
text_tokens = tokenizer(text)  # [10 tokens]

# Concatenate
input_tokens = [
    "<image_start>",
    *image_tokens,  # 256 image tokens
    "<image_end>",
    *text_tokens    # 10 text tokens
]

# Generate
output = llm.generate(input_tokens)
# "This image shows a cat sitting on a table."
```

**Method 2: Cross-Attention (ëœ ì¼ë°˜ì )**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text   â”‚ â†’ Tokenizer â†’ Text Tokens
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â†“
                         Self-Attention
                              â†“
                    Cross-Attention â† Image Tokens
                              â†“                â†‘
                         Feed-Forward         â”‚
                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  Image  â”‚ â†’ Vision Encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë¹„êµ:**

```
Early Fusion (Method 1):
  + êµ¬í˜„ ê°„ë‹¨
  + ë” ì¼ë°˜ì 
  + ì˜ˆì‹œ: LAVA, GPT-4V

Cross-Attention (Method 2):
  + ë” ì •êµí•œ ìƒí˜¸ì‘ìš©
  - êµ¬í˜„ ë³µì¡
  + ì˜ˆì‹œ: Llama 3 (paperì—ì„œ ì–¸ê¸‰)
```

### 2.3. LAVA ëª¨ë¸

**Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image           â”‚
â”‚  [224x224x3]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  CLIP   â”‚ Vision Encoder
    â”‚ Encoder â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Projection  â”‚ Image â†’ LLM token space
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Image Tokens
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
    â”‚   LLaMA LLM     â”‚ â† Text Tokens
    â”‚  (Decoder-only) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    Generated Response
```

**Training Process**

```
Stage 1: Projection Layer Training
  - Vision encoder: Frozen (CLIP)
  - LLM: Frozen (LLaMA)
  - Projection: Trainable
  ëª©í‘œ: Image tokensë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆê²Œ ë³€í™˜

Stage 2: Full Fine-tuning
  - Vision encoder: Frozen
  - LLM: Trainable
  - Projection: Trainable
  ëª©í‘œ: Instruction following
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```python
# Load model
model = LAVA()

# Prepare inputs
image = load_image("scene.jpg")
prompt = "USER: <image>\nWhat objects are in this room?\nASSISTANT:"

# Generate
response = model.generate(image, prompt)
# "I can see a sofa, a coffee table, and a bookshelf in this room."
```

## 3. Diffusion-based LLMs

### 3.1. ë™ê¸°: Autoregressiveì˜ í•œê³„

**í˜„ì¬ LLMì˜ ìƒì„± ë°©ì‹**

```
Autoregressive Generation:
Token 1 â†’ Token 2 â†’ Token 3 â†’ ... â†’ Token N

ë¬¸ì œì :
- ìˆœì°¨ì  (Sequential)
- ë³‘ë ¬í™” ë¶ˆê°€ëŠ¥ (Inference time)
- ê¸´ ì‹œí€€ìŠ¤ = ëŠë¦° ìƒì„±
```

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ:**

```python
# Generate "The cat sat on the mat"

Step 1: [] â†’ "The"
Step 2: ["The"] â†’ "cat"
Step 3: ["The", "cat"] â†’ "sat"
Step 4: ["The", "cat", "sat"] â†’ "on"
Step 5: ["The", "cat", "sat", "on"] â†’ "the"
Step 6: ["The", "cat", "sat", "on", "the"] â†’ "mat"

ì´ 6 steps (ìˆœì°¨ì )
â†’ GPU ë³‘ë ¬í™” ëª»í•¨
```

**Training vs Inference**

```
Training (ë³‘ë ¬ ê°€ëŠ¥):
Input: ["The", "cat", "sat", "on", "the", "mat"]
       â†’ Causal maskë¡œ í•œë²ˆì— ì²˜ë¦¬

Inference (ìˆœì°¨ì ):
ê° í† í°ì„ í•˜ë‚˜ì”© ìƒì„±
â†’ ì´ ë¶€ë¶„ì´ ë³‘ëª©!
```

### 3.2. Diffusionì´ë€?

**Visionì—ì„œì˜ Diffusion**

```
Image Generation:

Forward Process (Training):
Clean Image â†’ Add Noise â†’ ... â†’ Pure Noise
[ì‚¬ì§„]      [ì•½ê°„ ë…¸ì´ì¦ˆ] ... [ì™„ì „ ë…¸ì´ì¦ˆ]

Reverse Process (Generation):
Pure Noise â†’ Denoise â†’ ... â†’ Clean Image
[ì™„ì „ ë…¸ì´ì¦ˆ] [ì•½ê°„ ë…¸ì´ì¦ˆ] ... [ì‚¬ì§„]
```

**ì™œ Visionì—ì„œ ì˜ ì‘ë™í•˜ëŠ”ê°€?**

```
Images are Continuous:
- í”½ì…€ ê°’: 0.0 ~ 255.0 (ì‹¤ìˆ˜)
- ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€ ê°€ëŠ¥
- ì ì§„ì ìœ¼ë¡œ denoise ê°€ëŠ¥

ì˜ˆì‹œ:
í”½ì…€ [100, 150, 200]
â†’ ë…¸ì´ì¦ˆ ì¶”ê°€: [105, 148, 203]
â†’ ë” ì¶”ê°€: [110, 145, 207]
...
```

**Diffusionì˜ ì¥ì **

```
Parallel Generation:
- ëª¨ë“  í”½ì…€ì„ ë™ì‹œì— ìƒì„±
- ì ì§„ì ìœ¼ë¡œ ê°œì„ 
- GPU ë³‘ë ¬í™” ê°€ëŠ¥
```

### 3.3. Textì— Diffusion ì ìš©í•˜ê¸°

**ë¬¸ì œ: TextëŠ” Discrete!**

```
Text Tokens:
"cat" (ID: 5234) â†’ discrete
"dog" (ID: 7821) â†’ discrete

ë¬¸ì œ:
"cat" + noise = ???
ì¤‘ê°„ ê°’ì´ ì—†ìŒ!

5234 + noise = 5239 â†’ ë‹¤ë¥¸ í† í°!
```

**í•´ê²° ë°©ë²•ë“¤**

**Approach 1: Embedding Spaceì—ì„œ Diffusion**

```python
# Token â†’ Continuous embedding
token = "cat"
embedding = token_embedding(token)  # [0.1, -0.3, 0.5, ...]

# Add noise to embedding
noisy_embedding = embedding + noise

# Denoise
denoised_embedding = diffusion_model.denoise(noisy_embedding)

# Embedding â†’ Token
output_token = nearest_token(denoised_embedding)
```

**Approach 2: Logit Spaceì—ì„œ Diffusion**

```python
# Start from uniform distribution
logits = uniform([vocab_size])  # ëª¨ë“  í† í° ë™ì¼ í™•ë¥ 

# Iterative refinement
for step in diffusion_steps:
    logits = diffusion_model.denoise(logits)
    # ì ì  íŠ¹ì • í† í°ì— í™•ë¥ ì´ ì§‘ì¤‘ë¨

# Final token
token = argmax(logits)
```

**Approach 3: Discrete Diffusion**

```python
# Discrete state spaceì—ì„œ ì§ì ‘ diffusion
# Mask tokens ì‚¬ìš©

Start: [MASK] [MASK] [MASK] [MASK] [MASK]
Step 1: [The] [MASK] [MASK] [MASK] [MASK]
Step 2: [The] [cat] [MASK] [MASK] [MASK]
Step 3: [The] [cat] [sat] [MASK] [MASK]
Step 4: [The] [cat] [sat] [on] [MASK]
Step 5: [The] [cat] [sat] [on] [mat]
```

### 3.4. ìµœê·¼ ë°œì „

**Google's Experimental Model (2024)**

```
ë°œí‘œ ë‚´ìš©:
- Text diffusion model
- 2-4ë°° ë¹ ë¥¸ ìƒì„± ì†ë„
- ë¹„ìŠ·í•œ í’ˆì§ˆ

í•µì‹¬ ê¸°ìˆ :
- Continuous embedding diffusion
- Efficient denoising network
```

**Inception AI (2024)**

```
ë°œí‘œ ë‚´ìš©:
- Diffusion-based LLM
- ìƒˆë¡œìš´ training paradigm
- ë³‘ë ¬ ìƒì„± ê°€ëŠ¥

í—¤ë“œë¼ì¸:
"Breaking the sequential bottleneck"
```

**ì±Œë¦°ì§€**

```
ì•„ì§ í•´ê²°í•´ì•¼ í•  ë¬¸ì œë“¤:
1. í’ˆì§ˆ: Autoregressiveë§Œí¼ ì¢‹ì€ê°€?
2. ì¼ê´€ì„±: ê¸´ í…ìŠ¤íŠ¸ì—ì„œ coherence ìœ ì§€
3. í•™ìŠµ: Trainingì´ ì•ˆì •ì ì¸ê°€?
4. ì‹¤ìš©ì„±: ì‹¤ì œ productionì— ì‚¬ìš© ê°€ëŠ¥í•œê°€?

í˜„ì¬ ìƒíƒœ:
- ë§¤ìš° í™œë°œí•œ ì—°êµ¬ ë¶„ì•¼
- Promising results
- Production-readyëŠ” ì•„ì§...
```

**ì™œ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê°€?**

```
Potential Benefits:
1. ë” ë¹ ë¥¸ ì¶”ë¡  (ë³‘ë ¬í™”)
2. ë” ìœ ì—°í•œ ìƒì„± (ìˆœì„œ ë¬´ê´€)
3. ìƒˆë¡œìš´ applications

Autoregressive ëŒ€ë¹„:
âœ… ì†ë„ (ë³‘ë ¬í™”)
â“ í’ˆì§ˆ (ì•„ì§ ì—°êµ¬ ì¤‘)
â“ ì•ˆì •ì„± (ì•„ì§ ì—°êµ¬ ì¤‘)
```

## 4. Transformerì˜ í™•ì¥

**TransformerëŠ” Textì—ë§Œ êµ­í•œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤!**

```
ì›ë˜: Machine Translation (Text â†’ Text)
  â†“
í™•ì¥: Other Text Tasks
  â†“
í™•ì¥: Vision (Image Understanding, Generation)
  â†“
í™•ì¥: Audio (Speech Recognition, Generation)
  â†“
í™•ì¥: Recommendation Systems
  â†“
í™•ì¥: ...
```

**Vision ë¶„ì•¼**

```
Image Understanding:
- ViT (Vision Transformer)
- CLIP (Contrastive Learning)
- DINOv2 (Self-supervised)

Image Generation:
- Diffusion Transformer (DiT)
- Stable Diffusion (U-Net + Attention)
- DALL-E 3
```

**Audio ë¶„ì•¼**

```
Speech Recognition:
- Whisper (OpenAI)
- Wav2Vec 2.0 (Meta)

Speech Generation:
- VALL-E (Microsoft)
- AudioLM (Google)
```

**Recommendation ë¶„ì•¼**

```
Self-Attention for User Behavior:
- SASRec (Self-Attentive Sequential Rec)
- BERT4Rec (BERT for Recommendation)

Item features â†’ Transformer â†’ Recommendations
```

**í•µì‹¬ ë©”ì‹œì§€**

```
TransformerëŠ” ë²”ìš© ì•„í‚¤í…ì²˜:
- Self-attentionì€ ë²”ìš© ë©”ì»¤ë‹ˆì¦˜
- ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš© ê°€ëŠ¥
- ê° ë„ë©”ì¸ì— ë§ê²Œ adaptation í•„ìš”

ê³µí†µ íŒ¨í„´:
1. Inputì„ token-like unitsë¡œ ë³€í™˜
2. Position information ì¶”ê°€
3. Self-attentionìœ¼ë¡œ ê´€ê³„ í•™ìŠµ
4. Task-specific headë¡œ ì¶œë ¥
```

---

# Part 3: ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„

## 1. ì½”ìŠ¤ ì „ì²´ ìš”ì•½

**Journey Through CME 295**

```
Week 1-2: Transformer ê¸°ì´ˆ
  - Tokenization, Embeddings
  - Self-Attention
  - Transformer Architecture

Week 3-4: Improvements & Training
  - RoPE, GQA, RMSNorm
  - Scaling Laws
  - Flash Attention

Week 5-6: Making LLMs Useful
  - Fine-tuning Pipeline
  - Reward Modeling
  - Reasoning (GRPO)

Week 7-8: Agents & Evaluation
  - RAG, Tool Calling
  - LLM-as-a-Judge
  - Benchmarks

Week 9: Current Trends
  - Vision Transformer
  - Multimodal LLMs
  - Diffusion-based LLMs
```

## 2. í•µì‹¬ Takeaways

**Technical Skills**

```
âœ… Transformer êµ¬ì¡° ì´í•´
âœ… Position embeddings (RoPE ë“±)
âœ… Attention ìµœì í™” (GQA, Flash)
âœ… Training pipeline (Pre-training â†’ SFT â†’ RLHF)
âœ… Reasoning techniques (CoT, GRPO)
âœ… RAG êµ¬í˜„
âœ… Evaluation ë°©ë²•
```

**Conceptual Understanding**

```
âœ… Scaling lawsì˜ ì¤‘ìš”ì„±
âœ… Autoregressiveì˜ ì¥ë‹¨ì 
âœ… Reward modelingì˜ ì›ë¦¬
âœ… LLMì˜ í•œê³„ì™€ í•´ê²° ë°©ë²•
âœ… ìµœì‹  íŠ¸ë Œë“œ ë°©í–¥
```

**Practical Knowledge**

```
âœ… ì–¸ì œ ì–´ë–¤ ê¸°ë²•ì„ ì‚¬ìš©í• ì§€
âœ… Trade-offs ì´í•´
âœ… Production considerations
âœ… ìµœì‹  ëª¨ë¸ë“¤ì˜ ì„ íƒ ì´ìœ 
```

## 3. Final ì‹œí—˜ ë²”ìœ„

**í¬í•¨ë˜ëŠ” ë‚´ìš© (Lecture 5-8)**

```
âœ… Lecture 5: LLM Tuning
   - SFT, Preference Tuning
   - Reward Modeling (Bradley-Terry)
   - PPO

âœ… Lecture 6: LLM Reasoning
   - Chain of Thought
   - GRPO vs PPO
   - GRPO Done Right, DAPO

âœ… Lecture 7: Agentic LLMs
   - RAG (Retrieval, Augmentation, Generation)
   - Bi-encoder vs Cross-encoder
   - Tool Calling

âœ… Lecture 8: LLM Evaluation
   - Traditional metrics (BLEU, ROUGE)
   - LLM-as-a-Judge
   - Biases (Position, Verbosity, Self-enhancement)
   - Benchmarks (MMLU, GSM8K, HumanEval, etc.)
```

**í¬í•¨ë˜ì§€ ì•ŠëŠ” ë‚´ìš©**

```
âŒ Lecture 9 (ì´ë²ˆ ê°•ì˜):
   - Vision Transformer
   - Multimodal LLMs
   - Diffusion-based LLMs
```

**ì‹œí—˜ ì¤€ë¹„ íŒ**

```
1. í•µì‹¬ ê°œë… ì´í•´
   - ìˆ˜ì‹ì˜ ì˜ë¯¸
   - ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ ì›ë¦¬
   - Trade-offs

2. êµ¬ì²´ì ì¸ ì˜ˆì‹œ
   - RAGì˜ 2-step process
   - GRPO vs PPO ì°¨ì´
   - Reward modeling í•™ìŠµ ê³¼ì •

3. ì‹¤ì „ ì ìš©
   - ì–¸ì œ ë¬´ì—‡ì„ ì‚¬ìš©í•˜ëŠ”ê°€
   - ë¬¸ì œ í•´ê²° ì ‘ê·¼ ë°©ë²•
```

---

# ìš©ì–´ ì •ë¦¬

## Vision ê´€ë ¨

- **ViT (Vision Transformer)**: Transformerë¥¼ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ì ìš©í•œ ëª¨ë¸
- **Patch**: ì´ë¯¸ì§€ë¥¼ ê³ ì • í¬ê¸°ë¡œ ë¶„í• í•œ ë‹¨ìœ„
- **Patch Embedding**: ê° patchë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •
- **Inductive Bias**: ëª¨ë¸ì´ ê°€ì§€ê³  ìˆëŠ” ì‚¬ì „ ê°€ì •
- **CNN (Convolutional Neural Network)**: ì „í†µì ì¸ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸
- **Local Connectivity**: ì¸ì ‘í•œ í”½ì…€ë§Œ ì—°ê²°í•˜ëŠ” êµ¬ì¡°
- **Translation Invariance**: ìœ„ì¹˜ì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ íŒ¨í„´ ì¸ì‹

## Diffusion ê´€ë ¨

- **Diffusion Model**: ë…¸ì´ì¦ˆ ì œê±°ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸
- **Forward Process**: ê¹¨ë—í•œ ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •
- **Reverse Process**: ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ ê¹¨ë—í•˜ê²Œ ë§Œë“œëŠ” ê³¼ì •
- **Denoising**: ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ê³¼ì •
- **Continuous Space**: ì—°ì†ì ì¸ ê°’ì„ ê°€ì§€ëŠ” ê³µê°„ (ì´ë¯¸ì§€ í”½ì…€)
- **Discrete Space**: ë¶ˆì—°ì†ì ì¸ ê°’ì„ ê°€ì§€ëŠ” ê³µê°„ (í…ìŠ¤íŠ¸ í† í°)
- **Embedding Space Diffusion**: ì„ë² ë”© ê³µê°„ì—ì„œ diffusion ìˆ˜í–‰
- **Logit Space Diffusion**: Logit ê³µê°„ì—ì„œ diffusion ìˆ˜í–‰
- **Discrete Diffusion**: Discrete ê³µê°„ì—ì„œ ì§ì ‘ diffusion ìˆ˜í–‰

## Multimodal ê´€ë ¨

- **VLM (Vision-Language Model)**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸
- **Early Fusion**: ì…ë ¥ ë‹¨ê³„ì—ì„œ modalityë¥¼ ê²°í•©
- **Cross-Attention**: í•œ modalityê°€ ë‹¤ë¥¸ modalityë¥¼ ì°¸ì¡°í•˜ëŠ” attention
- **Vision Encoder**: ì´ë¯¸ì§€ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ
- **Projection Layer**: í•œ ê³µê°„ì„ ë‹¤ë¥¸ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë ˆì´ì–´
- **CLIP**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì¼í•œ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ëª¨ë¸
- **LAVA**: ëŒ€í‘œì ì¸ open-source VLM
- **Modality**: ë°ì´í„°ì˜ í˜•íƒœ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ ë“±)

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

ì´ê²ƒìœ¼ë¡œ CME 295ì˜ ëª¨ë“  ê°•ì˜ê°€ ëë‚¬ìŠµë‹ˆë‹¤. Final ì‹œí—˜ì„ ì˜ ì¤€ë¹„í•˜ì‹œê³ , ì•ìœ¼ë¡œë„ LLM ë¶„ì•¼ì˜ ë°œì „ì„ ê³„ì† ì£¼ì‹œí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤!

**Final ì‹œí—˜ ì¤€ë¹„:**
- Lecture 5-8 ë³µìŠµ
- í•µì‹¬ ê°œë…ê³¼ ì˜ˆì‹œ ìˆ™ì§€
- Trade-offs ì´í•´
- ì‹¤ì „ ì ìš© ëŠ¥ë ¥

**í–¥í›„ í•™ìŠµ ë°©í–¥:**
- ìµœì‹  ë…¼ë¬¸ follow
- ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì‹¤í—˜
- Production ë°°í¬ ê²½í—˜
- ìƒˆë¡œìš´ íŠ¸ë Œë“œ ì£¼ì‹œ
