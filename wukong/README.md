- [Abstract](#abstract)
- [Materials](#materials)
- [Arguments](#arguments)
  - [Wukong 클래스 생성자](#wukong-클래스-생성자)
  - [각 인자의 역할과 예제](#각-인자의-역할과-예제)
    - [1. `num_layers: int`](#1-num_layers-int)
    - [2. `num_sparse_emb: int`](#2-num_sparse_emb-int)
    - [3. `dim_emb: int`](#3-dim_emb-int)
    - [4. `dim_input_sparse: int`](#4-dim_input_sparse-int)
    - [5. `dim_input_dense: int`](#5-dim_input_dense-int)
    - [6. `num_emb_lcb: int`](#6-num_emb_lcb-int)
    - [7. `num_emb_fmb: int`](#7-num_emb_fmb-int)
    - [8. `rank_fmb: int`](#8-rank_fmb-int)
    - [9. `num_hidden_wukong: int`](#9-num_hidden_wukong-int)
    - [10. `dim_hidden_wukong: int`](#10-dim_hidden_wukong-int)
    - [11. `num_hidden_head: int`](#11-num_hidden_head-int)
    - [12. `dim_hidden_head: int`](#12-dim_hidden_head-int)
    - [13. `dim_output: int`](#13-dim_output-int)
    - [14. `dropout: float = 0.0`](#14-dropout-float--00)
  - [전체 데이터 흐름 예제](#전체-데이터-흐름-예제)
- [`ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ`](#σσvᵢvⱼxᵢxⱼ)
  - [1. 기호 해석](#1-기호-해석)
    - [Σ (시그마) - 합계 기호](#σ-시그마---합계-기호)
    - [⟨vᵢ,vⱼ⟩ - 내적 (Dot Product)](#vᵢvⱼ---내적-dot-product)
    - [xᵢ, xⱼ - 특성 값들](#xᵢ-xⱼ---특성-값들)
  - [2. 이중 합계 ΣΣ의 의미](#2-이중-합계-σς의-의미)
    - [첫 번째 Σ (i에 대한 합계)](#첫-번째-σ-i에-대한-합계)
    - [두 번째 Σ (j에 대한 합계)](#두-번째-σ-j에-대한-합계)
    - [전체 의미](#전체-의미)
  - [3. 구체적인 예제](#3-구체적인-예제)
    - [3개 특성 예제](#3개-특성-예제)
    - [이중 합계 계산](#이중-합계-계산)
  - [4. 중복 제거 이유](#4-중복-제거-이유)
    - [모든 쌍을 계산하면?](#모든-쌍을-계산하면)
    - [효율적인 계산](#효율적인-계산)
  - [5. 실제 추천 시스템 예제](#5-실제-추천-시스템-예제)
    - [4개 특성 예제](#4개-특성-예제)
    - [상호작용 계산](#상호작용-계산)
  - [6. 수식의 직관적 의미](#6-수식의-직관적-의미)
    - [각 항의 의미](#각-항의-의미)
- [FM](#fm)
  - [1. Linear Regression의 문제점](#1-linear-regression의-문제점)
    - [기본 선형 회귀 모델](#기본-선형-회귀-모델)
    - [문제점: 특성 간 상호작용 무시](#문제점-특성-간-상호작용-무시)
  - [2. FM 수식 해석](#2-fm-수식-해석)
    - [FM 수식](#fm-수식)
    - [각 항의 의미](#각-항의-의미-1)
      - [1) w₀ (편향)](#1-w-편향)
      - [2) Σwᵢxᵢ (1차 항 - 선형 효과)](#2-σwᵢxᵢ-1차-항---선형-효과)
      - [3) ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ (2차 항 - 상호작용)](#3-σσvᵢvⱼxᵢxⱼ-2차-항---상호작용)
  - [3. 상세한 예제](#3-상세한-예제)
    - [입력 데이터](#입력-데이터)
    - [계산 과정](#계산-과정)
      - [1) 1차 항 계산](#1-1차-항-계산)
      - [2) 2차 항 계산 (상호작용)](#2-2차-항-계산-상호작용)
      - [3) 최종 예측](#3-최종-예측)
  - [4. 상호작용의 직관적 이해](#4-상호작용의-직관적-이해)
    - [왜 내적(⟨vᵢ,vⱼ⟩)을 사용하는가?](#왜-내적vᵢvⱼ을-사용하는가)
      - [1) 임베딩의 의미](#1-임베딩의-의미)
      - [2) 내적의 의미](#2-내적의-의미)
  - [5. 실제 추천 시스템 예제](#5-실제-추천-시스템-예제-1)
    - [시나리오: 온라인 쇼핑몰](#시나리오-온라인-쇼핑몰)
    - [FM 계산](#fm-계산)
  - [6. FM의 장점](#6-fm의-장점)
    - [1) 희소 데이터에서도 효과적](#1-희소-데이터에서도-효과적)
    - [2) 계산 효율성](#2-계산-효율성)
    - [3) 해석 가능성](#3-해석-가능성)
- [FMB](#fmb)
  - [1. Factorization Machine (FM) 기본 개념](#1-factorization-machine-fm-기본-개념)
    - [FM이 해결하는 문제](#fm이-해결하는-문제)
    - [FM의 해결책](#fm의-해결책)
  - [2. FMB (Factorization Machine Block) 구조](#2-fmb-factorization-machine-block-구조)
    - [FMB의 핵심 아이디어](#fmb의-핵심-아이디어)
    - [단계별 처리 과정](#단계별-처리-과정)
    - [FMB 처리 과정](#fmb-처리-과정)
  - [3. FMB의 장점](#3-fmb의-장점)
    - [1. 효율적인 상호작용 계산](#1-효율적인-상호작용-계산)
    - [2. 학습 가능한 상호작용](#2-학습-가능한-상호작용)
    - [3. 차원 축소 효과](#3-차원-축소-효과)
  - [4. 실제 추천 시스템에서의 활용](#4-실제-추천-시스템에서의-활용)
    - [사용자-아이템 상호작용 예제](#사용자-아이템-상호작용-예제)
    - [상호작용 패턴 학습](#상호작용-패턴-학습)
  - [5. FMB vs 다른 방법들](#5-fmb-vs-다른-방법들)
    - [FMB vs DeepFM](#fmb-vs-deepfm)
    - [FMB vs DCN (Deep \& Cross Network)](#fmb-vs-dcn-deep--cross-network)

-----

# Abstract

대규모 추천 시스템에서도 언어 모델처럼 성능이 예측 가능하게 향상되는 "스케일링 법칙(scaling law)"을 만들기 위해, 효율적이고 확장 가능한 아키텍처

# Materials

- [Wukong: Towards a Scaling Law for Large-Scale Recommendation | arxiv](https://arxiv.org/abs/2403.02545)
- [wukong-recommendation | github](https://github.com/clabrugere/wukong-recommendation)

# Arguments

## Wukong 클래스 생성자

```python
class Wukong(nn.Module):
    def __init__(
        self,
        num_layers: int,
        num_sparse_emb: int,
        dim_emb: int,
        dim_input_sparse: int,
        dim_input_dense: int,
        num_emb_lcb: int,
        num_emb_fmb: int,
        rank_fmb: int,
        num_hidden_wukong: int,
        dim_hidden_wukong: int,
        num_hidden_head: int,
        dim_hidden_head: int,
        dim_output: int,
        dropout: float = 0.0,
    ) -> None:
```

## 각 인자의 역할과 예제

### 1. `num_layers: int`
**역할**: Wukong 레이어의 개수

**설명**: Factorization Machine 기반 상호작용 레이어의 깊이를 결정합니다.

**예제**:
```python
num_layers = 8

# 실제 입력 데이터
sparse_inputs = torch.tensor([
    [123456, 789012, 5, 14, 2, 100, 200, 300],  # 사용자1: 8개 스파스 특성
    [234567, 890123, 3, 20, 5, 150, 250, 350]   # 사용자2: 8개 스파스 특성
])
# shape: [2, 8]

dense_inputs = torch.tensor([
    [25.0, 100.0, 4.5, 0.8],  # 사용자1: 4개 밀집 특성
    [32.0, 150.0, 3.2, 0.6]   # 사용자2: 4개 밀집 특성
])
# shape: [2, 4]
```

### 2. `num_sparse_emb: int`
**역할**: 스파스 임베딩 테이블의 어휘 크기

**설명**: 카테고리형 변수들이 가질 수 있는 최대 고유 값 개수입니다.

**예제**:
```python
num_sparse_emb = 1000000  # 100만개

# 실제 카테고리형 값들
user_ids = [123456, 234567, 345678, ...]  # 0~999,999 범위
item_ids = [789012, 890123, 901234, ...]  # 0~999,999 범위
categories = [1, 2, 3, 4, 5, ...]         # 0~999 범위
hours = [0, 1, 2, ..., 23]                # 0~23 범위
days = [0, 1, 2, 3, 4, 5, 6]             # 0~6 범위
```

### 3. `dim_emb: int`
**역할**: 임베딩 벡터의 차원

**설명**: 각 카테고리형 변수를 몇 차원의 벡터로 변환할지 결정합니다.

**예제**:
```python
dim_emb = 24

# 임베딩 변환 예시
user_id_123456 = [0.1, 0.2, 0.3, ..., 0.24]  # 24차원
item_id_789012 = [0.5, 0.6, 0.7, ..., 0.48]  # 24차원
category_5 = [0.3, 0.4, 0.5, ..., 0.72]      # 24차원
```

### 4. `dim_input_sparse: int`
**역할**: 스파스 입력 특성의 개수

**설명**: 카테고리형 변수들의 개수를 지정합니다.

**예제**:
```python
dim_input_sparse = 8

# 8개 스파스 특성
sparse_features = [
    'user_id',      # 사용자 ID
    'item_id',      # 아이템 ID  
    'category',     # 카테고리
    'hour',         # 시간
    'day_of_week',  # 요일
    'feature_5',    # 추가 특성1
    'feature_6',    # 추가 특성2
    'feature_7'     # 추가 특성3
]
```

### 5. `dim_input_dense: int`
**역할**: 밀집 입력 특성의 개수

**설명**: 연속형 변수들의 개수를 지정합니다.

**예제**:
```python
dim_input_dense = 4

# 4개 밀집 특성
dense_features = [
    'age',      # 나이 (0-100)
    'price',    # 가격 (0-10000)
    'rating',   # 평점 (0-5)
    'discount'  # 할인율 (0-1)
]
```

### 6. `num_emb_lcb: int`
**역할**: Linear Compression Block의 출력 특성 수

**설명**: 선형 압축 블록에서 생성할 특성의 개수입니다.

**예제**:
```python
num_emb_lcb = 8

# LCB 처리 과정
# 입력: [2, 12, 24] (배치, 특성수, 임베딩차원)
# 출력: [2, 8, 24] (배치, LCB특성수, 임베딩차원)
```

### 7. `num_emb_fmb: int`
**역할**: Factorization Machine Block의 출력 특성 수

**설명**: Factorization Machine 블록에서 생성할 특성의 개수입니다.

**예제**:
```python
num_emb_fmb = 8

# FMB 처리 과정
# 입력: [2, 12, 24] (배치, 특성수, 임베딩차원)
# 출력: [2, 8, 24] (배치, FMB특성수, 임베딩차원)
```

### 8. `rank_fmb: int`
**역할**: Factorization Machine의 랭크

**설명**: FM에서 2차 상호작용을 계산할 때 사용하는 랭크 값입니다.

**예제**:
```python
rank_fmb = 24

# FM 계산 과정
# 특성 간 2차 상호작용을 24차원으로 압축하여 계산
# 예: user_id × item_id → 24차원 벡터
```

### 9. `num_hidden_wukong: int`
**역할**: Wukong 레이어 내 MLP의 히든 레이어 수

**설명**: Factorization Machine Block 내부의 MLP 깊이입니다.

**예제**:
```python
num_hidden_wukong = 3

# MLP 구조: 입력 → 3개 히든 레이어 → 출력
# 각 히든 레이어: dim_hidden_wukong = 2048 뉴런
```

### 10. `dim_hidden_wukong: int`
**역할**: Wukong 레이어 내 MLP의 히든 레이어 차원

**설명**: Factorization Machine Block 내부 MLP의 뉴런 수입니다.

**예제**:
```python
dim_hidden_wukong = 2048

# MLP 구조
# 입력: num_emb_in * rank = 12 * 24 = 288
# 히든1: 2048 뉴런
# 히든2: 2048 뉴런  
# 히든3: 2048 뉴런
# 출력: num_emb_out * dim_emb = 8 * 24 = 192
```

### 11. `num_hidden_head: int`
**역할**: 최종 projection head의 히든 레이어 수

**설명**: 최종 예측을 위한 MLP의 깊이입니다.

**예제**:
```python
num_hidden_head = 3

# Projection Head 구조
# 입력: (num_emb_lcb + num_emb_fmb) * dim_emb = 16 * 24 = 384
# 히든1: dim_hidden_head = 2048 뉴런
# 히든2: 2048 뉴런
# 히든3: 2048 뉴런
# 출력: dim_output = 1 (클릭 확률)
```

### 12. `dim_hidden_head: int`
**역할**: 최종 projection head의 히든 레이어 차원

**설명**: 최종 예측 MLP의 뉴런 수입니다.

**예제**:
```python
dim_hidden_head = 2048

# Projection Head의 각 히든 레이어: 2048 뉴런
```

### 13. `dim_output: int`
**역할**: 최종 출력 차원

**설명**: 모델의 최종 출력 크기입니다.

**예제**:
```python
dim_output = 1

# 이진 분류 (클릭 여부)
# 출력: [0.85] (클릭 확률 85%)
```

### 14. `dropout: float = 0.0`
**역할**: 드롭아웃 비율

**설명**: 과적합 방지를 위한 드롭아웃 비율입니다.

**예제**:
```python
dropout = 0.1

# 각 히든 레이어에서 10%의 뉴런을 무작위로 비활성화
```

## 전체 데이터 흐름 예제

```python
# 설정
config = {
    'num_layers': 8,
    'num_sparse_emb': 1000000,
    'dim_emb': 24,
    'dim_input_sparse': 8,
    'dim_input_dense': 4,
    'num_emb_lcb': 8,
    'num_emb_fmb': 8,
    'rank_fmb': 24,
    'num_hidden_wukong': 3,
    'dim_hidden_wukong': 2048,
    'num_hidden_head': 3,
    'dim_hidden_head': 2048,
    'dim_output': 1,
    'dropout': 0.0
}

# 입력 데이터
sparse_inputs = torch.tensor([
    [123456, 789012, 5, 14, 2, 100, 200, 300],  # 8개 스파스 특성
    [234567, 890123, 3, 20, 5, 150, 250, 350]
])
dense_inputs = torch.tensor([
    [25.0, 100.0, 4.5, 0.8],  # 4개 밀집 특성
    [32.0, 150.0, 3.2, 0.6]
])

# 모델 생성
model = Wukong(**config)

# 추론
outputs = model(sparse_inputs, dense_inputs)
# shape: [2, 1] - 배치 크기 2, 클릭 확률 1개
# 출력: [[0.85], [0.72]] - 사용자1은 85%, 사용자2는 72% 클릭 확률
```

이렇게 각 인자가 모델의 구조와 처리 과정을 결정하는 중요한 역할을 합니다.

# `ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ`

`ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ` 수식을 자세히 설명해드리겠습니다.

## 1. 기호 해석

### Σ (시그마) - 합계 기호
```python
# Σ는 "합계"를 의미
# Σᵢ = i=1부터 n까지의 합계

# 예시:
Σᵢ xᵢ = x₁ + x₂ + x₃ + ... + xₙ
```

### ⟨vᵢ,vⱼ⟩ - 내적 (Dot Product)
```python
# ⟨vᵢ,vⱼ⟩는 두 벡터 vᵢ와 vⱼ의 내적
# ⟨vᵢ,vⱼ⟩ = vᵢ₁×vⱼ₁ + vᵢ₂×vⱼ₂ + ... + vᵢₖ×vⱼₖ

# 예시:
vᵢ = [0.1, 0.2, 0.3]  # i번째 특성의 임베딩
vⱼ = [0.4, 0.5, 0.6]  # j번째 특성의 임베딩
⟨vᵢ,vⱼ⟩ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.04 + 0.10 + 0.18 = 0.32
```

### xᵢ, xⱼ - 특성 값들
```python
# xᵢ: i번째 특성의 값
# xⱼ: j번째 특성의 값

# 예시:
x₁ = 25   # 나이
x₂ = 100  # 가격
x₃ = 1    # 카테고리
```

## 2. 이중 합계 ΣΣ의 의미

### 첫 번째 Σ (i에 대한 합계)
```python
# i = 1부터 n까지 (n은 특성 개수)
# 각 특성 i에 대해
```

### 두 번째 Σ (j에 대한 합계)
```python
# j = i+1부터 n까지
# i보다 큰 j에 대해서만 (중복 제거)
```

### 전체 의미
```python
ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ = Σᵢ₌₁ⁿ Σⱼ₌ᵢ₊₁ⁿ ⟨vᵢ,vⱼ⟩xᵢxⱼ

# 모든 특성 쌍 (i,j)에 대해 상호작용 계산
# 단, i < j인 경우만 (중복 방지)
```

## 3. 구체적인 예제

### 3개 특성 예제
```python
# 특성들
x₁ = 25   # 나이
x₂ = 100  # 가격  
x₃ = 1    # 카테고리

# 임베딩 벡터들 (3차원)
v₁ = [0.1, 0.2, 0.3]  # 나이 임베딩
v₂ = [0.4, 0.5, 0.6]  # 가격 임베딩
v₃ = [0.7, 0.8, 0.9]  # 카테고리 임베딩
```

### 이중 합계 계산
```python
# i=1, j=2 (나이 × 가격)
⟨v₁,v₂⟩x₁x₂ = (0.1×0.4 + 0.2×0.5 + 0.3×0.6) × 25 × 100
             = (0.04 + 0.10 + 0.18) × 2500
             = 0.32 × 2500
             = 800

# i=1, j=3 (나이 × 카테고리)
⟨v₁,v₃⟩x₁x₃ = (0.1×0.7 + 0.2×0.8 + 0.3×0.9) × 25 × 1
             = (0.07 + 0.16 + 0.27) × 25
             = 0.50 × 25
             = 12.5

# i=2, j=3 (가격 × 카테고리)
⟨v₂,v₃⟩x₂x₃ = (0.4×0.7 + 0.5×0.8 + 0.6×0.9) × 100 × 1
             = (0.28 + 0.40 + 0.54) × 100
             = 1.22 × 100
             = 122

# 최종 결과
ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ = 800 + 12.5 + 122 = 934.5
```

## 4. 중복 제거 이유

### 모든 쌍을 계산하면?
```python
# 3개 특성의 모든 쌍 (9개)
# (1,1), (1,2), (1,3)
# (2,1), (2,2), (2,3)  
# (3,1), (3,2), (3,3)

# 문제점:
# (1,2)와 (2,1)은 같은 상호작용
# (1,1), (2,2), (3,3)은 자기 자신과의 상호작용 (의미 없음)
```

### 효율적인 계산
```python
# 실제 계산하는 쌍들 (3개)
# (1,2), (1,3), (2,3)

# 이유:
# ⟨vᵢ,vⱼ⟩ = ⟨vⱼ,vᵢ⟩ (내적은 교환법칙 성립)
# ⟨vᵢ,vᵢ⟩는 자기 자신과의 상호작용 (의미 없음)
```

## 5. 실제 추천 시스템 예제

### 4개 특성 예제
```python
# 특성들
x₁ = 25   # 나이
x₂ = 1    # 성별 (1=여성)
x₃ = 50   # 가격
x₄ = 2    # 카테고리 (2=화장품)

# 임베딩 벡터들
v₁ = [0.8, 0.2, 0.1]  # 나이
v₂ = [0.3, 0.9, 0.2]  # 성별
v₃ = [0.9, 0.1, 0.3]  # 가격
v₄ = [0.2, 0.8, 0.4]  # 카테고리
```

### 상호작용 계산
```python
# 계산하는 쌍들: (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)

# (1,2): 나이 × 성별
⟨v₁,v₂⟩x₁x₂ = (0.8×0.3 + 0.2×0.9 + 0.1×0.2) × 25 × 1
             = (0.24 + 0.18 + 0.02) × 25
             = 0.44 × 25 = 11

# (1,3): 나이 × 가격
⟨v₁,v₃⟩x₁x₃ = (0.8×0.9 + 0.2×0.1 + 0.1×0.3) × 25 × 50
             = (0.72 + 0.02 + 0.03) × 1250
             = 0.77 × 1250 = 962.5

# (1,4): 나이 × 카테고리
⟨v₁,v₄⟩x₁x₄ = (0.8×0.2 + 0.2×0.8 + 0.1×0.4) × 25 × 2
             = (0.16 + 0.16 + 0.04) × 50
             = 0.36 × 50 = 18

# (2,3): 성별 × 가격
⟨v₂,v₃⟩x₂x₃ = (0.3×0.9 + 0.9×0.1 + 0.2×0.3) × 1 × 50
             = (0.27 + 0.09 + 0.06) × 50
             = 0.42 × 50 = 21

# (2,4): 성별 × 카테고리
⟨v₂,v₄⟩x₂x₄ = (0.3×0.2 + 0.9×0.8 + 0.2×0.4) × 1 × 2
             = (0.06 + 0.72 + 0.08) × 2
             = 0.86 × 2 = 1.72

# (3,4): 가격 × 카테고리
⟨v₃,v₄⟩x₃x₄ = (0.9×0.2 + 0.1×0.8 + 0.3×0.4) × 50 × 2
             = (0.18 + 0.08 + 0.12) × 100
             = 0.38 × 100 = 38

# 최종 상호작용 합계
ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ = 11 + 962.5 + 18 + 21 + 1.72 + 38 = 1052.22
```

## 6. 수식의 직관적 의미

### 각 항의 의미
```python
# ⟨vᵢ,vⱼ⟩: 두 특성의 "유사도" 또는 "상관관계"
# xᵢxⱼ: 두 특성 값의 곱 (상호작용 강도)
# ⟨vᵢ,vⱼ⟩xᵢxⱼ: 두 특성의 상호작용 효과

# 예시:
# ⟨v_나이,v_가격⟩ = 0.77 (높은 상관관계)
# x_나이 = 25, x_가격 = 50
# 상호작용 = 0.77 × 25 × 50 = 962.5 (큰 양수)
# → 젊은 사용자 + 저가 아이템 = 높은 클릭률
```

이렇게 `ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ`는 모든 특성 쌍의 상호작용을 효율적으로 계산하는 핵심 수식입니다!

# FM

Factorization Machine (FM)의 수식을 단계별로 자세히 설명해드리겠습니다.

## 1. Linear Regression의 문제점

### 기본 선형 회귀 모델
```python
y = w₀ + w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ
```

### 문제점: 특성 간 상호작용 무시
```python
# 예시: 온라인 쇼핑 추천 시스템
# x₁: 사용자 나이 (25세)
# x₂: 아이템 가격 (100달러)
# x₃: 아이템 카테고리 (전자제품)

# 선형 모델 예측
y = w₀ + w₁×25 + w₂×100 + w₃×1

# 문제: 나이와 가격의 상호작용을 고려하지 못함!
# 실제로는: 젊은 사용자(25세) + 저가 아이템(100달러) = 높은 구매 확률
# 하지만 선형 모델은 이런 패턴을 학습할 수 없음
```

## 2. FM 수식 해석

### FM 수식
```python
y = w₀ + Σwᵢxᵢ + ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ
```

### 각 항의 의미

#### 1) w₀ (편향)
```python
w₀ = 0.3  # 기본 클릭률 30%

# 모든 특성이 0일 때의 기본값
# 예: 사용자가 아무것도 클릭하지 않았을 때의 기본 확률
```

#### 2) Σwᵢxᵢ (1차 항 - 선형 효과)
```python
# 각 특성의 개별 영향
Σwᵢxᵢ = w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ

# 예시:
w₁ = 0.01  # 나이 1살 증가당 +1% 클릭률
w₂ = -0.005 # 가격 1달러 증가당 -0.5% 클릭률
w₃ = 0.02  # 전자제품 카테고리 +2% 클릭률

# 사용자(25세, 100달러, 전자제품)
Σwᵢxᵢ = 0.01×25 + (-0.005)×100 + 0.02×1 = 0.25 - 0.5 + 0.02 = -0.23
```

#### 3) ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ (2차 항 - 상호작용)
```python
# 특성 쌍의 상호작용 효과
# ⟨vᵢ,vⱼ⟩: 두 특성 임베딩의 내적

# 예시: 3개 특성 (나이, 가격, 카테고리)
# 상호작용 항들:
# ⟨v₁,v₂⟩x₁x₂ (나이 × 가격)
# ⟨v₁,v₃⟩x₁x₃ (나이 × 카테고리)  
# ⟨v₂,v₃⟩x₂x₃ (가격 × 카테고리)
```

## 3. 상세한 예제

### 입력 데이터
```python
# 특성 값들
x₁ = 25  # 사용자 나이
x₂ = 100 # 아이템 가격 (달러)
x₃ = 1   # 아이템 카테고리 (1=전자제품)

# 1차 가중치들
w₀ = 0.3
w₁ = 0.01
w₂ = -0.005
w₃ = 0.02

# 임베딩 벡터들 (각 특성을 3차원으로 표현)
v₁ = [0.1, 0.2, 0.3]  # 나이 특성 임베딩
v₂ = [0.4, 0.5, 0.6]  # 가격 특성 임베딩
v₃ = [0.7, 0.8, 0.9]  # 카테고리 특성 임베딩
```

### 계산 과정

#### 1) 1차 항 계산
```python
Σwᵢxᵢ = w₁x₁ + w₂x₂ + w₃x₃
       = 0.01×25 + (-0.005)×100 + 0.02×1
       = 0.25 - 0.5 + 0.02
       = -0.23
```

#### 2) 2차 항 계산 (상호작용)
```python
# ⟨v₁,v₂⟩ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.04 + 0.10 + 0.18 = 0.32
# ⟨v₁,v₃⟩ = 0.1×0.7 + 0.2×0.8 + 0.3×0.9 = 0.07 + 0.16 + 0.27 = 0.50
# ⟨v₂,v₃⟩ = 0.4×0.7 + 0.5×0.8 + 0.6×0.9 = 0.28 + 0.40 + 0.54 = 1.22

ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ = ⟨v₁,v₂⟩x₁x₂ + ⟨v₁,v₃⟩x₁x₃ + ⟨v₂,v₃⟩x₂x₃
                = 0.32×25×100 + 0.50×25×1 + 1.22×100×1
                = 800 + 12.5 + 122
                = 934.5
```

#### 3) 최종 예측
```python
y = w₀ + Σwᵢxᵢ + ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ
  = 0.3 + (-0.23) + 934.5
  = 934.57

# 시그모이드 적용하여 확률로 변환
probability = 1 / (1 + exp(-934.57)) ≈ 1.0
# 매우 높은 클릭 확률!
```

## 4. 상호작용의 직관적 이해

### 왜 내적(⟨vᵢ,vⱼ⟩)을 사용하는가?

#### 1) 임베딩의 의미
```python
# v₁ = [0.1, 0.2, 0.3] (나이 특성)
# v₂ = [0.4, 0.5, 0.6] (가격 특성)

# 각 차원이 특정 "잠재 요인"을 나타냄
# 차원1: "가격 민감도" (0.1 vs 0.4)
# 차원2: "브랜드 선호도" (0.2 vs 0.5)  
# 차원3: "기술 친숙도" (0.3 vs 0.6)
```

#### 2) 내적의 의미
```python
⟨v₁,v₂⟩ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.32

# 내적이 크다 = 두 특성이 유사한 패턴을 가짐
# 내적이 작다 = 두 특성이 다른 패턴을 가짐

# 예시:
# - 나이와 가격이 모두 높은 상관관계 → 내적 큼
# - 나이와 카테고리가 독립적 → 내적 작음
```

## 5. 실제 추천 시스템 예제

### 시나리오: 온라인 쇼핑몰
```python
# 특성들
features = {
    'user_age': 25,           # 사용자 나이
    'user_gender': 1,         # 사용자 성별 (1=여성)
    'item_price': 50,         # 아이템 가격
    'item_category': 2,       # 아이템 카테고리 (2=화장품)
    'hour': 20,               # 시간 (저녁 8시)
    'day_of_week': 5          # 요일 (금요일)
}

# 학습된 가중치들
weights = {
    'w₀': 0.1,               # 기본 클릭률 10%
    'w_age': 0.005,          # 나이 1살당 +0.5%
    'w_gender': 0.02,        # 여성 +2%
    'w_price': -0.01,        # 가격 1달러당 -1%
    'w_category': 0.03,      # 화장품 +3%
    'w_hour': 0.001,         # 시간 1시간당 +0.1%
    'w_day': 0.005           # 금요일 +0.5%
}

# 임베딩 벡터들 (3차원)
embeddings = {
    'v_age': [0.8, 0.2, 0.1],      # 나이: 가격민감도 높음
    'v_gender': [0.3, 0.9, 0.2],   # 성별: 카테고리선호도 높음
    'v_price': [0.9, 0.1, 0.3],    # 가격: 가격민감도 매우높음
    'v_category': [0.2, 0.8, 0.4], # 카테고리: 성별선호도 높음
    'v_hour': [0.4, 0.3, 0.7],     # 시간: 시간대선호도 높음
    'v_day': [0.5, 0.4, 0.6]       # 요일: 시간대선호도 높음
}
```

### FM 계산
```python
# 1차 항
linear = 0.1 + 0.005×25 + 0.02×1 + (-0.01)×50 + 0.03×2 + 0.001×20 + 0.005×5
       = 0.1 + 0.125 + 0.02 + (-0.5) + 0.06 + 0.02 + 0.025
       = -0.17

# 주요 상호작용들
# 1. 성별 × 카테고리 (여성 × 화장품)
⟨v_gender, v_category⟩ = 0.3×0.2 + 0.9×0.8 + 0.2×0.4 = 0.06 + 0.72 + 0.08 = 0.86

# 2. 나이 × 가격 (젊은 사용자 × 저가)
⟨v_age, v_price⟩ = 0.8×0.9 + 0.2×0.1 + 0.1×0.3 = 0.72 + 0.02 + 0.03 = 0.77

# 3. 시간 × 요일 (저녁 × 금요일)
⟨v_hour, v_day⟩ = 0.4×0.5 + 0.3×0.4 + 0.7×0.6 = 0.20 + 0.12 + 0.42 = 0.74

# 2차 항
interaction = 0.86×1×2 + 0.77×25×50 + 0.74×20×5
            = 1.72 + 962.5 + 74
            = 1038.22

# 최종 예측
y = -0.17 + 1038.22 = 1038.05
probability = 1 / (1 + exp(-1038.05)) ≈ 1.0
# 매우 높은 클릭 확률!
```

## 6. FM의 장점

### 1) 희소 데이터에서도 효과적
```python
# 특성 조합이 학습 데이터에 없어도 예측 가능
# 예: (나이=30, 가격=75) 조합이 없어도
# v_나이와 v_가격이 학습되어 있으면 예측 가능
```

### 2) 계산 효율성
```python
# 모든 특성 쌍을 저장할 필요 없음
# O(n²) → O(n×k) (k는 임베딩 차원)
# n=1000개 특성, k=10차원 → 1000×10 = 10,000 파라미터
```

### 3) 해석 가능성
```python
# 각 임베딩 차원이 특정 "잠재 요인"을 나타냄
# 차원1: "가격 민감도"
# 차원2: "브랜드 선호도"
# 차원3: "기술 친숙도"
```

FM은 이렇게 선형 모델의 한계를 극복하면서도 효율적이고 해석 가능한 모델을 제공합니다!

# FMB

FMB (Factorization Machine Block)는 추천 시스템에서 매우 중요한 아키텍처입니다. Factorization Machine의 핵심 아이디어를 기반으로 하고 있어서, 먼저 FM부터 설명하고 FMB로 넘어가겠습니다.

## 1. Factorization Machine (FM) 기본 개념

### FM이 해결하는 문제
**기존 선형 모델의 한계**:
```python
# 선형 모델: y = w₀ + w₁x₁ + w₂x₂ + w₃x₃
# 문제: 특성 간 상호작용을 고려하지 못함

# 예시: 사용자 나이와 아이템 가격의 상호작용
# 젊은 사용자(나이=25) + 저가 아이템(가격=50) = 높은 클릭률
# 나이만 봐도, 가격만 봐도 이런 패턴을 찾을 수 없음
```

### FM의 해결책
```python
# FM 모델: y = w₀ + Σwᵢxᵢ + ΣΣ⟨vᵢ,vⱼ⟩xᵢxⱼ
# vᵢ, vⱼ: 각 특성의 임베딩 벡터
# ⟨vᵢ,vⱼ⟩: 두 특성 임베딩의 내적 (상호작용 강도)

# 예시: 사용자 나이와 아이템 가격의 상호작용
# v_나이 = [0.1, 0.2, 0.3]  # 나이 특성의 3차원 임베딩
# v_가격 = [0.4, 0.5, 0.6]  # 가격 특성의 3차원 임베딩
# 상호작용 = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.32
```

## 2. FMB (Factorization Machine Block) 구조

### FMB의 핵심 아이디어
FMB는 FM의 2차 상호작용을 효율적으로 계산하는 블록입니다.

### 단계별 처리 과정

**입력 데이터 예제**:
```python
# 배치 크기 2, 특성 수 4, 임베딩 차원 3
inputs = torch.tensor([
    # 첫 번째 샘플
    [[1.0, 2.0, 3.0],  # 특성1 (예: 사용자 나이)
     [4.0, 5.0, 6.0],  # 특성2 (예: 아이템 가격)
     [7.0, 8.0, 9.0],  # 특성3 (예: 카테고리)
     [10.0, 11.0, 12.0]], # 특성4 (예: 시간)
    # 두 번째 샘플
    [[13.0, 14.0, 15.0],  # 특성1
     [16.0, 17.0, 18.0],  # 특성2
     [19.0, 20.0, 21.0],  # 특성3
     [22.0, 23.0, 24.0]]  # 특성4
])
# shape: [2, 4, 3]
```

### FMB 처리 과정

**1단계: 차원 변환**
```python
# (bs, num_emb_in, dim_emb) -> (bs, dim_emb, num_emb_in)
outputs = inputs.permute(0, 2, 1)
# shape: [2, 3, 4] - 임베딩 차원과 특성 차원을 바꿈
```

**2단계: 가중치와 곱셈**
```python
# weight.shape: [num_emb_in, rank] = [4, 2]
weight = torch.tensor([
    [0.1, 0.2],  # 특성1의 랭크 가중치
    [0.3, 0.4],  # 특성2의 랭크 가중치
    [0.5, 0.6],  # 특성3의 랭크 가중치
    [0.7, 0.8]   # 특성4의 랭크 가중치
])

# (bs, dim_emb, num_emb_in) @ (num_emb_in, rank) -> (bs, dim_emb, rank)
outputs = outputs @ weight
# shape: [2, 3, 2]
```

**3단계: bmm으로 상호작용 계산**
```python
# (bs, num_emb_in, dim_emb) @ (bs, dim_emb, rank) -> (bs, num_emb_in, rank)
outputs = torch.bmm(inputs, outputs)
# shape: [2, 4, 2]

# 이 연산이 FM의 2차 상호작용을 계산!
# 각 특성 쌍의 상호작용이 rank 차원으로 압축됨
```

**4단계: 차원 재구성**
```python
# (bs, num_emb_in, rank) -> (bs, num_emb_in * rank)
outputs = outputs.view(-1, self.num_emb_in * self.rank)
# shape: [2, 8] - 4개 특성 × 2차원 랭크 = 8차원
```

**5단계: MLP로 변환**
```python
# (bs, num_emb_in * rank) -> (bs, num_emb_out * dim_emb)
outputs = self.mlp(self.norm(outputs))
# shape: [2, 6] - 2개 출력 특성 × 3차원 임베딩 = 6차원
```

**6단계: 최종 출력**
```python
# (bs, num_emb_out * dim_emb) -> (bs, num_emb_out, dim_emb)
outputs = outputs.view(-1, self.num_emb_out, self.dim_emb)
# shape: [2, 2, 3] - 배치, 출력 특성 수, 임베딩 차원
```

## 3. FMB의 장점

### 1. 효율적인 상호작용 계산
```python
# 기존 FM: O(n²) 복잡도 (모든 특성 쌍 계산)
# FMB: O(n×rank) 복잡도 (랭크로 압축)

# 예시: 100개 특성
# 기존 FM: 100×100 = 10,000개 상호작용
# FMB: 100×24 = 2,400개 상호작용 (랭크=24)
```

### 2. 학습 가능한 상호작용
```python
# weight 파라미터가 학습됨
# 특성 간 어떤 상호작용이 중요한지 자동 학습

# 예시:
# weight[user_age, :] = [0.8, 0.2]  # 나이 특성은 첫 번째 랭크에 집중
# weight[item_price, :] = [0.3, 0.9]  # 가격 특성은 두 번째 랭크에 집중
```

### 3. 차원 축소 효과
```python
# 입력: 4개 특성 × 3차원 = 12차원
# 상호작용: 4×4 = 16개 상호작용
# FMB 출력: 2개 특성 × 3차원 = 6차원
# 효과적으로 차원을 줄이면서 중요한 상호작용만 유지
```

## 4. 실제 추천 시스템에서의 활용

### 사용자-아이템 상호작용 예제
```python
# 입력 특성들
features = {
    'user_age': [25, 32, 45],      # 사용자 나이
    'user_gender': [1, 0, 1],      # 사용자 성별
    'item_price': [100, 200, 50],  # 아이템 가격
    'item_category': [1, 2, 1],    # 아이템 카테고리
    'hour': [14, 20, 10],          # 시간
    'day_of_week': [2, 5, 1]       # 요일
}

# FMB가 학습하는 상호작용들:
# 1. user_age × item_price: 젊은 사용자 + 저가 아이템
# 2. user_gender × item_category: 성별별 카테고리 선호도
# 3. hour × day_of_week: 시간대별 요일 패턴
# 4. user_age × hour: 나이대별 시간대 선호도
```

### 상호작용 패턴 학습
```python
# FMB가 발견할 수 있는 패턴들:
# - 20대 여성 + 화장품 카테고리 + 저녁 시간 = 높은 클릭률
# - 40대 남성 + 전자제품 카테고리 + 주말 = 높은 클릭률
# - 30대 + 중간 가격대 + 평일 오후 = 높은 클릭률
```

## 5. FMB vs 다른 방법들

### FMB vs DeepFM
```python
# DeepFM: FM + Deep Neural Network
# FMB: 순수 FM 기반, 더 효율적

# DeepFM: 복잡한 비선형 패턴 + FM 상호작용
# FMB: FM 상호작용에 집중, 스케일링에 유리
```

### FMB vs DCN (Deep & Cross Network)
```python
# DCN: 명시적 교차 특성 생성
# FMB: 임베딩 기반 상호작용 (더 효율적)

# DCN: 특성 수가 많아지면 계산량 폭증
# FMB: 랭크로 압축하여 효율적
```

FMB는 Factorization Machine의 핵심 아이디어를 현대적으로 구현한 것으로, 추천 시스템에서 특성 간 상호작용을 효율적으로 학습할 수 있게 해주는 중요한 아키텍처입니다.
